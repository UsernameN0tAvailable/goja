[
	{
		"documentation": "/**\n * A factory for array wrappers so that arrays can be used as keys in a map, sorted or not.\n *\n * The comparator implementation makes two assumptions:\n * - All elements are instances of Comparable\n * - When comparing two arrays, they both contain elements of the same type in corresponding\n *   indices.\n *\n * Otherwise, ClassCastExceptions may occur. The equality method can compare any two arrays.\n *\n * This class is not efficient and is mostly meant to compare really small arrays, like those\n * generally used as indices and keys in a KVStore.\n */",
		"name": "org.apache.spark.util.kvstore.ArrayWrappers",
		"extends": "",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  public static Comparable\u003cObject\u003e forArray(Object a)",
				"documentation": "/**\n * A factory for array wrappers so that arrays can be used as keys in a map, sorted or not.\n *\n * The comparator implementation makes two assumptions:\n * - All elements are instances of Comparable\n * - When comparing two arrays, they both contain elements of the same type in corresponding\n *   indices.\n *\n * Otherwise, ClassCastExceptions may occur. The equality method can compare any two arrays.\n *\n * This class is not efficient and is mostly meant to compare really small arrays, like those\n * generally used as indices and keys in a KVStore.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.ArrayWrappers.ComparableIntArray",
			"org.apache.spark.util.kvstore.ArrayWrappers.ComparableLongArray",
			"org.apache.spark.util.kvstore.ArrayWrappers.ComparableByteArray",
			"org.apache.spark.util.kvstore.ArrayWrappers.ComparableObjectArray"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.ArrayWrappers.ComparableIntArray",
		"extends": "",
		"Methods": [
			{
				"signature": "ComparableIntArray(int[] array)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int compareTo(ComparableIntArray other)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Comparable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.ArrayWrappers.ComparableLongArray",
		"extends": "",
		"Methods": [
			{
				"signature": "ComparableLongArray(long[] array)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int compareTo(ComparableLongArray other)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Comparable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.ArrayWrappers.ComparableByteArray",
		"extends": "",
		"Methods": [
			{
				"signature": "ComparableByteArray(byte[] array)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int compareTo(ComparableByteArray other)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Comparable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.ArrayWrappers.ComparableObjectArray",
		"extends": "",
		"Methods": [
			{
				"signature": "ComparableObjectArray(Object[] array)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    @SuppressWarnings(\"unchecked\")\n    public int compareTo(ComparableObjectArray other)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Comparable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Implementation of KVStore that keeps data deserialized in memory. This store does not index\n * data; instead, whenever iterating over an indexed field, the stored data is copied and sorted\n * according to the index. This saves memory but makes iteration more expensive.\n */",
		"name": "org.apache.spark.util.kvstore.InMemoryStore",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public \u003cT\u003e T getMetadata(Class\u003cT\u003e klass)",
				"documentation": "/**\n * Implementation of KVStore that keeps data deserialized in memory. This store does not index\n * data; instead, whenever iterating over an indexed field, the stored data is copied and sorted\n * according to the index. This saves memory but makes iteration more expensive.\n */"
			},
			{
				"signature": "@Override\n  public void setMetadata(Object value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long count(Class\u003c?\u003e type)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long count(Class\u003c?\u003e type, String index, Object indexedValue) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e T read(Class\u003cT\u003e klass, Object naturalKey)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(Object value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void delete(Class\u003c?\u003e type, Object naturalKey)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e KVStoreView\u003cT\u003e view(Class\u003cT\u003e type)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e boolean removeAllByIndexValues(\n      Class\u003cT\u003e klass,\n      String index,\n      Collection\u003c?\u003e indexValues)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  private static Comparable\u003cObject\u003e asKey(Object in)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  private static \u003cT\u003e KVStoreView\u003cT\u003e emptyView()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.util.kvstore.KVStore"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.InMemoryStore.InMemoryLists",
			"org.apache.spark.util.kvstore.InMemoryStore.NaturalKeys",
			"org.apache.spark.util.kvstore.InMemoryStore.InstanceList",
			"org.apache.spark.util.kvstore.InMemoryStore.InstanceList.CountingRemoveIfForEach",
			"org.apache.spark.util.kvstore.InMemoryStore.InMemoryIterator",
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.InMemoryStore.InMemoryLists",
			"org.apache.spark.util.kvstore.InMemoryStore.NaturalKeys",
			"org.apache.spark.util.kvstore.InMemoryStore.InstanceList",
			"org.apache.spark.util.kvstore.InMemoryStore.InMemoryView",
			"org.apache.spark.util.kvstore.InMemoryStore.InMemoryIterator"
		]
	},
	{
		"documentation": "/**\n   * Encapsulates ConcurrentHashMap so that the typing in and out of the map strictly maps a\n   * class of type T to an InstanceList of type T.\n   */",
		"name": "org.apache.spark.util.kvstore.InMemoryStore.InMemoryLists",
		"extends": "",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n    public \u003cT\u003e InstanceList\u003cT\u003e get(Class\u003cT\u003e type)",
				"documentation": "/**\n   * Encapsulates ConcurrentHashMap so that the typing in and out of the map strictly maps a\n   * class of type T to an InstanceList of type T.\n   */"
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n    public \u003cT\u003e void write(T value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public void clear()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.InMemoryStore"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * An alias class for the type \"{@literal ConcurrentHashMap\u003cComparable\u003cObject\u003e, Boolean\u003e}\",\n   * which is used as a concurrent hashset for storing natural keys\n   * and the boolean value doesn't matter.\n   */",
		"name": "org.apache.spark.util.kvstore.InMemoryStore.NaturalKeys",
		"extends": "java.util.concurrent.ConcurrentHashMap",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.InMemoryStore"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.InMemoryStore.InstanceList",
		"extends": "",
		"Methods": [
			{
				"signature": "private InstanceList(Class\u003c?\u003e klass)",
				"documentation": ""
			},
			{
				"signature": "KVTypeInfo.Accessor getIndexAccessor(String indexName)",
				"documentation": ""
			},
			{
				"signature": "int countingRemoveAllByIndexValues(String index, Collection\u003c?\u003e indexValues)",
				"documentation": ""
			},
			{
				"signature": "public T get(Object key)",
				"documentation": ""
			},
			{
				"signature": "public void put(T value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public boolean delete(Object key)",
				"documentation": ""
			},
			{
				"signature": "public boolean delete(Object key, T value)",
				"documentation": ""
			},
			{
				"signature": "private void deleteParentIndex(Object key)",
				"documentation": ""
			},
			{
				"signature": "public int size()",
				"documentation": ""
			},
			{
				"signature": "public InMemoryView\u003cT\u003e view()",
				"documentation": ""
			},
			{
				"signature": "private static \u003cT\u003e Predicate\u003c? super T\u003e getPredicate(\n        KVTypeInfo.Accessor getter,\n        Collection\u003c?\u003e values)",
				"documentation": ""
			},
			{
				"signature": "private static Object indexValueForEntity(KVTypeInfo.Accessor getter, Object entity)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.InMemoryStore.InstanceList.CountingRemoveIfForEach"
		],
		"usedBy": [
			"org.apache.spark.util.kvstore.InMemoryStore"
		],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.InMemoryStore.InstanceList.CountingRemoveIfForEach"
		]
	},
	{
		"documentation": "/**\n     * A BiConsumer to control multi-entity removal.  We use this in a forEach rather than an\n     * iterator because there is a bug in jdk8 which affects remove() on all concurrent map\n     * iterators.  https://bugs.openjdk.java.net/browse/JDK-8078645\n     */",
		"name": "org.apache.spark.util.kvstore.InMemoryStore.InstanceList.CountingRemoveIfForEach",
		"extends": "",
		"Methods": [
			{
				"signature": "CountingRemoveIfForEach(InstanceList\u003cT\u003e instanceList, Predicate\u003c? super T\u003e filter)",
				"documentation": "/**\n       * Keeps a count of the number of elements removed.  This count is not currently surfaced\n       * to clients of KVStore as Java's generic removeAll() construct returns only a boolean,\n       * but I found it handy to have the count of elements removed while debugging; a count being\n       * no more complicated than a boolean, I've retained that behavior here, even though there\n       * is no current requirement.\n       */"
			},
			{
				"signature": "@Override\n      public void accept(Comparable\u003cObject\u003e key, T value)",
				"documentation": ""
			},
			{
				"signature": "public int count()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.function.BiConsumer"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.InMemoryStore",
			"org.apache.spark.util.kvstore.InMemoryStore.InstanceList"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.InMemoryStore.InMemoryView",
		"extends": "org.apache.spark.util.kvstore.KVStoreView",
		"Methods": [
			{
				"signature": "InMemoryView(\n        ConcurrentMap\u003cComparable\u003cObject\u003e, T\u003e data,\n        KVTypeInfo ti,\n        String naturalParentIndexName,\n        ConcurrentMap\u003cComparable\u003cObject\u003e, NaturalKeys\u003e parentToChildrenMap)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Iterator\u003cT\u003e iterator()",
				"documentation": ""
			},
			{
				"signature": "private List\u003cT\u003e copyElements()",
				"documentation": "/**\n     * Create a copy of the input elements, filtering the values for child indices if needed.\n     */"
			},
			{
				"signature": "private int compare(T e1, T e2, KVTypeInfo.Accessor getter)",
				"documentation": ""
			},
			{
				"signature": "private int compare(T e1, KVTypeInfo.Accessor getter, Comparable\u003c?\u003e v2)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.InMemoryStore.InMemoryIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "InMemoryIterator(Iterator\u003cT\u003e iter)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public T next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void remove()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public List\u003cT\u003e next(int max)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean skip(long n)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.util.kvstore.KVStoreIterator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.InMemoryStore"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Tags a field to be indexed when storing an object.\n *\n * \u003cp\u003e\n * Types are required to have a natural index that uniquely identifies instances in the store.\n * The default value of the annotation identifies the natural index for the type.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Indexes allow for more efficient sorting of data read from the store. By annotating a field or\n * \"getter\" method with this annotation, an index will be created that will provide sorting based on\n * the string value of that field.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Note that creating indices means more space will be needed, and maintenance operations like\n * updating or deleting a value will become more expensive.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Indices are restricted to String, integral types (byte, short, int, long, boolean), and arrays\n * of those values.\n * \u003c/p\u003e\n */",
		"name": "org.apache.spark.util.kvstore.KVIndex",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.util.kvstore.LevelDBBenchmark",
			"org.apache.spark.util.kvstore.LevelDBBenchmark.SimpleType",
			"org.apache.spark.util.kvstore.LevelDBBenchmark.IndexedType",
			"org.apache.spark.util.kvstore.RocksDBBenchmark",
			"org.apache.spark.util.kvstore.RocksDBBenchmark.SimpleType",
			"org.apache.spark.util.kvstore.RocksDBBenchmark.IndexedType"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [
			"org.apache.spark.util.kvstore.ArrayKeyIndexType",
			"org.apache.spark.util.kvstore.CustomType1",
			"org.apache.spark.util.kvstore.CustomType2",
			"org.apache.spark.util.kvstore.IntKeyType",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.NoNaturalIndex2",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.DuplicateIndex",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.EmptyIndexName",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.IllegalIndexName",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.IllegalIndexMethod",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.NoNaturalIndex2",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.DuplicateIndex",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.EmptyIndexName",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.IllegalIndexName",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.IllegalIndexMethod"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Abstraction for a local key/value store for storing app data.\n *\n * \u003cp\u003e\n * There are two main features provided by the implementations of this interface:\n * \u003c/p\u003e\n *\n * \u003ch3\u003eSerialization\u003c/h3\u003e\n *\n * \u003cp\u003e\n * If the underlying data store requires serialization, data will be serialized to and deserialized\n * using a {@link KVStoreSerializer}, which can be customized by the application. The serializer is\n * based on Jackson, so it supports all the Jackson annotations for controlling the serialization of\n * app-defined types.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Data is also automatically compressed to save disk space.\n * \u003c/p\u003e\n *\n * \u003ch3\u003eAutomatic Key Management\u003c/h3\u003e\n *\n * \u003cp\u003e\n * When using the built-in key management, the implementation will automatically create unique\n * keys for each type written to the store. Keys are based on the type name, and always start\n * with the \"+\" prefix character (so that it's easy to use both manual and automatic key\n * management APIs without conflicts).\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Another feature of automatic key management is indexing; by annotating fields or methods of\n * objects written to the store with {@link KVIndex}, indices are created to sort the data\n * by the values of those properties. This makes it possible to provide sorting without having\n * to load all instances of those types from the store.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * KVStore instances are thread-safe for both reads and writes.\n * \u003c/p\u003e\n */",
		"name": "org.apache.spark.util.kvstore.KVStore",
		"extends": "java.io.Closeable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.util.kvstore.InMemoryStore",
			"org.apache.spark.util.kvstore.LevelDB",
			"org.apache.spark.util.kvstore.RocksDB"
		],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An iterator for KVStore.\n *\n * \u003cp\u003e\n * Iterators may keep references to resources that need to be closed. It's recommended that users\n * explicitly close iterators after they're used.\n * \u003c/p\u003e\n */",
		"name": "org.apache.spark.util.kvstore.KVStoreIterator",
		"extends": "Iterator,",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.util.kvstore.InMemoryStore.InMemoryIterator",
			"org.apache.spark.util.kvstore.LevelDBIterator",
			"org.apache.spark.util.kvstore.RocksDBIterator"
		],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Serializer used to translate between app-defined types and the disk-based stores.\n *\n * \u003cp\u003e\n * The serializer is based on Jackson, so values are written as JSON. It also allows \"naked strings\"\n * and integers to be written as values directly, which will be written as UTF-8 strings.\n * \u003c/p\u003e\n */",
		"name": "org.apache.spark.util.kvstore.KVStoreSerializer",
		"extends": "",
		"Methods": [
			{
				"signature": "public KVStoreSerializer()",
				"documentation": "/**\n   * Object mapper used to process app-specific types. If an application requires a specific\n   * configuration of the mapper, it can subclass this serializer and add custom configuration\n   * to this object.\n   */"
			},
			{
				"signature": "public final byte[] serialize(Object o) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  public final \u003cT\u003e T deserialize(byte[] data, Class\u003cT\u003e klass) throws Exception",
				"documentation": ""
			},
			{
				"signature": "final byte[] serialize(long value)",
				"documentation": ""
			},
			{
				"signature": "final long deserializeLong(byte[] data)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A configurable view that allows iterating over values in a {@link KVStore}.\n *\n * \u003cp\u003e\n * The different methods can be used to configure the behavior of the iterator. Calling the same\n * method multiple times is allowed; the most recent value will be used.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * The iterators returned by this view are of type {@link KVStoreIterator}; they auto-close\n * when used in a for loop that exhausts their contents, but when used manually, they need\n * to be closed explicitly unless all elements are read.\n * \u003c/p\u003e\n */",
		"name": "org.apache.spark.util.kvstore.KVStoreView",
		"extends": "",
		"Methods": [
			{
				"signature": "public KVStoreView\u003cT\u003e reverse()",
				"documentation": "/**\n   * Reverses the order of iteration. By default, iterates in ascending order.\n   */"
			},
			{
				"signature": "public KVStoreView\u003cT\u003e index(String name)",
				"documentation": "/**\n   * Iterates according to the given index.\n   */"
			},
			{
				"signature": "public KVStoreView\u003cT\u003e parent(Object value)",
				"documentation": "/**\n   * Defines the value of the parent index when iterating over a child index. Only elements that\n   * match the parent index's value will be included in the iteration.\n   *\n   * \u003cp\u003e\n   * Required for iterating over child indices, will generate an error if iterating over a\n   * parent-less index.\n   * \u003c/p\u003e\n   */"
			},
			{
				"signature": "public KVStoreView\u003cT\u003e first(Object value)",
				"documentation": "/**\n   * Iterates starting at the given value of the chosen index (inclusive).\n   */"
			},
			{
				"signature": "public KVStoreView\u003cT\u003e last(Object value)",
				"documentation": "/**\n   * Stops iteration at the given value of the chosen index (inclusive).\n   */"
			},
			{
				"signature": "public KVStoreView\u003cT\u003e max(long max)",
				"documentation": "/**\n   * Stops iteration after a number of elements has been retrieved.\n   */"
			},
			{
				"signature": "public KVStoreView\u003cT\u003e skip(long n)",
				"documentation": "/**\n   * Skips a number of elements at the start of iteration. Skipped elements are not accounted\n   * when using {@link #max(long)}.\n   */"
			},
			{
				"signature": "public KVStoreIterator\u003cT\u003e closeableIterator() throws Exception",
				"documentation": "/**\n   * Returns an iterator for the current configuration.\n   */"
			}
		],
		"interfaces": [
			"Iterable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.kvstore.InMemoryStore.InMemoryView"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Wrapper around types managed in a KVStore, providing easy access to their indexed fields.\n */",
		"name": "org.apache.spark.util.kvstore.KVTypeInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "public KVTypeInfo(Class\u003c?\u003e type)",
				"documentation": "/**\n * Wrapper around types managed in a KVStore, providing easy access to their indexed fields.\n */"
			},
			{
				"signature": "private void checkIndex(KVIndex idx, Map\u003cString, KVIndex\u003e indices)",
				"documentation": ""
			},
			{
				"signature": "public Class\u003c?\u003e type()",
				"documentation": ""
			},
			{
				"signature": "public Object getIndexValue(String indexName, Object instance) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public Stream\u003cKVIndex\u003e indices()",
				"documentation": ""
			},
			{
				"signature": "Accessor getAccessor(String indexName)",
				"documentation": ""
			},
			{
				"signature": "Accessor getParentAccessor(String indexName)",
				"documentation": ""
			},
			{
				"signature": "String getParentIndexName(String indexName)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVTypeInfo.FieldAccessor",
			"org.apache.spark.util.kvstore.KVTypeInfo.MethodAccessor",
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.Accessor",
			"org.apache.spark.util.kvstore.KVTypeInfo.FieldAccessor",
			"org.apache.spark.util.kvstore.KVTypeInfo.MethodAccessor"
		]
	},
	{
		"documentation": "/**\n   * Abstracts the difference between invoking a Field and a Method.\n   */",
		"name": "org.apache.spark.util.kvstore.Accessor",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.util.kvstore.KVTypeInfo.FieldAccessor",
			"org.apache.spark.util.kvstore.KVTypeInfo.MethodAccessor"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.KVTypeInfo.FieldAccessor",
		"extends": "",
		"Methods": [
			{
				"signature": "FieldAccessor(Field field)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Object get(Object instance) throws ReflectiveOperationException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Class\u003c?\u003e getType()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.util.kvstore.Accessor"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.KVTypeInfo"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.KVTypeInfo.MethodAccessor",
		"extends": "",
		"Methods": [
			{
				"signature": "MethodAccessor(Method method)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Object get(Object instance) throws ReflectiveOperationException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Class\u003c?\u003e getType()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.util.kvstore.Accessor"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.KVTypeInfo"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Implementation of KVStore that uses LevelDB as the underlying data store.\n */",
		"name": "org.apache.spark.util.kvstore.LevelDB",
		"extends": "",
		"Methods": [
			{
				"signature": "public LevelDB(File path) throws Exception",
				"documentation": "/**\n   * Trying to close a JNI LevelDB handle with a closed DB causes JVM crashes. This is used to\n   * ensure that all iterators are correctly closed before LevelDB is closed. Use weak references\n   * to ensure that the iterator can be GCed, when it is only referenced here.\n   */"
			},
			{
				"signature": "public LevelDB(File path, KVStoreSerializer serializer) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e T getMetadata(Class\u003cT\u003e klass) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setMetadata(Object value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "\u003cT\u003e T get(byte[] key, Class\u003cT\u003e klass) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void put(byte[] key, Object value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e T read(Class\u003cT\u003e klass, Object naturalKey) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(Object value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public void writeAll(List\u003c?\u003e values) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void updateBatch(\n      WriteBatch batch,\n      Object value,\n      byte[] data,\n      Class\u003c?\u003e klass,\n      LevelDBTypeInfo.Index naturalIndex,\n      Collection\u003cLevelDBTypeInfo.Index\u003e indices) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void delete(Class\u003c?\u003e type, Object naturalKey) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e KVStoreView\u003cT\u003e view(Class\u003cT\u003e type) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public Iterator\u003cT\u003e iterator()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e boolean removeAllByIndexValues(\n      Class\u003cT\u003e klass,\n      String index,\n      Collection\u003c?\u003e indexValues) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long count(Class\u003c?\u003e type) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long count(Class\u003c?\u003e type, String index, Object indexedValue) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "void closeIterator(LevelDBIterator\u003c?\u003e it) throws IOException",
				"documentation": "/**\n   * Closes the given iterator if the DB is still open. Trying to close a JNI LevelDB handle\n   * with a closed DB can cause JVM crashes, so this ensures that situation does not happen.\n   */"
			},
			{
				"signature": "void notifyIteratorClosed(LevelDBIterator\u003c?\u003e it)",
				"documentation": "/**\n   * Remove iterator from iterator tracker. `LevelDBIterator` calls it to notify\n   * iterator is closed.\n   */"
			},
			{
				"signature": "LevelDBTypeInfo getTypeInfo(Class\u003c?\u003e type) throws Exception",
				"documentation": "/** Returns metadata about indices for the given type. */"
			},
			{
				"signature": "DB db()",
				"documentation": "/**\n   * Try to avoid use-after close since that has the tendency of crashing the JVM. This doesn't\n   * prevent methods that retrieved the instance from using it after close, but hopefully will\n   * catch most cases; otherwise, we'll need some kind of locking.\n   */"
			},
			{
				"signature": "private byte[] getTypeAlias(Class\u003c?\u003e klass) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.util.kvstore.KVStore"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.LevelDB.PrefixCache",
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.LevelDB.TypeAliases",
			"org.apache.spark.util.kvstore.LevelDB.PrefixCache"
		]
	},
	{
		"documentation": "/** Needs to be public for Jackson. */",
		"name": "org.apache.spark.util.kvstore.LevelDB.TypeAliases",
		"extends": "",
		"Methods": [
			{
				"signature": "TypeAliases(Map\u003cString, byte[]\u003e aliases)",
				"documentation": "/** Needs to be public for Jackson. */"
			},
			{
				"signature": "TypeAliases()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDB.PrefixCache",
		"extends": "",
		"Methods": [
			{
				"signature": "PrefixCache(Object entity)",
				"documentation": ""
			},
			{
				"signature": "byte[] getPrefix(LevelDBTypeInfo.Index idx) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.LevelDB"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "LevelDBIterator(Class\u003cT\u003e type, LevelDB db, KVStoreView\u003cT\u003e params) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public T next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void remove()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public List\u003cT\u003e next(int max)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean skip(long n)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Override\n  protected void finalize() throws Throwable",
				"documentation": "/**\n   * Because it's tricky to expose closeable iterators through many internal APIs, especially\n   * when Scala wrappers are used, this makes sure that, hopefully, the JNI resources held by\n   * the iterator will eventually be released.\n   */"
			},
			{
				"signature": "private byte[] loadNext()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  static boolean startsWith(byte[] key, byte[] prefix)",
				"documentation": ""
			},
			{
				"signature": "private boolean isEndMarker(byte[] key)",
				"documentation": ""
			},
			{
				"signature": "static int compare(byte[] a, byte[] b)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.util.kvstore.KVStoreIterator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Holds metadata about app-specific types stored in LevelDB. Serves as a cache for data collected\n * via reflection, to make it cheaper to access it multiple times.\n *\n * \u003cp\u003e\n * The hierarchy of keys stored in LevelDB looks roughly like the following. This hierarchy ensures\n * that iteration over indices is easy, and that updating values in the store is not overly\n * expensive. Of note, indices choose using more disk space (one value per key) instead of keeping\n * lists of pointers, which would be more expensive to update at runtime.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Indentation defines when a sub-key lives under a parent key. In LevelDB, this means the full\n * key would be the concatenation of everything up to that point in the hierarchy, with each\n * component separated by a NULL byte.\n * \u003c/p\u003e\n *\n * \u003cpre\u003e\n * +TYPE_NAME\n *   NATURAL_INDEX\n *     +NATURAL_KEY\n *     -\n *   -NATURAL_INDEX\n *   INDEX_NAME\n *     +INDEX_VALUE\n *       +NATURAL_KEY\n *     -INDEX_VALUE\n *     .INDEX_VALUE\n *       CHILD_INDEX_NAME\n *         +CHILD_INDEX_VALUE\n *           NATURAL_KEY_OR_DATA\n *         -\n *   -INDEX_NAME\n * \u003c/pre\u003e\n *\n * \u003cp\u003e\n * Entity data (either the entity's natural key or a copy of the data) is stored in all keys\n * that end with \"+\u003csomething\u003e\". A count of all objects that match a particular top-level index\n * value is kept at the end marker (\"-\u003csomething\u003e\"). A count is also kept at the natural index's end\n * marker, to make it easy to retrieve the number of all elements of a particular type.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * To illustrate, given a type \"Foo\", with a natural index and a second index called \"bar\", you'd\n * have these keys and values in the store for two instances, one with natural key \"key1\" and the\n * other \"key2\", both with value \"yes\" for \"bar\":\n * \u003c/p\u003e\n *\n * \u003cpre\u003e\n * Foo __main__ +key1   [data for instance 1]\n * Foo __main__ +key2   [data for instance 2]\n * Foo __main__ -       [count of all Foo]\n * Foo bar +yes +key1   [instance 1 key or data, depending on index type]\n * Foo bar +yes +key2   [instance 2 key or data, depending on index type]\n * Foo bar +yes -       [count of all Foo with \"bar=yes\" ]\n * \u003c/pre\u003e\n *\n * \u003cp\u003e\n * Note that all indexed values are prepended with \"+\", even if the index itself does not have an\n * explicit end marker. This allows for easily skipping to the end of an index by telling LevelDB\n * to seek to the \"phantom\" end marker of the index. Throughout the code and comments, this part\n * of the full LevelDB key is generally referred to as the \"index value\" of the entity.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Child indices are stored after their parent index. In the example above, let's assume there is\n * a child index \"child\", whose parent is \"bar\". If both instances have value \"no\" for this field,\n * the data in the store would look something like the following:\n * \u003c/p\u003e\n *\n * \u003cpre\u003e\n * ...\n * Foo bar +yes -\n * Foo bar .yes .child +no +key1   [instance 1 key or data, depending on index type]\n * Foo bar .yes .child +no +key2   [instance 2 key or data, depending on index type]\n * ...\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.util.kvstore.LevelDBTypeInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "LevelDBTypeInfo(LevelDB db, Class\u003c?\u003e type, byte[] alias) throws Exception",
				"documentation": ""
			},
			{
				"signature": "Class\u003c?\u003e type()",
				"documentation": ""
			},
			{
				"signature": "byte[] keyPrefix()",
				"documentation": ""
			},
			{
				"signature": "Index naturalIndex()",
				"documentation": ""
			},
			{
				"signature": "Index index(String name)",
				"documentation": ""
			},
			{
				"signature": "Collection\u003cIndex\u003e indices()",
				"documentation": ""
			},
			{
				"signature": "byte[] buildKey(byte[]... components)",
				"documentation": ""
			},
			{
				"signature": "byte[] buildKey(boolean addTypePrefix, byte[]... components)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.Index"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.Index"
		]
	},
	{
		"documentation": "/**\n   * Models a single index in LevelDB. See top-level class's javadoc for a description of how the\n   * keys are generated.\n   */",
		"name": "org.apache.spark.util.kvstore.Index",
		"extends": "",
		"Methods": [
			{
				"signature": "private Index(KVIndex self, KVTypeInfo.Accessor accessor, Index parent)",
				"documentation": "/**\n   * Models a single index in LevelDB. See top-level class's javadoc for a description of how the\n   * keys are generated.\n   */"
			},
			{
				"signature": "boolean isCopy()",
				"documentation": ""
			},
			{
				"signature": "boolean isChild()",
				"documentation": ""
			},
			{
				"signature": "Index parent()",
				"documentation": ""
			},
			{
				"signature": "byte[] childPrefix(Object value)",
				"documentation": "/**\n     * Creates a key prefix for child indices of this index. This allows the prefix to be\n     * calculated only once, avoiding redundant work when multiple child indices of the\n     * same parent index exist.\n     */"
			},
			{
				"signature": "Object getValue(Object entity) throws Exception",
				"documentation": "/**\n     * Gets the index value for a particular entity (which is the value of the field or method\n     * tagged with the index annotation). This is used as part of the LevelDB key where the\n     * entity (or its id) is stored.\n     */"
			},
			{
				"signature": "private void checkParent(byte[] prefix)",
				"documentation": ""
			},
			{
				"signature": "byte[] keyPrefix(byte[] prefix)",
				"documentation": "/** The prefix for all keys that belong to this index. */"
			},
			{
				"signature": "byte[] start(byte[] prefix, Object value)",
				"documentation": "/**\n     * The key where to start ascending iteration for entities whose value for the indexed field\n     * match the given value.\n     */"
			},
			{
				"signature": "byte[] end(byte[] prefix)",
				"documentation": "/** The key for the index's end marker. */"
			},
			{
				"signature": "byte[] end(byte[] prefix, Object value)",
				"documentation": "/** The key for the end marker for entries with the given value. */"
			},
			{
				"signature": "byte[] entityKey(byte[] prefix, Object entity) throws Exception",
				"documentation": "/** The full key in the index that identifies the given entity. */"
			},
			{
				"signature": "private void updateCount(WriteBatch batch, byte[] key, long delta)",
				"documentation": ""
			},
			{
				"signature": "private void addOrRemove(\n        WriteBatch batch,\n        Object entity,\n        Object existing,\n        byte[] data,\n        byte[] naturalKey,\n        byte[] prefix) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void add(\n        WriteBatch batch,\n        Object entity,\n        Object existing,\n        byte[] data,\n        byte[] naturalKey,\n        byte[] prefix) throws Exception",
				"documentation": "/**\n     * Add an entry to the index.\n     *\n     * @param batch Write batch with other related changes.\n     * @param entity The entity being added to the index.\n     * @param existing The entity being replaced in the index, or null.\n     * @param data Serialized entity to store (when storing the entity, not a reference).\n     * @param naturalKey The value's natural key (to avoid re-computing it for every index).\n     * @param prefix The parent index prefix, if this is a child index.\n     */"
			},
			{
				"signature": "void remove(\n        WriteBatch batch,\n        Object entity,\n        byte[] naturalKey,\n        byte[] prefix) throws Exception",
				"documentation": "/**\n     * Remove a value from the index.\n     *\n     * @param batch Write batch with other related changes.\n     * @param entity The entity being removed, to identify the index entry to modify.\n     * @param naturalKey The value's natural key (to avoid re-computing it for every index).\n     * @param prefix The parent index prefix, if this is a child index.\n     */"
			},
			{
				"signature": "long getCount(byte[] key)",
				"documentation": ""
			},
			{
				"signature": "byte[] toParentKey(Object value)",
				"documentation": ""
			},
			{
				"signature": "byte[] toKey(Object value)",
				"documentation": ""
			},
			{
				"signature": "byte[] toKey(Object value, byte prefix)",
				"documentation": "/**\n     * Translates a value to be used as part of the store key.\n     *\n     * Integral numbers are encoded as a string in a way that preserves lexicographical\n     * ordering. The string is prepended with a marker telling whether the number is negative\n     * or positive (\"*\" for negative and \"=\" for positive are used since \"-\" and \"+\" have the\n     * opposite of the desired order), and then the number is encoded into a hex string (so\n     * it occupies twice the number of bytes as the original type).\n     *\n     * Arrays are encoded by encoding each element separately, separated by KEY_SEPARATOR.\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.LevelDBTypeInfo",
			"org.apache.spark.util.kvstore.RocksDBTypeInfo"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Implementation of KVStore that uses RocksDB as the underlying data store.\n */",
		"name": "org.apache.spark.util.kvstore.RocksDB",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * Implementation of KVStore that uses RocksDB as the underlying data store.\n */"
			},
			{
				"signature": "public RocksDB(File path) throws Exception",
				"documentation": "/**\n   * Trying to close a JNI RocksDB handle with a closed DB causes JVM crashes. This is used to\n   * ensure that all iterators are correctly closed before RocksDB is closed. Use weak references\n   * to ensure that the iterator can be GCed, when it is only referenced here.\n   */"
			},
			{
				"signature": "public RocksDB(File path, KVStoreSerializer serializer) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e T getMetadata(Class\u003cT\u003e klass) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setMetadata(Object value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "\u003cT\u003e T get(byte[] key, Class\u003cT\u003e klass) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void put(byte[] key, Object value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e T read(Class\u003cT\u003e klass, Object naturalKey) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(Object value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public void writeAll(List\u003c?\u003e values) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void updateBatch(\n      WriteBatch batch,\n      Object value,\n      byte[] data,\n      Class\u003c?\u003e klass,\n      RocksDBTypeInfo.Index naturalIndex,\n      Collection\u003cRocksDBTypeInfo.Index\u003e indices) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void delete(Class\u003c?\u003e type, Object naturalKey) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e KVStoreView\u003cT\u003e view(Class\u003cT\u003e type) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public Iterator\u003cT\u003e iterator()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e boolean removeAllByIndexValues(\n      Class\u003cT\u003e klass,\n      String index,\n      Collection\u003c?\u003e indexValues) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long count(Class\u003c?\u003e type) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long count(Class\u003c?\u003e type, String index, Object indexedValue) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "void closeIterator(RocksDBIterator\u003c?\u003e it) throws IOException",
				"documentation": "/**\n   * Closes the given iterator if the DB is still open. Trying to close a JNI RocksDB handle\n   * with a closed DB can cause JVM crashes, so this ensures that situation does not happen.\n   */"
			},
			{
				"signature": "void notifyIteratorClosed(RocksDBIterator\u003c?\u003e it)",
				"documentation": "/**\n   * Remove iterator from iterator tracker. `RocksDBIterator` calls it to notify\n   * iterator is closed.\n   */"
			},
			{
				"signature": "RocksDBTypeInfo getTypeInfo(Class\u003c?\u003e type) throws Exception",
				"documentation": "/** Returns metadata about indices for the given type. */"
			},
			{
				"signature": "org.rocksdb.RocksDB db()",
				"documentation": "/**\n   * Try to avoid use-after close since that has the tendency of crashing the JVM. This doesn't\n   * prevent methods that retrieved the instance from using it after close, but hopefully will\n   * catch most cases; otherwise, we'll need some kind of locking.\n   */"
			},
			{
				"signature": "private byte[] getTypeAlias(Class\u003c?\u003e klass) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.util.kvstore.KVStore"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.RocksDB.PrefixCache",
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.RocksDB.TypeAliases",
			"org.apache.spark.util.kvstore.RocksDB.PrefixCache"
		]
	},
	{
		"documentation": "/** Needs to be public for Jackson. */",
		"name": "org.apache.spark.util.kvstore.RocksDB.TypeAliases",
		"extends": "",
		"Methods": [
			{
				"signature": "TypeAliases(Map\u003cString, byte[]\u003e aliases)",
				"documentation": "/** Needs to be public for Jackson. */"
			},
			{
				"signature": "TypeAliases()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDB.PrefixCache",
		"extends": "",
		"Methods": [
			{
				"signature": "PrefixCache(Object entity)",
				"documentation": ""
			},
			{
				"signature": "byte[] getPrefix(RocksDBTypeInfo.Index idx) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.RocksDB"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "RocksDBIterator(Class\u003cT\u003e type, RocksDB db, KVStoreView\u003cT\u003e params) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public T next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void remove()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public List\u003cT\u003e next(int max)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean skip(long n)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected void finalize() throws Throwable",
				"documentation": "/**\n   * Because it's tricky to expose closeable iterators through many internal APIs, especially\n   * when Scala wrappers are used, this makes sure that, hopefully, the JNI resources held by\n   * the iterator will eventually be released.\n   */"
			},
			{
				"signature": "private byte[] loadNext()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  static boolean startsWith(byte[] key, byte[] prefix)",
				"documentation": ""
			},
			{
				"signature": "private boolean isEndMarker(byte[] key)",
				"documentation": ""
			},
			{
				"signature": "static int compare(byte[] a, byte[] b)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.util.kvstore.KVStoreIterator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Holds metadata about app-specific types stored in RocksDB. Serves as a cache for data collected\n * via reflection, to make it cheaper to access it multiple times.\n *\n * \u003cp\u003e\n * The hierarchy of keys stored in RocksDB looks roughly like the following. This hierarchy ensures\n * that iteration over indices is easy, and that updating values in the store is not overly\n * expensive. Of note, indices choose using more disk space (one value per key) instead of keeping\n * lists of pointers, which would be more expensive to update at runtime.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Indentation defines when a sub-key lives under a parent key. In RocksDB, this means the full\n * key would be the concatenation of everything up to that point in the hierarchy, with each\n * component separated by a NULL byte.\n * \u003c/p\u003e\n *\n * \u003cpre\u003e\n * +TYPE_NAME\n *   NATURAL_INDEX\n *     +NATURAL_KEY\n *     -\n *   -NATURAL_INDEX\n *   INDEX_NAME\n *     +INDEX_VALUE\n *       +NATURAL_KEY\n *     -INDEX_VALUE\n *     .INDEX_VALUE\n *       CHILD_INDEX_NAME\n *         +CHILD_INDEX_VALUE\n *           NATURAL_KEY_OR_DATA\n *         -\n *   -INDEX_NAME\n * \u003c/pre\u003e\n *\n * \u003cp\u003e\n * Entity data (either the entity's natural key or a copy of the data) is stored in all keys\n * that end with \"+\u003csomething\u003e\". A count of all objects that match a particular top-level index\n * value is kept at the end marker (\"-\u003csomething\u003e\"). A count is also kept at the natural index's end\n * marker, to make it easy to retrieve the number of all elements of a particular type.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * To illustrate, given a type \"Foo\", with a natural index and a second index called \"bar\", you'd\n * have these keys and values in the store for two instances, one with natural key \"key1\" and the\n * other \"key2\", both with value \"yes\" for \"bar\":\n * \u003c/p\u003e\n *\n * \u003cpre\u003e\n * Foo __main__ +key1   [data for instance 1]\n * Foo __main__ +key2   [data for instance 2]\n * Foo __main__ -       [count of all Foo]\n * Foo bar +yes +key1   [instance 1 key or data, depending on index type]\n * Foo bar +yes +key2   [instance 2 key or data, depending on index type]\n * Foo bar +yes -       [count of all Foo with \"bar=yes\" ]\n * \u003c/pre\u003e\n *\n * \u003cp\u003e\n * Note that all indexed values are prepended with \"+\", even if the index itself does not have an\n * explicit end marker. This allows for easily skipping to the end of an index by telling RocksDB\n * to seek to the \"phantom\" end marker of the index. Throughout the code and comments, this part\n * of the full RocksDB key is generally referred to as the \"index value\" of the entity.\n * \u003c/p\u003e\n *\n * \u003cp\u003e\n * Child indices are stored after their parent index. In the example above, let's assume there is\n * a child index \"child\", whose parent is \"bar\". If both instances have value \"no\" for this field,\n * the data in the store would look something like the following:\n * \u003c/p\u003e\n *\n * \u003cpre\u003e\n * ...\n * Foo bar +yes -\n * Foo bar .yes .child +no +key1   [instance 1 key or data, depending on index type]\n * Foo bar .yes .child +no +key2   [instance 2 key or data, depending on index type]\n * ...\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.util.kvstore.RocksDBTypeInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "RocksDBTypeInfo(RocksDB db, Class\u003c?\u003e type, byte[] alias) throws Exception",
				"documentation": ""
			},
			{
				"signature": "Class\u003c?\u003e type()",
				"documentation": ""
			},
			{
				"signature": "byte[] keyPrefix()",
				"documentation": ""
			},
			{
				"signature": "Index naturalIndex()",
				"documentation": ""
			},
			{
				"signature": "Index index(String name)",
				"documentation": ""
			},
			{
				"signature": "Collection\u003cIndex\u003e indices()",
				"documentation": ""
			},
			{
				"signature": "byte[] buildKey(byte[]... components)",
				"documentation": ""
			},
			{
				"signature": "byte[] buildKey(boolean addTypePrefix, byte[]... components)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.Index"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.Index"
		]
	},
	{
		"documentation": "/**\n   * Models a single index in RocksDB. See top-level class's javadoc for a description of how the\n   * keys are generated.\n   */",
		"name": "org.apache.spark.util.kvstore.Index",
		"extends": "",
		"Methods": [
			{
				"signature": "private Index(KVIndex self, KVTypeInfo.Accessor accessor, Index parent)",
				"documentation": "/**\n   * Models a single index in RocksDB. See top-level class's javadoc for a description of how the\n   * keys are generated.\n   */"
			},
			{
				"signature": "boolean isCopy()",
				"documentation": ""
			},
			{
				"signature": "boolean isChild()",
				"documentation": ""
			},
			{
				"signature": "Index parent()",
				"documentation": ""
			},
			{
				"signature": "byte[] childPrefix(Object value)",
				"documentation": "/**\n     * Creates a key prefix for child indices of this index. This allows the prefix to be\n     * calculated only once, avoiding redundant work when multiple child indices of the\n     * same parent index exist.\n     */"
			},
			{
				"signature": "Object getValue(Object entity) throws Exception",
				"documentation": "/**\n     * Gets the index value for a particular entity (which is the value of the field or method\n     * tagged with the index annotation). This is used as part of the RocksDB key where the\n     * entity (or its id) is stored.\n     */"
			},
			{
				"signature": "private void checkParent(byte[] prefix)",
				"documentation": ""
			},
			{
				"signature": "byte[] keyPrefix(byte[] prefix)",
				"documentation": "/** The prefix for all keys that belong to this index. */"
			},
			{
				"signature": "byte[] start(byte[] prefix, Object value)",
				"documentation": "/**\n     * The key where to start ascending iteration for entities whose value for the indexed field\n     * match the given value.\n     */"
			},
			{
				"signature": "byte[] end(byte[] prefix)",
				"documentation": "/** The key for the index's end marker. */"
			},
			{
				"signature": "byte[] end(byte[] prefix, Object value)",
				"documentation": "/** The key for the end marker for entries with the given value. */"
			},
			{
				"signature": "byte[] entityKey(byte[] prefix, Object entity) throws Exception",
				"documentation": "/** The full key in the index that identifies the given entity. */"
			},
			{
				"signature": "private void updateCount(WriteBatch batch, byte[] key, long delta) throws RocksDBException",
				"documentation": ""
			},
			{
				"signature": "private void addOrRemove(\n        WriteBatch batch,\n        Object entity,\n        Object existing,\n        byte[] data,\n        byte[] naturalKey,\n        byte[] prefix) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void add(\n        WriteBatch batch,\n        Object entity,\n        Object existing,\n        byte[] data,\n        byte[] naturalKey,\n        byte[] prefix) throws Exception",
				"documentation": "/**\n     * Add an entry to the index.\n     *\n     * @param batch Write batch with other related changes.\n     * @param entity The entity being added to the index.\n     * @param existing The entity being replaced in the index, or null.\n     * @param data Serialized entity to store (when storing the entity, not a reference).\n     * @param naturalKey The value's natural key (to avoid re-computing it for every index).\n     * @param prefix The parent index prefix, if this is a child index.\n     */"
			},
			{
				"signature": "void remove(\n        WriteBatch batch,\n        Object entity,\n        byte[] naturalKey,\n        byte[] prefix) throws Exception",
				"documentation": "/**\n     * Remove a value from the index.\n     *\n     * @param batch Write batch with other related changes.\n     * @param entity The entity being removed, to identify the index entry to modify.\n     * @param naturalKey The value's natural key (to avoid re-computing it for every index).\n     * @param prefix The parent index prefix, if this is a child index.\n     */"
			},
			{
				"signature": "long getCount(byte[] key) throws RocksDBException",
				"documentation": ""
			},
			{
				"signature": "byte[] toParentKey(Object value)",
				"documentation": ""
			},
			{
				"signature": "byte[] toKey(Object value)",
				"documentation": ""
			},
			{
				"signature": "byte[] toKey(Object value, byte prefix)",
				"documentation": "/**\n     * Translates a value to be used as part of the store key.\n     *\n     * Integral numbers are encoded as a string in a way that preserves lexicographical\n     * ordering. The string is prepended with a marker telling whether the number is negative\n     * or positive (\"*\" for negative and \"=\" for positive are used since \"-\" and \"+\" have the\n     * opposite of the desired order), and then the number is encoded into a hex string (so\n     * it occupies twice the number of bytes as the original type).\n     *\n     * Arrays are encoded by encoding each element separately, separated by KEY_SEPARATOR.\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.LevelDBTypeInfo",
			"org.apache.spark.util.kvstore.RocksDBTypeInfo"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Exception thrown when the store implementation is not compatible with the underlying data.\n */",
		"name": "org.apache.spark.util.kvstore.UnsupportedStoreVersionException",
		"extends": "java.io.IOException",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.ArrayKeyIndexType",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.ArrayWrappersSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testGenericArrayKey()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.CustomType1",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.CustomType2",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.DBIteratorSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void setupClass()",
				"documentation": "/**\n   * Implementations should override this method; it is called only once, before all tests are\n   * run. Any state can be safely stored in static variables and cleaned up in a @AfterClass\n   * handler.\n   */"
			},
			{
				"signature": "@AfterClass\n  public static void cleanupData() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void setup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void refIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void copyIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void numericIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void childIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalIndexDescending() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void refIndexDescending() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void copyIndexDescending() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void numericIndexDescending() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void childIndexDescending() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalIndexWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void refIndexWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void copyIndexWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void numericIndexWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void childIndexWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalIndexDescendingWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void refIndexDescendingWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void copyIndexDescendingWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void numericIndexDescendingWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void childIndexDescendingWithStart() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalIndexWithSkip() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void refIndexWithSkip() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void copyIndexWithSkip() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void childIndexWithSkip() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalIndexWithMax() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void copyIndexWithMax() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void childIndexWithMax() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalIndexWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void refIndexWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void copyIndexWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void numericIndexWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void childIndexWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalIndexDescendingWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void refIndexDescendingWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void copyIndexDescendingWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void numericIndexDescendingWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void childIndexDescendingWithLast() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRefWithIntNaturalKey() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private CustomType1 pickLimit()",
				"documentation": ""
			},
			{
				"signature": "private int pickCount()",
				"documentation": ""
			},
			{
				"signature": "private \u003cT extends Comparable\u003cT\u003e\u003e int compareWithFallback(\n      T v1,\n      T v2,\n      CustomType1 ct1,\n      CustomType1 ct2)",
				"documentation": "/**\n   * Compares the two values and falls back to comparing the natural key of CustomType1\n   * if they're the same, to mimic the behavior of the indexing code.\n   */"
			},
			{
				"signature": "private void testIteration(\n      final BaseComparator order,\n      final KVStoreView\u003cCustomType1\u003e params,\n      final CustomType1 first,\n      final CustomType1 last) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void compareLists(Iterable\u003c?\u003e expected, List\u003c?\u003e actual)",
				"documentation": "/** Could use assertEquals(), but that creates hard to read errors for large lists. */"
			},
			{
				"signature": "private KVStoreView\u003cCustomType1\u003e view() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private List\u003cCustomType1\u003e collect(KVStoreView\u003cCustomType1\u003e view) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private List\u003cCustomType1\u003e sortBy(Comparator\u003cCustomType1\u003e comp)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.kvstore.InMemoryIteratorSuite",
			"org.apache.spark.util.kvstore.LevelDBIteratorSuite",
			"org.apache.spark.util.kvstore.RocksDBIteratorSuite"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.BaseComparator"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.BaseComparator",
		"extends": "java.util.Comparator",
		"Methods": [
			{
				"signature": "default BaseComparator fallback()",
				"documentation": "/**\n     * Returns a comparator that falls back to natural order if this comparator's ordering\n     * returns equality for two elements. Used to mimic how the index sorts things internally.\n     */"
			},
			{
				"signature": "default BaseComparator reverse()",
				"documentation": "/** Reverses the order of this comparator. */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.InMemoryIteratorSuite",
		"extends": "org.apache.spark.util.kvstore.DBIteratorSuite",
		"Methods": [
			{
				"signature": "@Override\n  protected KVStore createStore()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.InMemoryStoreSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testObjectWriteReadDelete() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMultipleObjectWriteReadDelete() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMetadata() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUpdate() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testArrayIndices() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRemoveAll() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBasicIteration() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDeleteParentIndex() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.IntKeyType",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A set of small benchmarks for the LevelDB implementation.\n *\n * The benchmarks are run over two different types (one with just a natural index, and one\n * with a ref index), over a set of 2^20 elements, and the following tests are performed:\n *\n * - write (then update) elements in sequential natural key order\n * - write (then update) elements in random natural key order\n * - iterate over natural index, ascending and descending\n * - iterate over ref index, ascending and descending\n */",
		"name": "org.apache.spark.util.kvstore.LevelDBBenchmark",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setup() throws Exception",
				"documentation": "/**\n * A set of small benchmarks for the LevelDB implementation.\n *\n * The benchmarks are run over two different types (one with just a natural index, and one\n * with a ref index), over a set of 2^20 elements, and the following tests are performed:\n *\n * - write (then update) elements in sequential natural key order\n * - write (then update) elements in random natural key order\n * - iterate over natural index, ascending and descending\n * - iterate over ref index, ascending and descending\n */"
			},
			{
				"signature": "@After\n  public void cleanup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void report()",
				"documentation": ""
			},
			{
				"signature": "private static String toMs(double nanos)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sequentialWritesNoIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomWritesNoIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sequentialWritesIndexedType() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomWritesIndexedTypeAndIteration() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void iterate(KVStoreView\u003c?\u003e view, String name) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void writeAll(List\u003c?\u003e entries, String timerName) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void deleteNoIndex(List\u003cSimpleType\u003e entries, String timerName) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void deleteIndexed(List\u003cIndexedType\u003e entries, String timerName) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private List\u003cSimpleType\u003e createSimpleType()",
				"documentation": ""
			},
			{
				"signature": "private List\u003cIndexedType\u003e createIndexedType()",
				"documentation": ""
			},
			{
				"signature": "private Timer newTimer(String name)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex",
			"org.apache.spark.util.kvstore.LevelDBBenchmark.SimpleType",
			"org.apache.spark.util.kvstore.LevelDBBenchmark.IndexedType"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.LevelDBBenchmark.SimpleType",
			"org.apache.spark.util.kvstore.LevelDBBenchmark.IndexedType"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBBenchmark.SimpleType",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [
			"org.apache.spark.util.kvstore.LevelDBBenchmark"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBBenchmark.IndexedType",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [
			"org.apache.spark.util.kvstore.LevelDBBenchmark"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBIteratorSuite",
		"extends": "org.apache.spark.util.kvstore.DBIteratorSuite",
		"Methods": [
			{
				"signature": "@AfterClass\n  public static void cleanup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected KVStore createStore() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@After\n  public void cleanup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void setup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReopenAndVersionCheckDb() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testObjectWriteReadDelete() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMultipleObjectWriteReadDelete() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMultipleTypesWriteReadDelete() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMetadata() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUpdate() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRemoveAll() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSkip() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNegativeIndexValues() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCloseLevelDBIterator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private CustomType1 createCustomType1(int i)",
				"documentation": ""
			},
			{
				"signature": "private int countKeys(Class\u003c?\u003e type) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBTypeInfoSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testIndexAnnotation() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoNaturalIndex()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoNaturalIndex2()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDuplicateIndex()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEmptyIndexName()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIllegalIndexName()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIllegalIndexMethod()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKeyClashes() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNumEncoding() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testArrayIndices() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private LevelDBTypeInfo newTypeInfo(Class\u003c?\u003e type) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void assertBefore(byte[] key1, byte[] key2)",
				"documentation": ""
			},
			{
				"signature": "private void assertBefore(String str1, String str2)",
				"documentation": ""
			},
			{
				"signature": "private void assertSame(byte[] key1, byte[] key2)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.NoNaturalIndex",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.NoNaturalIndex2",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.DuplicateIndex",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.EmptyIndexName",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.IllegalIndexName",
			"org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.IllegalIndexMethod"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.NoNaturalIndex",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.NoNaturalIndex2",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.DuplicateIndex",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.EmptyIndexName",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.IllegalIndexName",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.LevelDBTypeInfoSuite.IllegalIndexMethod",
		"extends": "",
		"Methods": [
			{
				"signature": "@KVIndex(\"id\")\n    public String id(boolean illegalParam)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A set of small benchmarks for the RocksDB implementation.\n *\n * The benchmarks are run over two different types (one with just a natural index, and one\n * with a ref index), over a set of 2^20 elements, and the following tests are performed:\n *\n * - write (then update) elements in sequential natural key order\n * - write (then update) elements in random natural key order\n * - iterate over natural index, ascending and descending\n * - iterate over ref index, ascending and descending\n */",
		"name": "org.apache.spark.util.kvstore.RocksDBBenchmark",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setup() throws Exception",
				"documentation": "/**\n * A set of small benchmarks for the RocksDB implementation.\n *\n * The benchmarks are run over two different types (one with just a natural index, and one\n * with a ref index), over a set of 2^20 elements, and the following tests are performed:\n *\n * - write (then update) elements in sequential natural key order\n * - write (then update) elements in random natural key order\n * - iterate over natural index, ascending and descending\n * - iterate over ref index, ascending and descending\n */"
			},
			{
				"signature": "@After\n  public void cleanup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void report()",
				"documentation": ""
			},
			{
				"signature": "private static String toMs(double nanos)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sequentialWritesNoIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomWritesNoIndex() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sequentialWritesIndexedType() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomWritesIndexedTypeAndIteration() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void iterate(KVStoreView\u003c?\u003e view, String name) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void writeAll(List\u003c?\u003e entries, String timerName) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void deleteNoIndex(List\u003cSimpleType\u003e entries, String timerName) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void deleteIndexed(List\u003cIndexedType\u003e entries, String timerName) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private List\u003cSimpleType\u003e createSimpleType()",
				"documentation": ""
			},
			{
				"signature": "private List\u003cIndexedType\u003e createIndexedType()",
				"documentation": ""
			},
			{
				"signature": "private Timer newTimer(String name)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex",
			"org.apache.spark.util.kvstore.RocksDBBenchmark.SimpleType",
			"org.apache.spark.util.kvstore.RocksDBBenchmark.IndexedType"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.RocksDBBenchmark.SimpleType",
			"org.apache.spark.util.kvstore.RocksDBBenchmark.IndexedType"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBBenchmark.SimpleType",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [
			"org.apache.spark.util.kvstore.RocksDBBenchmark"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBBenchmark.IndexedType",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [
			"org.apache.spark.util.kvstore.RocksDBBenchmark"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBIteratorSuite",
		"extends": "org.apache.spark.util.kvstore.DBIteratorSuite",
		"Methods": [
			{
				"signature": "@AfterClass\n  public static void cleanup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected KVStore createStore() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@After\n  public void cleanup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void setup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReopenAndVersionCheckDb() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testObjectWriteReadDelete() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMultipleObjectWriteReadDelete() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMultipleTypesWriteReadDelete() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMetadata() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUpdate() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRemoveAll() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSkip() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNegativeIndexValues() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCloseRocksDBIterator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private CustomType1 createCustomType1(int i)",
				"documentation": ""
			},
			{
				"signature": "private int countKeys(Class\u003c?\u003e type) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBTypeInfoSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testIndexAnnotation() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoNaturalIndex()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoNaturalIndex2()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDuplicateIndex()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEmptyIndexName()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIllegalIndexName()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIllegalIndexMethod()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKeyClashes() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNumEncoding() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testArrayIndices() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private RocksDBTypeInfo newTypeInfo(Class\u003c?\u003e type) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void assertBefore(byte[] key1, byte[] key2)",
				"documentation": ""
			},
			{
				"signature": "private void assertBefore(String str1, String str2)",
				"documentation": ""
			},
			{
				"signature": "private void assertSame(byte[] key1, byte[] key2)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.NoNaturalIndex",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.NoNaturalIndex2",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.DuplicateIndex",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.EmptyIndexName",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.IllegalIndexName",
			"org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.IllegalIndexMethod"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.NoNaturalIndex",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.NoNaturalIndex2",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.DuplicateIndex",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.EmptyIndexName",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.IllegalIndexName",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.kvstore.RocksDBTypeInfoSuite.IllegalIndexMethod",
		"extends": "",
		"Methods": [
			{
				"signature": "@KVIndex(\"id\")\n    public String id(boolean illegalParam)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.kvstore.KVIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Contains the context to create a {@link TransportServer}, {@link TransportClientFactory}, and to\n * setup Netty Channel pipelines with a\n * {@link org.apache.spark.network.server.TransportChannelHandler}.\n *\n * There are two communication protocols that the TransportClient provides, control-plane RPCs and\n * data-plane \"chunk fetching\". The handling of the RPCs is performed outside of the scope of the\n * TransportContext (i.e., by a user-provided handler), and it is responsible for setting up streams\n * which can be streamed through the data plane in chunks using zero-copy IO.\n *\n * The TransportServer and TransportClientFactory both create a TransportChannelHandler for each\n * channel. As each TransportChannelHandler contains a TransportClient, this enables server\n * processes to send messages back to the client on an existing channel.\n */",
		"name": "org.apache.spark.network.TransportContext",
		"extends": "",
		"Methods": [
			{
				"signature": "public TransportContext(TransportConf conf, RpcHandler rpcHandler)",
				"documentation": "/**\n   * Force to create MessageEncoder and MessageDecoder so that we can make sure they will be created\n   * before switching the current context class loader to ExecutorClassLoader.\n   *\n   * Netty's MessageToMessageEncoder uses Javassist to generate a matcher class and the\n   * implementation calls \"Class.forName\" to check if this calls is already generated. If the\n   * following two objects are created in \"ExecutorClassLoader.findClass\", it will cause\n   * \"ClassCircularityError\". This is because loading this Netty generated class will call\n   * \"ExecutorClassLoader.findClass\" to search this class, and \"ExecutorClassLoader\" will try to use\n   * RPC to load it and cause to load the non-exist matcher class again. JVM will report\n   * `ClassCircularityError` to prevent such infinite recursion. (See SPARK-17714)\n   */"
			},
			{
				"signature": "public TransportContext(\n      TransportConf conf,\n      RpcHandler rpcHandler,\n      boolean closeIdleConnections)",
				"documentation": ""
			},
			{
				"signature": "public TransportContext(\n      TransportConf conf,\n      RpcHandler rpcHandler,\n      boolean closeIdleConnections,\n      boolean isClientOnly)",
				"documentation": "/**\n   * Enables TransportContext initialization for underlying client and server.\n   *\n   * @param conf TransportConf\n   * @param rpcHandler RpcHandler responsible for handling requests and responses.\n   * @param closeIdleConnections Close idle connections if it is set to true.\n   * @param isClientOnly This config indicates the TransportContext is only used by a client.\n   *                     This config is more important when external shuffle is enabled.\n   *                     It stops creating extra event loop and subsequent thread pool\n   *                     for shuffle clients to handle chunked fetch requests.\n   */"
			},
			{
				"signature": "public TransportClientFactory createClientFactory(List\u003cTransportClientBootstrap\u003e bootstraps)",
				"documentation": "/**\n   * Initializes a ClientFactory which runs the given TransportClientBootstraps prior to returning\n   * a new Client. Bootstraps will be executed synchronously, and must run successfully in order\n   * to create a Client.\n   */"
			},
			{
				"signature": "public TransportClientFactory createClientFactory()",
				"documentation": ""
			},
			{
				"signature": "public TransportServer createServer(int port, List\u003cTransportServerBootstrap\u003e bootstraps)",
				"documentation": "/** Create a server which will attempt to bind to a specific port. */"
			},
			{
				"signature": "public TransportServer createServer(\n      String host, int port, List\u003cTransportServerBootstrap\u003e bootstraps)",
				"documentation": "/** Create a server which will attempt to bind to a specific host and port. */"
			},
			{
				"signature": "public TransportServer createServer(List\u003cTransportServerBootstrap\u003e bootstraps)",
				"documentation": "/** Creates a new server, binding to any available ephemeral port. */"
			},
			{
				"signature": "public TransportServer createServer()",
				"documentation": ""
			},
			{
				"signature": "public TransportChannelHandler initializePipeline(SocketChannel channel)",
				"documentation": ""
			},
			{
				"signature": "public TransportChannelHandler initializePipeline(\n      SocketChannel channel,\n      RpcHandler channelRpcHandler)",
				"documentation": "/**\n   * Initializes a client or server Netty Channel Pipeline which encodes/decodes messages and\n   * has a {@link org.apache.spark.network.server.TransportChannelHandler} to handle request or\n   * response messages.\n   *\n   * @param channel The channel to initialize.\n   * @param channelRpcHandler The RPC handler to use for the channel.\n   *\n   * @return Returns the created TransportChannelHandler, which includes a TransportClient that can\n   * be used to communicate on this channel. The TransportClient is directly associated with a\n   * ChannelHandler to ensure all users of the same channel get the same TransportClient object.\n   */"
			}
		],
		"interfaces": [
			"java.io.Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.crypto.AuthIntegrationSuite.AuthTestCtx",
			"org.apache.spark.network.sasl.SparkSaslSuite.SaslTestCtx"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.client.TransportClientFactorySuite",
			"org.apache.spark.network.crypto.AuthIntegrationSuite",
			"org.apache.spark.network.sasl.SparkSaslSuite",
			"org.apache.spark.network.util.NettyMemoryMetricsSuite",
			"org.apache.spark.network.shuffle.ExternalBlockStoreClient",
			"org.apache.spark.network.sasl.SaslIntegrationSuite",
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.buffer.ManagedBuffer",
			"org.apache.spark.network.buffer.NettyManagedBuffer",
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.client.BaseResponseCallback",
			"org.apache.spark.network.client.ChunkFetchFailureException",
			"org.apache.spark.network.client.ChunkReceivedCallback",
			"org.apache.spark.network.client.MergedBlockMetaResponseCallback",
			"org.apache.spark.network.client.RpcResponseCallback",
			"org.apache.spark.network.client.StreamCallback",
			"org.apache.spark.network.client.StreamCallbackWithID",
			"org.apache.spark.network.client.StreamInterceptor",
			"org.apache.spark.network.client.TransportClient",
			"org.apache.spark.network.client.TransportClientBootstrap",
			"org.apache.spark.network.client.TransportClientFactory"
		]
	},
	{
		"documentation": "/**\n * A {@link ManagedBuffer} backed by a segment in a file.\n */",
		"name": "org.apache.spark.network.buffer.FileSegmentManagedBuffer",
		"extends": "org.apache.spark.network.buffer.ManagedBuffer",
		"Methods": [
			{
				"signature": "public FileSegmentManagedBuffer(TransportConf conf, File file, long offset, long length)",
				"documentation": "/**\n * A {@link ManagedBuffer} backed by a segment in a file.\n */"
			},
			{
				"signature": "@Override\n  public long size()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ByteBuffer nioByteBuffer() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public InputStream createInputStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer retain()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer release()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object convertToNetty() throws IOException",
				"documentation": ""
			},
			{
				"signature": "public File getFile()",
				"documentation": ""
			},
			{
				"signature": "public long getOffset()",
				"documentation": ""
			},
			{
				"signature": "public long getLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.StreamTestHelper"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.LimitedInputStream"
		],
		"usedBy": [
			"org.apache.spark.network.ChunkFetchIntegrationSuite",
			"org.apache.spark.network.protocol.MergedBlockMetaSuccessSuite",
			"org.apache.spark.network.sasl.SparkSaslSuite",
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolver",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver",
			"org.apache.spark.network.shuffle.SimpleDownloadFile",
			"org.apache.spark.network.shuffle.SimpleDownloadWritableChannel"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This interface provides an immutable view for data in the form of bytes. The implementation\n * should specify how the data is provided:\n *\n * - {@link FileSegmentManagedBuffer}: data backed by part of a file\n * - {@link NioManagedBuffer}: data backed by a NIO ByteBuffer\n * - {@link NettyManagedBuffer}: data backed by a Netty ByteBuf\n *\n * The concrete buffer implementation might be managed outside the JVM garbage collector.\n * For example, in the case of {@link NettyManagedBuffer}, the buffers are reference counted.\n * In that case, if the buffer is going to be passed around to a different thread, retain/release\n * should be called.\n */",
		"name": "org.apache.spark.network.buffer.ManagedBuffer",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.buffer.NettyManagedBuffer",
			"org.apache.spark.network.buffer.NioManagedBuffer"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A {@link ManagedBuffer} backed by a Netty {@link ByteBuf}.\n */",
		"name": "org.apache.spark.network.buffer.NettyManagedBuffer",
		"extends": "org.apache.spark.network.buffer.ManagedBuffer",
		"Methods": [
			{
				"signature": "public NettyManagedBuffer(ByteBuf buf)",
				"documentation": "/**\n * A {@link ManagedBuffer} backed by a Netty {@link ByteBuf}.\n */"
			},
			{
				"signature": "@Override\n  public long size()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ByteBuffer nioByteBuffer() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public InputStream createInputStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer retain()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer release()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object convertToNetty() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.TestManagedBuffer"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.protocol.ChunkFetchSuccess",
			"org.apache.spark.network.protocol.MergedBlockMetaSuccess",
			"org.apache.spark.network.protocol.OneWayMessage",
			"org.apache.spark.network.protocol.RpcRequest",
			"org.apache.spark.network.protocol.RpcResponse",
			"org.apache.spark.network.protocol.UploadStream",
			"org.apache.spark.network.sasl.SaslMessage",
			"org.apache.spark.network.protocol.MessageWithHeaderSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockPusherSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A {@link ManagedBuffer} backed by {@link ByteBuffer}.\n */",
		"name": "org.apache.spark.network.buffer.NioManagedBuffer",
		"extends": "org.apache.spark.network.buffer.ManagedBuffer",
		"Methods": [
			{
				"signature": "public NioManagedBuffer(ByteBuffer buf)",
				"documentation": "/**\n * A {@link ManagedBuffer} backed by {@link ByteBuffer}.\n */"
			},
			{
				"signature": "@Override\n  public long size()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ByteBuffer nioByteBuffer() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public InputStream createInputStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer retain()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer release()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object convertToNetty() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.StreamTestHelper"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.client.TransportClient",
			"org.apache.spark.network.server.TransportRequestHandler",
			"org.apache.spark.network.ChunkFetchIntegrationSuite",
			"org.apache.spark.network.RequestTimeoutIntegrationSuite",
			"org.apache.spark.network.RpcIntegrationSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockPusher",
			"org.apache.spark.network.shuffle.BlockPushCallback",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockPusherSuite",
			"org.apache.spark.network.shuffle.RetryingBlockTransferorSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A basic callback. This is extended by {@link RpcResponseCallback} and\n * {@link MergedBlockMetaResponseCallback} so that both RpcRequests and MergedBlockMetaRequests\n * can be handled in {@link TransportResponseHandler} a similar way.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.network.client.BaseResponseCallback",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.client.MergedBlockMetaResponseCallback",
			"org.apache.spark.network.client.RpcResponseCallback"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * General exception caused by a remote exception while fetching a chunk.\n */",
		"name": "org.apache.spark.network.client.ChunkFetchFailureException",
		"extends": "RuntimeException",
		"Methods": [
			{
				"signature": "public ChunkFetchFailureException(String errorMsg, Throwable cause)",
				"documentation": "/**\n * General exception caused by a remote exception while fetching a chunk.\n */"
			},
			{
				"signature": "public ChunkFetchFailureException(String errorMsg)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Callback for the result of a single chunk result. For a single stream, the callbacks are\n * guaranteed to be called by the same thread in the same order as the requests for chunks were\n * made.\n *\n * Note that if a general stream failure occurs, all outstanding chunk requests may be failed.\n */",
		"name": "org.apache.spark.network.client.ChunkReceivedCallback",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ChunkFetchIntegrationSuite",
			"org.apache.spark.network.shuffle.AppIsolationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Callback for the result of a single\n * {@link org.apache.spark.network.protocol.MergedBlockMetaRequest}.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.network.client.MergedBlockMetaResponseCallback",
		"extends": "org.apache.spark.network.client.BaseResponseCallback",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockStoreClient"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Callback for the result of a single RPC. This will be invoked once with either success or\n * failure.\n */",
		"name": "org.apache.spark.network.client.RpcResponseCallback",
		"extends": "org.apache.spark.network.client.BaseResponseCallback",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.BlockStoreClient",
			"org.apache.spark.network.shuffle.ExternalBlockStoreClient",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Callback for streaming data. Stream data will be offered to the\n * {@link #onData(String, ByteBuffer)} method as it arrives. Once all the stream data is received,\n * {@link #onComplete(String)} will be called.\n * \u003cp\u003e\n * The network library guarantees that a single thread will call these methods at a time, but\n * different call may be made by different threads.\n */",
		"name": "org.apache.spark.network.client.StreamCallback",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.client.StreamCallbackWithID"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.client.StreamCallbackWithID",
		"extends": "org.apache.spark.network.client.StreamCallback",
		"Methods": [
			{
				"signature": "default ByteBuffer getCompletionResponse()",
				"documentation": "/**\n   * Response to return to client upon the completion of a stream. Currently only invoked in\n   * {@link org.apache.spark.network.server.TransportRequestHandler#processStreamUpload}\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interceptor that is registered with the frame decoder to feed stream data to a\n * callback.\n */",
		"name": "org.apache.spark.network.client.StreamInterceptor",
		"extends": "",
		"Methods": [
			{
				"signature": "public StreamInterceptor(\n      MessageHandler\u003cT\u003e handler,\n      String streamId,\n      long byteCount,\n      StreamCallback callback)",
				"documentation": "/**\n * An interceptor that is registered with the frame decoder to feed stream data to a\n * callback.\n */"
			},
			{
				"signature": "@Override\n  public void exceptionCaught(Throwable cause) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelInactive() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void deactivateStream()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean handle(ByteBuf buf) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.util.TransportFrameDecoder.Interceptor"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Client for fetching consecutive chunks of a pre-negotiated stream. This API is intended to allow\n * efficient transfer of a large amount of data, broken up into chunks with size ranging from\n * hundreds of KB to a few MB.\n *\n * Note that while this client deals with the fetching of chunks from a stream (i.e., data plane),\n * the actual setup of the streams is done outside the scope of the transport layer. The convenience\n * method \"sendRPC\" is provided to enable control plane communication between the client and server\n * to perform this setup.\n *\n * For example, a typical workflow might be:\n * client.sendRPC(new OpenFile(\"/foo\")) --\u0026gt; returns StreamId = 100\n * client.fetchChunk(streamId = 100, chunkIndex = 0, callback)\n * client.fetchChunk(streamId = 100, chunkIndex = 1, callback)\n * ...\n * client.sendRPC(new CloseStream(100))\n *\n * Construct an instance of TransportClient using {@link TransportClientFactory}. A single\n * TransportClient may be used for multiple streams, but any given stream must be restricted to a\n * single client, in order to avoid out-of-order responses.\n *\n * NB: This class is used to make requests to the server, while {@link TransportResponseHandler} is\n * responsible for handling responses from the server.\n *\n * Concurrency: thread safe and can be called from multiple threads.\n */",
		"name": "org.apache.spark.network.client.TransportClient",
		"extends": "",
		"Methods": [
			{
				"signature": "public TransportClient(Channel channel, TransportResponseHandler handler)",
				"documentation": "/**\n * Client for fetching consecutive chunks of a pre-negotiated stream. This API is intended to allow\n * efficient transfer of a large amount of data, broken up into chunks with size ranging from\n * hundreds of KB to a few MB.\n *\n * Note that while this client deals with the fetching of chunks from a stream (i.e., data plane),\n * the actual setup of the streams is done outside the scope of the transport layer. The convenience\n * method \"sendRPC\" is provided to enable control plane communication between the client and server\n * to perform this setup.\n *\n * For example, a typical workflow might be:\n * client.sendRPC(new OpenFile(\"/foo\")) --\u0026gt; returns StreamId = 100\n * client.fetchChunk(streamId = 100, chunkIndex = 0, callback)\n * client.fetchChunk(streamId = 100, chunkIndex = 1, callback)\n * ...\n * client.sendRPC(new CloseStream(100))\n *\n * Construct an instance of TransportClient using {@link TransportClientFactory}. A single\n * TransportClient may be used for multiple streams, but any given stream must be restricted to a\n * single client, in order to avoid out-of-order responses.\n *\n * NB: This class is used to make requests to the server, while {@link TransportResponseHandler} is\n * responsible for handling responses from the server.\n *\n * Concurrency: thread safe and can be called from multiple threads.\n */"
			},
			{
				"signature": "public Channel getChannel()",
				"documentation": ""
			},
			{
				"signature": "public boolean isActive()",
				"documentation": ""
			},
			{
				"signature": "public SocketAddress getSocketAddress()",
				"documentation": ""
			},
			{
				"signature": "public String getClientId()",
				"documentation": "/**\n   * Returns the ID used by the client to authenticate itself when authentication is enabled.\n   *\n   * @return The client ID, or null if authentication is disabled.\n   */"
			},
			{
				"signature": "public void setClientId(String id)",
				"documentation": "/**\n   * Sets the authenticated client ID. This is meant to be used by the authentication layer.\n   *\n   * Trying to set a different client ID after it's been set will result in an exception.\n   */"
			},
			{
				"signature": "public void fetchChunk(\n      long streamId,\n      int chunkIndex,\n      ChunkReceivedCallback callback)",
				"documentation": "/**\n   * Requests a single chunk from the remote side, from the pre-negotiated streamId.\n   *\n   * Chunk indices go from 0 onwards. It is valid to request the same chunk multiple times, though\n   * some streams may not support this.\n   *\n   * Multiple fetchChunk requests may be outstanding simultaneously, and the chunks are guaranteed\n   * to be returned in the same order that they were requested, assuming only a single\n   * TransportClient is used to fetch the chunks.\n   *\n   * @param streamId Identifier that refers to a stream in the remote StreamManager. This should\n   *                 be agreed upon by client and server beforehand.\n   * @param chunkIndex 0-based index of the chunk to fetch\n   * @param callback Callback invoked upon successful receipt of chunk, or upon any failure.\n   */"
			},
			{
				"signature": "@Override\n      void handleFailure(String errorMsg, Throwable cause)",
				"documentation": ""
			},
			{
				"signature": "public void stream(String streamId, StreamCallback callback)",
				"documentation": "/**\n   * Request to stream the data with the given stream ID from the remote end.\n   *\n   * @param streamId The stream to fetch.\n   * @param callback Object to call with the stream data.\n   */"
			},
			{
				"signature": "@Override\n      void handleFailure(String errorMsg, Throwable cause) throws Exception",
				"documentation": "/**\n   * Request to stream the data with the given stream ID from the remote end.\n   *\n   * @param streamId The stream to fetch.\n   * @param callback Object to call with the stream data.\n   */"
			},
			{
				"signature": "public long sendRpc(ByteBuffer message, RpcResponseCallback callback)",
				"documentation": "/**\n   * Sends an opaque message to the RpcHandler on the server-side. The callback will be invoked\n   * with the server's response or upon any failure.\n   *\n   * @param message The message to send.\n   * @param callback Callback to handle the RPC's reply.\n   * @return The RPC's id.\n   */"
			},
			{
				"signature": "public void sendMergedBlockMetaReq(\n      String appId,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId,\n      MergedBlockMetaResponseCallback callback)",
				"documentation": "/**\n   * Sends a MergedBlockMetaRequest message to the server. The response of this message is\n   * either a {@link MergedBlockMetaSuccess} or {@link RpcFailure}.\n   *\n   * @param appId applicationId.\n   * @param shuffleId shuffle id.\n   * @param shuffleMergeId shuffleMergeId is used to uniquely identify merging process\n   *                       of shuffle by an indeterminate stage attempt.\n   * @param reduceId reduce id.\n   * @param callback callback the handle the reply.\n   */"
			},
			{
				"signature": "public long uploadStream(\n      ManagedBuffer meta,\n      ManagedBuffer data,\n      RpcResponseCallback callback)",
				"documentation": "/**\n   * Send data to the remote end as a stream.  This differs from stream() in that this is a request\n   * to *send* data to the remote end, not to receive it from the remote.\n   *\n   * @param meta meta data associated with the stream, which will be read completely on the\n   *             receiving end before the stream itself.\n   * @param data this will be streamed to the remote end to allow for transferring large amounts\n   *             of data without reading into memory.\n   * @param callback handles the reply -- onSuccess will only be called when both message and data\n   *                 are received successfully.\n   */"
			},
			{
				"signature": "public ByteBuffer sendRpcSync(ByteBuffer message, long timeoutMs)",
				"documentation": "/**\n   * Synchronously sends an opaque message to the RpcHandler on the server-side, waiting for up to\n   * a specified timeout for a response.\n   */"
			},
			{
				"signature": "public void send(ByteBuffer message)",
				"documentation": "/**\n   * Sends an opaque message to the RpcHandler on the server-side. No reply is expected for the\n   * message, and no delivery guarantees are made.\n   *\n   * @param message The message to send.\n   */"
			},
			{
				"signature": "public void removeRpcRequest(long requestId)",
				"documentation": "/**\n   * Removes any state associated with the given RPC.\n   *\n   * @param requestId The RPC id returned by {@link #sendRpc(ByteBuffer, RpcResponseCallback)}.\n   */"
			},
			{
				"signature": "public void timeOut()",
				"documentation": "/** Mark this channel as having timed out. */"
			},
			{
				"signature": "@VisibleForTesting\n  public TransportResponseHandler getHandler()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "private static long requestId()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.client.StdChannelListener",
			"org.apache.spark.network.client.RpcChannelListener",
			"org.apache.spark.network.util.NettyUtils"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.client.StdChannelListener",
			"org.apache.spark.network.client.RpcChannelListener"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.client.StdChannelListener",
		"extends": "",
		"Methods": [
			{
				"signature": "StdChannelListener(Object requestId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void operationComplete(Future\u003c? super Void\u003e future) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void handleFailure(String errorMsg, Throwable cause) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [
			"io.netty.util.concurrent.GenericFutureListener"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.client.RpcChannelListener"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.NettyUtils"
		],
		"usedBy": [
			"org.apache.spark.network.client.TransportClient"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.client.RpcChannelListener",
		"extends": "org.apache.spark.network.client.StdChannelListener",
		"Methods": [
			{
				"signature": "RpcChannelListener(long rpcRequestId, BaseResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    void handleFailure(String errorMsg, Throwable cause)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.client.TransportClient"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A bootstrap which is executed on a TransportClient before it is returned to the user.\n * This enables an initial exchange of information (e.g., SASL authentication tokens) on a once-per-\n * connection basis.\n *\n * Since connections (and TransportClients) are reused as much as possible, it is generally\n * reasonable to perform an expensive bootstrapping operation, as they often share a lifespan with\n * the JVM itself.\n */",
		"name": "org.apache.spark.network.client.TransportClientBootstrap",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Factory for creating {@link TransportClient}s by using createClient.\n *\n * The factory maintains a connection pool to other hosts and should return the same\n * TransportClient for the same remote host. It also shares a single worker thread pool for\n * all TransportClients.\n *\n * TransportClients will be reused whenever possible. Prior to completing the creation of a new\n * TransportClient, all given {@link TransportClientBootstrap}s will be run.\n */",
		"name": "org.apache.spark.network.client.TransportClientFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "public TransportClientFactory(\n      TransportContext context,\n      List\u003cTransportClientBootstrap\u003e clientBootstraps)",
				"documentation": "/** Random number generator for picking connections between peers. */"
			}
		],
		"interfaces": [
			"java.io.Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.client.TransportClientFactory.ClientPool",
			"org.apache.spark.network.client.TransportResponseHandler",
			"org.apache.spark.network.crypto.AuthClientBootstrap",
			"org.apache.spark.network.crypto.AuthEngine",
			"org.apache.spark.network.crypto.AuthMessage",
			"org.apache.spark.network.crypto.AuthRpcHandler",
			"org.apache.spark.network.crypto.AuthServerBootstrap",
			"org.apache.spark.network.crypto.TransportCipher",
			"org.apache.spark.network.protocol.AbstractMessage",
			"org.apache.spark.network.protocol.AbstractResponseMessage",
			"org.apache.spark.network.protocol.ChunkFetchFailure",
			"org.apache.spark.network.protocol.ChunkFetchRequest",
			"org.apache.spark.network.protocol.ChunkFetchSuccess",
			"org.apache.spark.network.protocol.Encodable",
			"org.apache.spark.network.protocol.Encoders",
			"org.apache.spark.network.protocol.MergedBlockMetaRequest",
			"org.apache.spark.network.protocol.MergedBlockMetaSuccess",
			"org.apache.spark.network.protocol.Message",
			"org.apache.spark.network.protocol.MessageDecoder",
			"org.apache.spark.network.protocol.MessageEncoder",
			"org.apache.spark.network.protocol.MessageWithHeader",
			"org.apache.spark.network.protocol.OneWayMessage",
			"org.apache.spark.network.protocol.RequestMessage",
			"org.apache.spark.network.protocol.ResponseMessage",
			"org.apache.spark.network.protocol.RpcFailure",
			"org.apache.spark.network.protocol.RpcRequest",
			"org.apache.spark.network.protocol.RpcResponse",
			"org.apache.spark.network.protocol.StreamChunkId",
			"org.apache.spark.network.protocol.StreamFailure",
			"org.apache.spark.network.protocol.StreamRequest",
			"org.apache.spark.network.protocol.StreamResponse",
			"org.apache.spark.network.protocol.UploadStream",
			"org.apache.spark.network.sasl.SaslClientBootstrap",
			"org.apache.spark.network.sasl.SaslEncryption",
			"org.apache.spark.network.sasl.SaslEncryptionBackend",
			"org.apache.spark.network.sasl.SaslMessage",
			"org.apache.spark.network.sasl.SaslRpcHandler",
			"org.apache.spark.network.sasl.SaslServerBootstrap",
			"org.apache.spark.network.sasl.SecretKeyHolder",
			"org.apache.spark.network.sasl.SparkSaslClient",
			"org.apache.spark.network.sasl.SparkSaslServer",
			"org.apache.spark.network.server.AbstractAuthRpcHandler",
			"org.apache.spark.network.server.BlockPushNonFatalFailure",
			"org.apache.spark.network.server.ChunkFetchRequestHandler",
			"org.apache.spark.network.server.MessageHandler",
			"org.apache.spark.network.server.NoOpRpcHandler",
			"org.apache.spark.network.server.OneForOneStreamManager",
			"org.apache.spark.network.server.RpcHandler",
			"org.apache.spark.network.server.StreamManager",
			"org.apache.spark.network.server.TransportChannelHandler",
			"org.apache.spark.network.server.TransportRequestHandler",
			"org.apache.spark.network.server.TransportServer"
		]
	},
	{
		"documentation": "/** A simple data structure to track the pool of clients between two peer nodes. */",
		"name": "org.apache.spark.network.client.TransportClientFactory.ClientPool",
		"extends": "",
		"Methods": [
			{
				"signature": "ClientPool(int size)",
				"documentation": "/** A simple data structure to track the pool of clients between two peer nodes. */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Handler that processes server responses, in response to requests issued from a\n * [[TransportClient]]. It works by tracking the list of outstanding requests (and their callbacks).\n *\n * Concurrency: thread safe and can be called from multiple threads.\n */",
		"name": "org.apache.spark.network.client.TransportResponseHandler",
		"extends": "org.apache.spark.network.server.MessageHandler",
		"Methods": [
			{
				"signature": "public TransportResponseHandler(Channel channel)",
				"documentation": "/** Records the time (in system nanoseconds) that the last fetch or RPC request was sent. */"
			},
			{
				"signature": "public void addFetchRequest(StreamChunkId streamChunkId, ChunkReceivedCallback callback)",
				"documentation": ""
			},
			{
				"signature": "public void removeFetchRequest(StreamChunkId streamChunkId)",
				"documentation": ""
			},
			{
				"signature": "public void addRpcRequest(long requestId, BaseResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "public void removeRpcRequest(long requestId)",
				"documentation": ""
			},
			{
				"signature": "public void addStreamCallback(String streamId, StreamCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public void deactivateStream()",
				"documentation": ""
			},
			{
				"signature": "private void failOutstandingRequests(Throwable cause)",
				"documentation": "/**\n   * Fire the failure callback for all outstanding requests. This is called when we have an\n   * uncaught exception or pre-mature connection termination.\n   */"
			},
			{
				"signature": "@Override\n  public void channelActive()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelInactive()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void exceptionCaught(Throwable cause)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void handle(ResponseMessage message) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public int numOutstandingRequests()",
				"documentation": "/** Returns total number of outstanding requests (fetch requests + rpcs) */"
			},
			{
				"signature": "public Boolean hasOutstandingRequests()",
				"documentation": "/** Check if there are any outstanding requests (fetch requests + rpcs) */"
			},
			{
				"signature": "public long getTimeOfLastRequestNs()",
				"documentation": "/** Returns the time in nanoseconds of when the last request was sent out. */"
			},
			{
				"signature": "public void updateTimeOfLastRequest()",
				"documentation": "/** Updates the time of the last request to the current system time. */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.NettyUtils"
		],
		"usedBy": [
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Bootstraps a {@link TransportClient} by performing authentication using Spark's auth protocol.\n *\n * This bootstrap falls back to using the SASL bootstrap if the server throws an error during\n * authentication, and the configuration allows it. This is used for backwards compatibility\n * with external shuffle services that do not support the new protocol.\n *\n * It also automatically falls back to SASL if the new encryption backend is disabled, so that\n * callers only need to install this bootstrap when authentication is enabled.\n */",
		"name": "org.apache.spark.network.crypto.AuthClientBootstrap",
		"extends": "",
		"Methods": [
			{
				"signature": "public AuthClientBootstrap(\n      TransportConf conf,\n      String appId,\n      SecretKeyHolder secretKeyHolder)",
				"documentation": "/**\n * Bootstraps a {@link TransportClient} by performing authentication using Spark's auth protocol.\n *\n * This bootstrap falls back to using the SASL bootstrap if the server throws an error during\n * authentication, and the configuration allows it. This is used for backwards compatibility\n * with external shuffle services that do not support the new protocol.\n *\n * It also automatically falls back to SASL if the new encryption backend is disabled, so that\n * callers only need to install this bootstrap when authentication is enabled.\n */"
			},
			{
				"signature": "@Override\n  public void doBootstrap(TransportClient client, Channel channel)",
				"documentation": ""
			},
			{
				"signature": "private void doSparkAuth(TransportClient client, Channel channel)\n    throws GeneralSecurityException, IOException",
				"documentation": ""
			},
			{
				"signature": "private void doSaslAuth(TransportClient client, Channel channel)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.TransportClientBootstrap"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.sasl.SaslClientBootstrap"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockStoreClient",
			"org.apache.spark.network.shuffle.AppIsolationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A helper class for abstracting authentication and key negotiation details.\n * This supports a forward-secure authentication protocol based on X25519 Diffie-Hellman Key\n * Exchange, using a pre-shared key to derive an AES-GCM key encrypting key.\n */",
		"name": "org.apache.spark.network.crypto.AuthEngine",
		"extends": "",
		"Methods": [
			{
				"signature": "AuthEngine(String appId, String preSharedSecret, TransportConf conf)",
				"documentation": "/**\n * A helper class for abstracting authentication and key negotiation details.\n * This supports a forward-secure authentication protocol based on X25519 Diffie-Hellman Key\n * Exchange, using a pre-shared key to derive an AES-GCM key encrypting key.\n */"
			},
			{
				"signature": "@VisibleForTesting\n  void setClientPrivateKey(byte[] privateKey)",
				"documentation": ""
			},
			{
				"signature": "private AuthMessage encryptEphemeralPublicKey(\n      byte[] ephemeralX25519PublicKey,\n      byte[] transcript) throws GeneralSecurityException",
				"documentation": "/**\n   * This method will derive a key from a pre-shared secret, a random salt, and an arbitrary\n   * transcript. It will then use that derived key to AES-GCM encrypt an ephemeral X25519 public\n   * key.\n   *\n   * @param ephemeralX25519PublicKey Ephemeral X25519 Public Key to encrypt under a derived key.\n   * @param transcript               Optional byte array representing a protocol transcript, which\n   *                                 is mixed into the key derivation and included as AES-GCM\n   *                                 associated authenticated data (AAD).\n   * @return An encrypted ephemeral X25519 public key.\n   * @throws GeneralSecurityException If HKDF key deriviation or AES-GCM encryption fails.\n   */"
			},
			{
				"signature": "private byte[] decryptEphemeralPublicKey(\n      AuthMessage encryptedPublicKey,\n      byte[] transcript) throws GeneralSecurityException",
				"documentation": "/**\n   * This method will derive a key from a pre-shared secret, a random salt, and an arbitrary\n   * transcript. It will then use that derived key to AES-GCM encrypt an ephemeral X25519\n   * public key.\n   *\n   * @param encryptedPublicKey An X25519 public key to decrypt with a derived key\n   * @param transcript         Optional byte array representing a protocol transcript, which is\n   *                           mixed into the key derivation and included as AES-GCM associated\n   *                           authenticated data (AAD).\n   * @return A decrypted ephemeral public key\n   * @throws GeneralSecurityException If decryption fails, notably if authenticated checks fails.\n   */"
			},
			{
				"signature": "AuthMessage challenge() throws GeneralSecurityException",
				"documentation": "/**\n   * Encrypt an ephemeral X25519 public key to be sent to the server as a challenge.\n   *\n   * @return An encrypted client ephemeral public key to be sent to the server.\n   */"
			},
			{
				"signature": "AuthMessage response(AuthMessage encryptedClientPublicKey) throws GeneralSecurityException",
				"documentation": "/**\n   * Validates the client challenge by decrypting the ephemeral X25519 public key, computing a\n   * shared secret from it, then encrypting a server ephemeral X25519 public key for the client.\n   *\n   * @param encryptedClientPublicKey The encrypted public key from the client to be decrypted.\n   * @return An encrypted server ephemeral public key to be sent to the client.\n   */"
			},
			{
				"signature": "void deriveSessionCipher(AuthMessage encryptedClientPublicKey,\n                           AuthMessage encryptedServerPublicKey) throws GeneralSecurityException",
				"documentation": "/**\n   * Validates the server response and initializes the cipher to use for the session.\n   *\n   * @param encryptedClientPublicKey The encrypted ephemeral public key from the client.\n   * @param encryptedServerPublicKey The encrypted ephemeral public key from the server.\n   */"
			},
			{
				"signature": "private TransportCipher generateTransportCipher(\n      byte[] sharedSecret,\n      boolean isClient,\n      byte[] transcript) throws GeneralSecurityException",
				"documentation": ""
			},
			{
				"signature": "private byte[] getTranscript(AuthMessage... encryptedPublicKeys)",
				"documentation": ""
			},
			{
				"signature": "TransportCipher sessionCipher()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A message sent in the forward secure authentication protocol, containing an app ID, a salt for\n * key derivation, and an encrypted payload.\n *\n * Please see crypto/README.md for more details of implementation.\n */",
		"name": "org.apache.spark.network.crypto.AuthMessage",
		"extends": "",
		"Methods": [
			{
				"signature": "AuthMessage(String appId, byte[] salt, byte[] ciphertext)",
				"documentation": "/** Serialization tag used to catch incorrect payloads. */"
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static AuthMessage decodeMessage(ByteBuffer buffer)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.Encodable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings",
			"org.apache.spark.network.protocol.Encoders.ByteArrays"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * RPC Handler which performs authentication using Spark's auth protocol before delegating to a\n * child RPC handler. If the configuration allows, this handler will delegate messages to a SASL\n * RPC handler for further authentication, to support for clients that do not support Spark's\n * protocol.\n *\n * The delegate will only receive messages if the given connection has been successfully\n * authenticated. A connection may be authenticated at most once.\n */",
		"name": "org.apache.spark.network.crypto.AuthRpcHandler",
		"extends": "org.apache.spark.network.server.AbstractAuthRpcHandler",
		"Methods": [
			{
				"signature": "AuthRpcHandler(\n      TransportConf conf,\n      Channel channel,\n      RpcHandler delegate,\n      SecretKeyHolder secretKeyHolder)",
				"documentation": "/** RPC handler for auth handshake when falling back to SASL auth. */"
			},
			{
				"signature": "@Override\n  protected boolean doAuthChallenge(\n      TransportClient client,\n      ByteBuffer message,\n      RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MergedBlockMetaReqHandler getMergedBlockMetaReqHandler()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.sasl.SaslRpcHandler"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A bootstrap which is executed on a TransportServer's client channel once a client connects\n * to the server, enabling authentication using Spark's auth protocol (and optionally SASL for\n * clients that don't support the new protocol).\n *\n * It also automatically falls back to SASL if the new encryption backend is disabled, so that\n * callers only need to install this bootstrap when authentication is enabled.\n */",
		"name": "org.apache.spark.network.crypto.AuthServerBootstrap",
		"extends": "",
		"Methods": [
			{
				"signature": "public AuthServerBootstrap(TransportConf conf, SecretKeyHolder secretKeyHolder)",
				"documentation": "/**\n * A bootstrap which is executed on a TransportServer's client channel once a client connects\n * to the server, enabling authentication using Spark's auth protocol (and optionally SASL for\n * clients that don't support the new protocol).\n *\n * It also automatically falls back to SASL if the new encryption backend is disabled, so that\n * callers only need to install this bootstrap when authentication is enabled.\n */"
			},
			{
				"signature": "public RpcHandler doBootstrap(Channel channel, RpcHandler rpcHandler)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.server.TransportServerBootstrap"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.sasl.SaslServerBootstrap"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.AppIsolationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Cipher for encryption and decryption.\n */",
		"name": "org.apache.spark.network.crypto.TransportCipher",
		"extends": "",
		"Methods": [
			{
				"signature": "public TransportCipher(\n      Properties conf,\n      String cipher,\n      SecretKeySpec key,\n      byte[] inIv,\n      byte[] outIv)",
				"documentation": "/**\n * Cipher for encryption and decryption.\n */"
			},
			{
				"signature": "public String getCipherTransformation()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  SecretKeySpec getKey()",
				"documentation": ""
			},
			{
				"signature": "public byte[] getInputIv()",
				"documentation": "/** The IV for the input channel (i.e. output channel of the remote side). */"
			},
			{
				"signature": "public byte[] getOutputIv()",
				"documentation": "/** The IV for the output channel (i.e. input channel of the remote side). */"
			},
			{
				"signature": "@VisibleForTesting\n  CryptoOutputStream createOutputStream(WritableByteChannel ch) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  CryptoInputStream createInputStream(ReadableByteChannel ch) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void addToChannel(Channel ch) throws IOException",
				"documentation": "/**\n   * Add handlers to channel.\n   *\n   * @param ch the channel for adding handlers\n   * @throws IOException\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.crypto.TransportCipher.EncryptionHandler",
			"org.apache.spark.network.crypto.TransportCipher.DecryptionHandler",
			"org.apache.spark.network.crypto.TransportCipher.EncryptedMessage",
			"org.apache.spark.network.util.ByteArrayReadableChannel",
			"org.apache.spark.network.util.ByteArrayWritableChannel"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.crypto.TransportCipher.EncryptionHandler",
			"org.apache.spark.network.crypto.TransportCipher.DecryptionHandler",
			"org.apache.spark.network.crypto.TransportCipher.EncryptedMessage"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.crypto.TransportCipher.EncryptionHandler",
		"extends": "ChannelOutboundHandlerAdapter",
		"Methods": [
			{
				"signature": "EncryptionHandler(TransportCipher cipher) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n      throws Exception",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    EncryptedMessage createEncryptedMessage(Object msg)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void reportError()",
				"documentation": "/**\n     * SPARK-25535. Workaround for CRYPTO-141. Avoid further interaction with the underlying cipher\n     * after an error occurs.\n     */"
			},
			{
				"signature": "boolean isCipherValid()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.ByteArrayWritableChannel"
		],
		"usedBy": [
			"org.apache.spark.network.crypto.TransportCipher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.crypto.TransportCipher.DecryptionHandler",
		"extends": "ChannelInboundHandlerAdapter",
		"Methods": [
			{
				"signature": "DecryptionHandler(TransportCipher cipher) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void channelRead(ChannelHandlerContext ctx, Object data) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.ByteArrayReadableChannel"
		],
		"usedBy": [
			"org.apache.spark.network.crypto.TransportCipher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.crypto.TransportCipher.EncryptedMessage",
		"extends": "org.apache.spark.network.util.AbstractFileRegion",
		"Methods": [
			{
				"signature": "EncryptedMessage(\n        EncryptionHandler handler,\n        CryptoOutputStream cos,\n        Object msg,\n        ByteArrayWritableChannel ch)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long count()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long position()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long transferred()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public EncryptedMessage touch(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public EncryptedMessage retain(int increment)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean release(int decrement)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long transferTo(WritableByteChannel target, long position) throws IOException",
				"documentation": ""
			},
			{
				"signature": "do",
				"documentation": ""
			},
			{
				"signature": "private void encryptMore() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void deallocate()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.ByteArrayWritableChannel"
		],
		"usedBy": [
			"org.apache.spark.network.crypto.TransportCipher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Abstract class for messages which optionally contain a body kept in a separate buffer.\n */",
		"name": "org.apache.spark.network.protocol.AbstractMessage",
		"extends": "",
		"Methods": [
			{
				"signature": "protected AbstractMessage()",
				"documentation": "/**\n * Abstract class for messages which optionally contain a body kept in a separate buffer.\n */"
			},
			{
				"signature": "protected AbstractMessage(ManagedBuffer body, boolean isBodyInFrame)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer body()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isBodyInFrame()",
				"documentation": ""
			},
			{
				"signature": "protected boolean equals(AbstractMessage other)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.Message"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.protocol.AbstractResponseMessage",
			"org.apache.spark.network.protocol.ChunkFetchFailure",
			"org.apache.spark.network.protocol.ChunkFetchRequest",
			"org.apache.spark.network.protocol.MergedBlockMetaRequest",
			"org.apache.spark.network.protocol.OneWayMessage",
			"org.apache.spark.network.protocol.RpcFailure",
			"org.apache.spark.network.protocol.RpcRequest",
			"org.apache.spark.network.protocol.StreamFailure",
			"org.apache.spark.network.protocol.StreamRequest",
			"org.apache.spark.network.protocol.UploadStream"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Abstract class for response messages.\n */",
		"name": "org.apache.spark.network.protocol.AbstractResponseMessage",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "protected AbstractResponseMessage(ManagedBuffer body, boolean isBodyInFrame)",
				"documentation": "/**\n * Abstract class for response messages.\n */"
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.ResponseMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.protocol.ChunkFetchSuccess",
			"org.apache.spark.network.protocol.MergedBlockMetaSuccess",
			"org.apache.spark.network.protocol.RpcResponse",
			"org.apache.spark.network.protocol.StreamResponse"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Response to {@link ChunkFetchRequest} when there is an error fetching the chunk.\n */",
		"name": "org.apache.spark.network.protocol.ChunkFetchFailure",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public ChunkFetchFailure(StreamChunkId streamChunkId, String errorString)",
				"documentation": "/**\n * Response to {@link ChunkFetchRequest} when there is an error fetching the chunk.\n */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static ChunkFetchFailure decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.ResponseMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.server.ChunkFetchRequestHandler",
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Request to fetch a sequence of a single chunk of a stream. This will correspond to a single\n * {@link org.apache.spark.network.protocol.ResponseMessage} (either success or failure).\n */",
		"name": "org.apache.spark.network.protocol.ChunkFetchRequest",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public ChunkFetchRequest(StreamChunkId streamChunkId)",
				"documentation": "/**\n * Request to fetch a sequence of a single chunk of a stream. This will correspond to a single\n * {@link org.apache.spark.network.protocol.ResponseMessage} (either success or failure).\n */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static ChunkFetchRequest decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.RequestMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Response to {@link ChunkFetchRequest} when a chunk exists and has been successfully fetched.\n *\n * Note that the server-side encoding of this messages does NOT include the buffer itself, as this\n * may be written by Netty in a more efficient manner (i.e., zero-copy write).\n * Similarly, the client-side decoding will reuse the Netty ByteBuf as the buffer.\n */",
		"name": "org.apache.spark.network.protocol.ChunkFetchSuccess",
		"extends": "org.apache.spark.network.protocol.AbstractResponseMessage",
		"Methods": [
			{
				"signature": "public ChunkFetchSuccess(StreamChunkId streamChunkId, ManagedBuffer buffer)",
				"documentation": "/**\n * Response to {@link ChunkFetchRequest} when a chunk exists and has been successfully fetched.\n *\n * Note that the server-side encoding of this messages does NOT include the buffer itself, as this\n * may be written by Netty in a more efficient manner (i.e., zero-copy write).\n * Similarly, the client-side decoding will reuse the Netty ByteBuf as the buffer.\n */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": "/** Encoding does NOT include 'buffer' itself. See {@link MessageEncoder}. */"
			},
			{
				"signature": "@Override\n  public ResponseMessage createFailureResponse(String error)",
				"documentation": ""
			},
			{
				"signature": "public static ChunkFetchSuccess decode(ByteBuf buf)",
				"documentation": "/** Decoding uses the given ByteBuf as our data, and will retain() it. */"
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer"
		],
		"usedBy": [
			"org.apache.spark.network.server.ChunkFetchRequestHandler",
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface for an object which can be encoded into a ByteBuf. Multiple Encodable objects are\n * stored in a single, pre-allocated ByteBuf, so Encodables must also provide their length.\n *\n * Encodable objects should provide a static \"decode(ByteBuf)\" method which is invoked by\n * {@link MessageDecoder}. During decoding, if the object uses the ByteBuf as its data (rather than\n * just copying data from it), then you must retain() the ByteBuf.\n *\n * Additionally, when adding a new Encodable Message, add it to {@link Message.Type}.\n */",
		"name": "org.apache.spark.network.protocol.Encodable",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.protocol.Message"
		],
		"implementedBy": [
			"org.apache.spark.network.protocol.Type",
			"org.apache.spark.network.protocol.StreamChunkId"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Provides a canonical set of Encoders for simple types. */",
		"name": "org.apache.spark.network.protocol.Encoders",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings",
			"org.apache.spark.network.protocol.Encoders.Bitmaps"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.protocol.Encoders.Strings",
			"org.apache.spark.network.protocol.Encoders.Bitmaps",
			"org.apache.spark.network.protocol.Encoders.ByteArrays",
			"org.apache.spark.network.protocol.Encoders.StringArrays",
			"org.apache.spark.network.protocol.Encoders.IntArrays",
			"org.apache.spark.network.protocol.Encoders.LongArrays",
			"org.apache.spark.network.protocol.Encoders.BitmapArrays"
		]
	},
	{
		"documentation": "/** Strings are encoded with their length followed by UTF-8 bytes. */",
		"name": "org.apache.spark.network.protocol.Encoders.Strings",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int encodedLength(String s)",
				"documentation": "/** Strings are encoded with their length followed by UTF-8 bytes. */"
			},
			{
				"signature": "public static void encode(ByteBuf buf, String s)",
				"documentation": ""
			},
			{
				"signature": "public static String decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.crypto.AuthMessage",
			"org.apache.spark.network.protocol.Encoders",
			"org.apache.spark.network.sasl.SaslMessage",
			"org.apache.spark.network.shuffle.protocol.AbstractFetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.BlockPushReturnCode",
			"org.apache.spark.network.shuffle.protocol.DiagnoseCorruption",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunks",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.FinalizeShuffleMerge",
			"org.apache.spark.network.shuffle.protocol.GetLocalDirsForExecutors",
			"org.apache.spark.network.shuffle.protocol.OpenBlocks",
			"org.apache.spark.network.shuffle.protocol.PushBlockStream",
			"org.apache.spark.network.shuffle.protocol.RegisterExecutor",
			"org.apache.spark.network.shuffle.protocol.RemoveBlocks",
			"org.apache.spark.network.shuffle.protocol.UploadBlock",
			"org.apache.spark.network.shuffle.protocol.UploadBlockStream",
			"org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver",
			"org.apache.spark.network.shuffle.protocol.mesos.ShuffleServiceHeartbeat"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Bitmaps are encoded with their serialization length followed by the serialization bytes.\n   *\n   * @since 3.1.0\n   */",
		"name": "org.apache.spark.network.protocol.Encoders.Bitmaps",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int encodedLength(RoaringBitmap b)",
				"documentation": "/**\n   * Bitmaps are encoded with their serialization length followed by the serialization bytes.\n   *\n   * @since 3.1.0\n   */"
			},
			{
				"signature": "public static void encode(ByteBuf buf, RoaringBitmap b)",
				"documentation": "/**\n     * The input ByteBuf for this encoder should have enough write capacity to fit the serialized\n     * bitmap. Other encoders which use {@link io.netty.buffer.AbstractByteBuf#writeBytes(byte[])}\n     * to write can expand the buf as writeBytes calls {@link ByteBuf#ensureWritable} internally.\n     * However, this encoder doesn't rely on netty's writeBytes and will fail if the input buf\n     * doesn't have enough write capacity.\n     */"
			},
			{
				"signature": "public static RoaringBitmap decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.protocol.Encoders"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Byte arrays are encoded with their length followed by bytes. */",
		"name": "org.apache.spark.network.protocol.Encoders.ByteArrays",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int encodedLength(byte[] arr)",
				"documentation": "/** Byte arrays are encoded with their length followed by bytes. */"
			},
			{
				"signature": "public static void encode(ByteBuf buf, byte[] arr)",
				"documentation": ""
			},
			{
				"signature": "public static byte[] decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.crypto.AuthMessage",
			"org.apache.spark.network.shuffle.protocol.UploadBlock"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** String arrays are encoded with the number of strings followed by per-String encoding. */",
		"name": "org.apache.spark.network.protocol.Encoders.StringArrays",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int encodedLength(String[] strings)",
				"documentation": "/** String arrays are encoded with the number of strings followed by per-String encoding. */"
			},
			{
				"signature": "public static void encode(ByteBuf buf, String[] strings)",
				"documentation": ""
			},
			{
				"signature": "public static String[] decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo",
			"org.apache.spark.network.shuffle.protocol.LocalDirsForExecutors",
			"org.apache.spark.network.shuffle.protocol.OpenBlocks",
			"org.apache.spark.network.shuffle.protocol.RemoveBlocks"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Integer arrays are encoded with their length followed by integers. */",
		"name": "org.apache.spark.network.protocol.Encoders.IntArrays",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int encodedLength(int[] ints)",
				"documentation": "/** Integer arrays are encoded with their length followed by integers. */"
			},
			{
				"signature": "public static void encode(ByteBuf buf, int[] ints)",
				"documentation": ""
			},
			{
				"signature": "public static int[] decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunks",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.LocalDirsForExecutors",
			"org.apache.spark.network.shuffle.protocol.MergeStatuses"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Long integer arrays are encoded with their length followed by long integers. */",
		"name": "org.apache.spark.network.protocol.Encoders.LongArrays",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int encodedLength(long[] longs)",
				"documentation": "/** Long integer arrays are encoded with their length followed by long integers. */"
			},
			{
				"signature": "public static void encode(ByteBuf buf, long[] longs)",
				"documentation": ""
			},
			{
				"signature": "public static long[] decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.MergeStatuses"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Bitmap arrays are encoded with the number of bitmaps followed by per-Bitmap encoding.\n   *\n   * @since 3.1.0\n   */",
		"name": "org.apache.spark.network.protocol.Encoders.BitmapArrays",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int encodedLength(RoaringBitmap[] bitmaps)",
				"documentation": "/**\n   * Bitmap arrays are encoded with the number of bitmaps followed by per-Bitmap encoding.\n   *\n   * @since 3.1.0\n   */"
			},
			{
				"signature": "public static void encode(ByteBuf buf, RoaringBitmap[] bitmaps)",
				"documentation": ""
			},
			{
				"signature": "public static RoaringBitmap[] decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.protocol.MergeStatuses"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Request to find the meta information for the specified merged block. The meta information\n * contains the number of chunks in the merged blocks and the maps ids in each chunk.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.network.protocol.MergedBlockMetaRequest",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public MergedBlockMetaRequest(\n      long requestId,\n      String appId,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId)",
				"documentation": "/**\n * Request to find the meta information for the specified merged block. The meta information\n * contains the number of chunks in the merged blocks and the maps ids in each chunk.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "@Override\n  public Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static MergedBlockMetaRequest decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.RequestMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Response to {@link MergedBlockMetaRequest} request.\n * Note that the server-side encoding of this messages does NOT include the buffer itself.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.network.protocol.MergedBlockMetaSuccess",
		"extends": "org.apache.spark.network.protocol.AbstractResponseMessage",
		"Methods": [
			{
				"signature": "public MergedBlockMetaSuccess(\n      long requestId,\n      int numChunks,\n      ManagedBuffer chunkBitmapsBuffer)",
				"documentation": "/**\n * Response to {@link MergedBlockMetaRequest} request.\n * Note that the server-side encoding of this messages does NOT include the buffer itself.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "@Override\n  public Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": "/** Encoding does NOT include 'buffer' itself. See {@link MessageEncoder}. */"
			},
			{
				"signature": "public int getNumChunks()",
				"documentation": ""
			},
			{
				"signature": "public static MergedBlockMetaSuccess decode(ByteBuf buf)",
				"documentation": "/** Decoding uses the given ByteBuf as our data, and will retain() it. */"
			},
			{
				"signature": "@Override\n  public ResponseMessage createFailureResponse(String error)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer"
		],
		"usedBy": [
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** An on-the-wire transmittable message. */",
		"name": "org.apache.spark.network.protocol.Message",
		"extends": "org.apache.spark.network.protocol.Encodable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.protocol.RequestMessage",
			"org.apache.spark.network.protocol.ResponseMessage"
		],
		"implementedBy": [
			"org.apache.spark.network.protocol.AbstractMessage"
		],
		"uses": [
			"org.apache.spark.network.protocol.Type"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.protocol.Type"
		]
	},
	{
		"documentation": "/** Preceding every serialized Message is its type, which allows us to deserialize it. */",
		"name": "org.apache.spark.network.protocol.Type",
		"extends": "",
		"Methods": [
			{
				"signature": "Type(int id)",
				"documentation": "/** Preceding every serialized Message is its type, which allows us to deserialize it. */"
			},
			{
				"signature": "public byte id()",
				"documentation": ""
			},
			{
				"signature": "@Override public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static Type decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.Encodable"
		],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.protocol.Message"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Decoder used by the client side to encode server-to-client responses.\n * This encoder is stateless so it is safe to be shared by multiple threads.\n */",
		"name": "org.apache.spark.network.protocol.MessageDecoder",
		"extends": "io.netty.handler.codec.MessageToMessageDecoder",
		"Methods": [
			{
				"signature": "private MessageDecoder()",
				"documentation": "/**\n * Decoder used by the client side to encode server-to-client responses.\n * This encoder is stateless so it is safe to be shared by multiple threads.\n */"
			},
			{
				"signature": "@Override\n  public void decode(ChannelHandlerContext ctx, ByteBuf in, List\u003cObject\u003e out)",
				"documentation": ""
			},
			{
				"signature": "private Message decode(Message.Type msgType, ByteBuf in)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Encoder used by the server side to encode server-to-client responses.\n * This encoder is stateless so it is safe to be shared by multiple threads.\n */",
		"name": "org.apache.spark.network.protocol.MessageEncoder",
		"extends": "io.netty.handler.codec.MessageToMessageEncoder",
		"Methods": [
			{
				"signature": "private MessageEncoder()",
				"documentation": "/**\n * Encoder used by the server side to encode server-to-client responses.\n * This encoder is stateless so it is safe to be shared by multiple threads.\n */"
			},
			{
				"signature": "@Override\n  public void encode(ChannelHandlerContext ctx, Message in, List\u003cObject\u003e out) throws Exception",
				"documentation": "/***\n   * Encodes a Message by invoking its encode() method. For non-data messages, we will add one\n   * ByteBuf to 'out' containing the total frame length, the message type, and the message itself.\n   * In the case of a ChunkFetchSuccess, we will also add the ManagedBuffer corresponding to the\n   * data to 'out', in order to enable zero-copy transfer.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A wrapper message that holds two separate pieces (a header and a body).\n *\n * The header must be a ByteBuf, while the body can be a ByteBuf or a FileRegion.\n */",
		"name": "org.apache.spark.network.protocol.MessageWithHeader",
		"extends": "org.apache.spark.network.util.AbstractFileRegion",
		"Methods": [
			{
				"signature": "MessageWithHeader(\n      @Nullable ManagedBuffer managedBuffer,\n      ByteBuf header,\n      Object body,\n      long bodyLength)",
				"documentation": "/**\n   * Construct a new MessageWithHeader.\n   *\n   * @param managedBuffer the {@link ManagedBuffer} that the message body came from. This needs to\n   *                      be passed in so that the buffer can be freed when this message is\n   *                      deallocated. Ownership of the caller's reference to this buffer is\n   *                      transferred to this class, so if the caller wants to continue to use the\n   *                      ManagedBuffer in other messages then they will need to call retain() on\n   *                      it before passing it to this constructor. This may be null if and only if\n   *                      `body` is a {@link FileRegion}.\n   * @param header the message header.\n   * @param body the message body. Must be either a {@link ByteBuf} or a {@link FileRegion}.\n   * @param bodyLength the length of the message body, in bytes.\n     */"
			},
			{
				"signature": "@Override\n  public long count()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long position()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long transferred()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long transferTo(final WritableByteChannel target, final long position) throws IOException",
				"documentation": "/**\n   * This code is more complicated than you would think because we might require multiple\n   * transferTo invocations in order to transfer a single MessageWithHeader to avoid busy waiting.\n   *\n   * The contract is that the caller will ensure position is properly set to the total number\n   * of bytes transferred so far (i.e. value returned by transferred()).\n   */"
			},
			{
				"signature": "@Override\n  protected void deallocate()",
				"documentation": ""
			},
			{
				"signature": "private int copyByteBuf(ByteBuf buf, WritableByteChannel target) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MessageWithHeader touch(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MessageWithHeader retain(int increment)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean release(int decrement)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A RPC that does not expect a reply, which is handled by a remote\n * {@link org.apache.spark.network.server.RpcHandler}.\n */",
		"name": "org.apache.spark.network.protocol.OneWayMessage",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public OneWayMessage(ManagedBuffer body)",
				"documentation": "/**\n * A RPC that does not expect a reply, which is handled by a remote\n * {@link org.apache.spark.network.server.RpcHandler}.\n */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static OneWayMessage decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.RequestMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer"
		],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Messages from the client to the server. */",
		"name": "org.apache.spark.network.protocol.RequestMessage",
		"extends": "org.apache.spark.network.protocol.Message",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.protocol.ChunkFetchRequest",
			"org.apache.spark.network.protocol.MergedBlockMetaRequest",
			"org.apache.spark.network.protocol.OneWayMessage",
			"org.apache.spark.network.protocol.RpcRequest",
			"org.apache.spark.network.protocol.StreamRequest",
			"org.apache.spark.network.protocol.UploadStream"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Messages from the server to the client. */",
		"name": "org.apache.spark.network.protocol.ResponseMessage",
		"extends": "org.apache.spark.network.protocol.Message",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.protocol.AbstractResponseMessage",
			"org.apache.spark.network.protocol.ChunkFetchFailure",
			"org.apache.spark.network.protocol.RpcFailure",
			"org.apache.spark.network.protocol.StreamFailure"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Response to {@link RpcRequest} for a failed RPC. */",
		"name": "org.apache.spark.network.protocol.RpcFailure",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public RpcFailure(long requestId, String errorString)",
				"documentation": "/** Response to {@link RpcRequest} for a failed RPC. */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static RpcFailure decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n   public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.ResponseMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A generic RPC which is handled by a remote {@link org.apache.spark.network.server.RpcHandler}.\n * This will correspond to a single\n * {@link org.apache.spark.network.protocol.ResponseMessage} (either success or failure).\n */",
		"name": "org.apache.spark.network.protocol.RpcRequest",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public RpcRequest(long requestId, ManagedBuffer message)",
				"documentation": "/** Used to link an RPC request with its response. */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static RpcRequest decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.RequestMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer"
		],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Response to {@link RpcRequest} for a successful RPC. */",
		"name": "org.apache.spark.network.protocol.RpcResponse",
		"extends": "org.apache.spark.network.protocol.AbstractResponseMessage",
		"Methods": [
			{
				"signature": "public RpcResponse(long requestId, ManagedBuffer message)",
				"documentation": "/** Response to {@link RpcRequest} for a successful RPC. */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ResponseMessage createFailureResponse(String error)",
				"documentation": ""
			},
			{
				"signature": "public static RpcResponse decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer"
		],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n* Encapsulates a request for a particular chunk of a stream.\n*/",
		"name": "org.apache.spark.network.protocol.StreamChunkId",
		"extends": "",
		"Methods": [
			{
				"signature": "public StreamChunkId(long streamId, int chunkIndex)",
				"documentation": "/**\n* Encapsulates a request for a particular chunk of a stream.\n*/"
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "public void encode(ByteBuf buffer)",
				"documentation": ""
			},
			{
				"signature": "public static StreamChunkId decode(ByteBuf buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.Encodable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Message indicating an error when transferring a stream.\n */",
		"name": "org.apache.spark.network.protocol.StreamFailure",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public StreamFailure(String streamId, String error)",
				"documentation": "/**\n * Message indicating an error when transferring a stream.\n */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static StreamFailure decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.ResponseMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Request to stream data from the remote end.\n * \u003cp\u003e\n * The stream ID is an arbitrary string that needs to be negotiated between the two endpoints before\n * the data can be streamed.\n */",
		"name": "org.apache.spark.network.protocol.StreamRequest",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public StreamRequest(String streamId)",
				"documentation": "/**\n * Request to stream data from the remote end.\n * \u003cp\u003e\n * The stream ID is an arbitrary string that needs to be negotiated between the two endpoints before\n * the data can be streamed.\n */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static StreamRequest decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.RequestMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Response to {@link StreamRequest} when the stream has been successfully opened.\n * \u003cp\u003e\n * Note the message itself does not contain the stream data. That is written separately by the\n * sender. The receiver is expected to set a temporary channel handler that will consume the\n * number of bytes this message says the stream has.\n */",
		"name": "org.apache.spark.network.protocol.StreamResponse",
		"extends": "org.apache.spark.network.protocol.AbstractResponseMessage",
		"Methods": [
			{
				"signature": "public StreamResponse(String streamId, long byteCount, ManagedBuffer buffer)",
				"documentation": "/**\n * Response to {@link StreamRequest} when the stream has been successfully opened.\n * \u003cp\u003e\n * Note the message itself does not contain the stream data. That is written separately by the\n * sender. The receiver is expected to set a temporary channel handler that will consume the\n * number of bytes this message says the stream has.\n */"
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": "/** Encoding does NOT include 'buffer' itself. See {@link MessageEncoder}. */"
			},
			{
				"signature": "@Override\n  public ResponseMessage createFailureResponse(String error)",
				"documentation": ""
			},
			{
				"signature": "public static StreamResponse decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An RPC with data that is sent outside of the frame, so it can be read as a stream.\n */",
		"name": "org.apache.spark.network.protocol.UploadStream",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "public UploadStream(long requestId, ManagedBuffer meta, ManagedBuffer body)",
				"documentation": "/** Used to link an RPC request with its response. */"
			},
			{
				"signature": "private UploadStream(long requestId, ManagedBuffer meta, long bodyByteCount)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static UploadStream decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.RequestMessage"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Bootstraps a {@link TransportClient} by performing SASL authentication on the connection. The\n * server should be setup with a {@link SaslRpcHandler} with matching keys for the given appId.\n */",
		"name": "org.apache.spark.network.sasl.SaslClientBootstrap",
		"extends": "",
		"Methods": [
			{
				"signature": "public SaslClientBootstrap(TransportConf conf, String appId, SecretKeyHolder secretKeyHolder)",
				"documentation": "/**\n * Bootstraps a {@link TransportClient} by performing SASL authentication on the connection. The\n * server should be setup with a {@link SaslRpcHandler} with matching keys for the given appId.\n */"
			},
			{
				"signature": "@Override\n  public void doBootstrap(TransportClient client, Channel channel)",
				"documentation": "/**\n   * Performs SASL authentication by sending a token, and then proceeding with the SASL\n   * challenge-response tokens until we either successfully authenticate or throw an exception\n   * due to mismatch.\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.TransportClientBootstrap"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.crypto.AuthClientBootstrap",
			"org.apache.spark.network.shuffle.AppIsolationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Provides SASL-based encryption for transport channels. The single method exposed by this\n * class installs the needed channel handlers on a connected channel.\n */",
		"name": "org.apache.spark.network.sasl.SaslEncryption",
		"extends": "",
		"Methods": [
			{
				"signature": "static void addToChannel(\n      Channel channel,\n      SaslEncryptionBackend backend,\n      int maxOutboundBlockSize)",
				"documentation": "/**\n   * Adds channel handlers that perform encryption / decryption of data using SASL.\n   *\n   * @param channel The channel.\n   * @param backend The SASL backend.\n   * @param maxOutboundBlockSize Max size in bytes of outgoing encrypted blocks, to control\n   *                             memory usage.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.sasl.SaslEncryption.EncryptionHandler",
			"org.apache.spark.network.sasl.SaslEncryption.DecryptionHandler",
			"org.apache.spark.network.sasl.SaslEncryption.EncryptedMessage",
			"org.apache.spark.network.util.ByteArrayWritableChannel",
			"org.apache.spark.network.util.NettyUtils"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.network.sasl.SaslEncryption.EncryptionHandler",
			"org.apache.spark.network.sasl.SaslEncryption.DecryptionHandler",
			"org.apache.spark.network.sasl.SaslEncryption.EncryptedMessage"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.SaslEncryption.EncryptionHandler",
		"extends": "io.netty.channel.ChannelOutboundHandlerAdapter",
		"Methods": [
			{
				"signature": "EncryptionHandler(SaslEncryptionBackend backend, int maxOutboundBlockSize)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n      throws Exception",
				"documentation": "/**\n     * Wrap the incoming message in an implementation that will perform encryption lazily. This is\n     * needed to guarantee ordering of the outgoing encrypted packets - they need to be decrypted in\n     * the same order, and netty doesn't have an atomic ChannelHandlerContext.write() API, so it\n     * does not guarantee any ordering.\n     */"
			},
			{
				"signature": "@Override\n    public void handlerRemoved(ChannelHandlerContext ctx) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.sasl.SaslEncryption"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.SaslEncryption.DecryptionHandler",
		"extends": "io.netty.handler.codec.MessageToMessageDecoder",
		"Methods": [
			{
				"signature": "DecryptionHandler(SaslEncryptionBackend backend)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List\u003cObject\u003e out)\n      throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.sasl.SaslEncryption"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.SaslEncryption.EncryptedMessage",
		"extends": "org.apache.spark.network.util.AbstractFileRegion",
		"Methods": [
			{
				"signature": "EncryptedMessage(SaslEncryptionBackend backend, Object msg, int maxOutboundBlockSize)",
				"documentation": "/**\n     * A channel used to buffer input data for encryption. The channel has an upper size bound\n     * so that if the input is larger than the allowed buffer, it will be broken into multiple\n     * chunks. Made non-final to enable lazy initialization, which saves memory.\n     */"
			},
			{
				"signature": "@Override\n    public long count()",
				"documentation": "/**\n     * Returns the size of the original (unencrypted) message.\n     *\n     * This makes assumptions about how netty treats FileRegion instances, because there's no way\n     * to know beforehand what will be the size of the encrypted message. Namely, it assumes\n     * that netty will try to transfer data from this message while\n     * \u003ccode\u003etransferred() \u003c count()\u003c/code\u003e. So these two methods return, technically, wrong data,\n     * but netty doesn't know better.\n     */"
			},
			{
				"signature": "@Override\n    public long position()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long transferred()",
				"documentation": "/**\n     * Returns an approximation of the amount of data transferred. See {@link #count()}.\n     */"
			},
			{
				"signature": "@Override\n    public EncryptedMessage touch(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public EncryptedMessage retain(int increment)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean release(int decrement)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long transferTo(final WritableByteChannel target, final long position)\n      throws IOException",
				"documentation": "/**\n     * Transfers data from the original message to the channel, encrypting it in the process.\n     *\n     * This method also breaks down the original message into smaller chunks when needed. This\n     * is done to keep memory usage under control. This avoids having to copy the whole message\n     * data into memory at once, and can avoid ballooning memory usage when transferring large\n     * messages such as shuffle blocks.\n     *\n     * The {@link #transferred()} counter also behaves a little funny, in that it won't go forward\n     * until a whole chunk has been written. This is done because the code can't use the actual\n     * number of bytes written to the channel as the transferred count (see {@link #count()}).\n     * Instead, once an encrypted chunk is written to the output (including its header), the\n     * size of the original block will be added to the {@link #transferred()} amount.\n     */"
			},
			{
				"signature": "do",
				"documentation": "/**\n     * Transfers data from the original message to the channel, encrypting it in the process.\n     *\n     * This method also breaks down the original message into smaller chunks when needed. This\n     * is done to keep memory usage under control. This avoids having to copy the whole message\n     * data into memory at once, and can avoid ballooning memory usage when transferring large\n     * messages such as shuffle blocks.\n     *\n     * The {@link #transferred()} counter also behaves a little funny, in that it won't go forward\n     * until a whole chunk has been written. This is done because the code can't use the actual\n     * number of bytes written to the channel as the transferred count (see {@link #count()}).\n     * Instead, once an encrypted chunk is written to the output (including its header), the\n     * size of the original block will be added to the {@link #transferred()} amount.\n     */"
			},
			{
				"signature": "private void nextChunk() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void deallocate()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.ByteArrayWritableChannel"
		],
		"usedBy": [
			"org.apache.spark.network.sasl.SaslEncryption"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.SaslEncryptionBackend",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.sasl.SparkSaslClient",
			"org.apache.spark.network.sasl.SparkSaslServer"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Encodes a Sasl-related message which is attempting to authenticate using some credentials tagged\n * with the given appId. This appId allows a single SaslRpcHandler to multiplex different\n * applications which may be using different sets of credentials.\n */",
		"name": "org.apache.spark.network.sasl.SaslMessage",
		"extends": "org.apache.spark.network.protocol.AbstractMessage",
		"Methods": [
			{
				"signature": "SaslMessage(String appId, byte[] message)",
				"documentation": "/** Serialization tag used to catch incorrect payloads. */"
			},
			{
				"signature": "SaslMessage(String appId, ByteBuf message)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Message.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static SaslMessage decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer",
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * RPC Handler which performs SASL authentication before delegating to a child RPC handler.\n * The delegate will only receive messages if the given connection has been successfully\n * authenticated. A connection may be authenticated at most once.\n *\n * Note that the authentication process consists of multiple challenge-response pairs, each of\n * which are individual RPCs.\n */",
		"name": "org.apache.spark.network.sasl.SaslRpcHandler",
		"extends": "org.apache.spark.network.server.AbstractAuthRpcHandler",
		"Methods": [
			{
				"signature": "public SaslRpcHandler(\n      TransportConf conf,\n      Channel channel,\n      RpcHandler delegate,\n      SecretKeyHolder secretKeyHolder)",
				"documentation": "/** Class which provides secret keys which are shared by server and client on a per-app basis. */"
			},
			{
				"signature": "@Override\n  public boolean doAuthChallenge(\n      TransportClient client,\n      ByteBuffer message,\n      RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelInactive(TransportClient client)",
				"documentation": ""
			},
			{
				"signature": "private void complete(boolean dispose)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.crypto.AuthRpcHandler"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A bootstrap which is executed on a TransportServer's client channel once a client connects\n * to the server. This allows customizing the client channel to allow for things such as SASL\n * authentication.\n */",
		"name": "org.apache.spark.network.sasl.SaslServerBootstrap",
		"extends": "",
		"Methods": [
			{
				"signature": "public SaslServerBootstrap(TransportConf conf, SecretKeyHolder secretKeyHolder)",
				"documentation": "/**\n * A bootstrap which is executed on a TransportServer's client channel once a client connects\n * to the server. This allows customizing the client channel to allow for things such as SASL\n * authentication.\n */"
			},
			{
				"signature": "public RpcHandler doBootstrap(Channel channel, RpcHandler rpcHandler)",
				"documentation": "/**\n   * Wrap the given application handler in a SaslRpcHandler that will handle the initial SASL\n   * negotiation.\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.network.server.TransportServerBootstrap"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.crypto.AuthIntegrationSuite.AuthTestCtx"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.crypto.AuthServerBootstrap",
			"org.apache.spark.network.crypto.AuthIntegrationSuite",
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface for getting a secret key associated with some application.\n */",
		"name": "org.apache.spark.network.sasl.SecretKeyHolder",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.sasl.ShuffleSecretManager"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A SASL Client for Spark which simply keeps track of the state of a single SASL session, from the\n * initial state to the \"authenticated\" state. This client initializes the protocol via a\n * firstToken, which is then followed by a set of challenges and responses.\n */",
		"name": "org.apache.spark.network.sasl.SparkSaslClient",
		"extends": "",
		"Methods": [
			{
				"signature": "public SparkSaslClient(String secretKeyId, SecretKeyHolder secretKeyHolder, boolean encrypt)",
				"documentation": "/**\n * A SASL Client for Spark which simply keeps track of the state of a single SASL session, from the\n * initial state to the \"authenticated\" state. This client initializes the protocol via a\n * firstToken, which is then followed by a set of challenges and responses.\n */"
			},
			{
				"signature": "public Object getNegotiatedProperty(String name)",
				"documentation": "/** Returns the value of a negotiated property. */"
			},
			{
				"signature": "@Override\n  public byte[] wrap(byte[] data, int offset, int len) throws SaslException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] unwrap(byte[] data, int offset, int len) throws SaslException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.sasl.SaslEncryptionBackend"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.sasl.ClientCallbackHandler"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.sasl.ClientCallbackHandler"
		]
	},
	{
		"documentation": "/**\n   * Implementation of javax.security.auth.callback.CallbackHandler\n   * that works with share secrets.\n   */",
		"name": "org.apache.spark.network.sasl.ClientCallbackHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void handle(Callback[] callbacks) throws UnsupportedCallbackException",
				"documentation": "/**\n   * Implementation of javax.security.auth.callback.CallbackHandler\n   * that works with share secrets.\n   */"
			}
		],
		"interfaces": [
			"javax.security.auth.callback.CallbackHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.sasl.SparkSaslClient"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A SASL Server for Spark which simply keeps track of the state of a single SASL session, from the\n * initial state to the \"authenticated\" state. (It is not a server in the sense of accepting\n * connections on some socket.)\n */",
		"name": "org.apache.spark.network.sasl.SparkSaslServer",
		"extends": "",
		"Methods": [
			{
				"signature": "public SparkSaslServer(\n      String secretKeyId,\n      SecretKeyHolder secretKeyHolder,\n      boolean alwaysEncrypt)",
				"documentation": "/** Identifier for a certain secret key within the secretKeyHolder. */"
			},
			{
				"signature": "public Object getNegotiatedProperty(String name)",
				"documentation": "/** Returns the value of a negotiated property. */"
			},
			{
				"signature": "@Override\n  public byte[] wrap(byte[] data, int offset, int len) throws SaslException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] unwrap(byte[] data, int offset, int len) throws SaslException",
				"documentation": ""
			},
			{
				"signature": "public static String encodeIdentifier(String identifier)",
				"documentation": ""
			},
			{
				"signature": "public static char[] encodePassword(String password)",
				"documentation": "/** Encode a password as a base64-encoded char[] array. */"
			},
			{
				"signature": "private static String getBase64EncodedString(String str)",
				"documentation": "/** Return a Base64-encoded string. */"
			}
		],
		"interfaces": [
			"org.apache.spark.network.sasl.SaslEncryptionBackend"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.sasl.DigestCallbackHandler"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.sasl.DigestCallbackHandler"
		]
	},
	{
		"documentation": "/**\n   * Implementation of javax.security.auth.callback.CallbackHandler for SASL DIGEST-MD5 mechanism.\n   */",
		"name": "org.apache.spark.network.sasl.DigestCallbackHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void handle(Callback[] callbacks) throws UnsupportedCallbackException",
				"documentation": "/**\n   * Implementation of javax.security.auth.callback.CallbackHandler for SASL DIGEST-MD5 mechanism.\n   */"
			}
		],
		"interfaces": [
			"javax.security.auth.callback.CallbackHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.sasl.SparkSaslServer"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * RPC Handler which performs authentication, and when it's successful, delegates further\n * calls to another RPC handler. The authentication handshake itself should be implemented\n * by subclasses.\n */",
		"name": "org.apache.spark.network.server.AbstractAuthRpcHandler",
		"extends": "org.apache.spark.network.server.RpcHandler",
		"Methods": [
			{
				"signature": "protected AbstractAuthRpcHandler(RpcHandler delegate)",
				"documentation": "/** RpcHandler we will delegate to for authenticated connections. */"
			},
			{
				"signature": "@Override\n  public final void receive(\n      TransportClient client,\n      ByteBuffer message,\n      RpcResponseCallback callback)",
				"documentation": "/**\n   * Responds to an authentication challenge.\n   *\n   * @return Whether the client is authenticated.\n   */"
			},
			{
				"signature": "@Override\n  public final void receive(TransportClient client, ByteBuffer message)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final StreamCallbackWithID receiveStream(\n      TransportClient client,\n      ByteBuffer message,\n      RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public StreamManager getStreamManager()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelActive(TransportClient client)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelInactive(TransportClient client)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void exceptionCaught(Throwable cause, TransportClient client)",
				"documentation": ""
			},
			{
				"signature": "public boolean isAuthenticated()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MergedBlockMetaReqHandler getMergedBlockMetaReqHandler()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A special RuntimeException thrown when shuffle service experiences a non-fatal failure\n * with handling block push requests with push-based shuffle. Due to the best-effort nature\n * of push-based shuffle, there are cases where the exceptions gets thrown under certain\n * relatively common cases such as when a pushed block is received after the corresponding\n * shuffle is merge finalized or when a pushed block experiences merge collision. Under these\n * scenarios, we throw this special RuntimeException.\n */",
		"name": "org.apache.spark.network.server.BlockPushNonFatalFailure",
		"extends": "RuntimeException",
		"Methods": [
			{
				"signature": "public BlockPushNonFatalFailure(ByteBuffer response, String msg)",
				"documentation": "/**\n   * The error code of the failure. This field is only set on the client side when a\n   * BlockPushNonFatalFailure is recreated from the error code received from the server.\n   */"
			},
			{
				"signature": "public BlockPushNonFatalFailure(ReturnCode returnCode, String msg)",
				"documentation": ""
			},
			{
				"signature": "public ByteBuffer getResponse()",
				"documentation": ""
			},
			{
				"signature": "public ReturnCode getReturnCode()",
				"documentation": ""
			},
			{
				"signature": "public static ReturnCode getReturnCode(byte id)",
				"documentation": ""
			},
			{
				"signature": "public static boolean shouldNotRetryErrorCode(ReturnCode returnCode)",
				"documentation": ""
			},
			{
				"signature": "public static String getErrorMsg(String blockId, ReturnCode errorCode)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.ReturnCode"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockPusher",
			"org.apache.spark.network.shuffle.BlockPushCallback",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.PushBlockStreamCallback",
			"org.apache.spark.network.shuffle.ErrorHandlerSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockPusherSuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.server.ReturnCode"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.server.ReturnCode",
		"extends": "",
		"Methods": [
			{
				"signature": "ReturnCode(int id, String errorMsgSuffix)",
				"documentation": "/**\n     * Indicate the application attempt is not the latest attempt on the server side.\n     * When the client gets this code, it will not retry pushing the block.\n     */"
			},
			{
				"signature": "public byte id()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.server.BlockPushNonFatalFailure"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A dedicated ChannelHandler for processing ChunkFetchRequest messages. When sending response\n * of ChunkFetchRequest messages to the clients, the thread performing the I/O on the underlying\n * channel could potentially be blocked due to disk contentions. If several hundreds of clients\n * send ChunkFetchRequest to the server at the same time, it could potentially occupying all\n * threads from TransportServer's default EventLoopGroup for waiting for disk reads before it\n * can send the block data back to the client as part of the ChunkFetchSuccess messages. As a\n * result, it would leave no threads left to process other RPC messages, which takes much less\n * time to process, and could lead to client timing out on either performing SASL authentication,\n * registering executors, or waiting for response for an OpenBlocks messages.\n */",
		"name": "org.apache.spark.network.server.ChunkFetchRequestHandler",
		"extends": "io.netty.channel.SimpleChannelInboundHandler",
		"Methods": [
			{
				"signature": "public ChunkFetchRequestHandler(\n      TransportClient client,\n      StreamManager streamManager,\n      Long maxChunksBeingTransferred,\n      boolean syncModeEnabled)",
				"documentation": "/** The max number of chunks being transferred and not finished yet. */"
			},
			{
				"signature": "@Override\n  public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected void channelRead0(\n      ChannelHandlerContext ctx,\n      final ChunkFetchRequest msg) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public void processFetchRequest(\n      final Channel channel, final ChunkFetchRequest msg) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private ChannelFuture respond(\n      final Channel channel,\n      final Encodable result) throws InterruptedException",
				"documentation": "/**\n   * The invocation to channel.writeAndFlush is async, and the actual I/O on the\n   * channel will be handled by the EventLoop the channel is registered to. So even\n   * though we are processing the ChunkFetchRequest in a separate thread pool, the actual I/O,\n   * which is the potentially blocking call that could deplete server handler threads, is still\n   * being processed by TransportServer's default EventLoopGroup.\n   *\n   * When syncModeEnabled is true, Spark will throttle the max number of threads that channel I/O\n   * for sending response to ChunkFetchRequest, the thread calling channel.writeAndFlush will wait\n   * for the completion of sending response back to client by invoking await(). This will throttle\n   * the rate at which threads from ChunkFetchRequest dedicated EventLoopGroup submit channel I/O\n   * requests to TransportServer's default EventLoopGroup, thus making sure that we can reserve\n   * some threads in TransportServer's default EventLoopGroup for handling other RPC messages.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.ChunkFetchFailure",
			"org.apache.spark.network.protocol.ChunkFetchSuccess"
		],
		"usedBy": [
			"org.apache.spark.network.ChunkFetchRequestHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Handles either request or response messages coming off of Netty. A MessageHandler instance\n * is associated with a single Netty Channel (though it may have multiple clients on the same\n * Channel.)\n */",
		"name": "org.apache.spark.network.server.MessageHandler",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.server.TransportRequestHandler"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** An RpcHandler suitable for a client-only TransportContext, which cannot receive RPCs. */",
		"name": "org.apache.spark.network.server.NoOpRpcHandler",
		"extends": "org.apache.spark.network.server.RpcHandler",
		"Methods": [
			{
				"signature": "public NoOpRpcHandler()",
				"documentation": "/** An RpcHandler suitable for a client-only TransportContext, which cannot receive RPCs. */"
			},
			{
				"signature": "@Override\n  public void receive(TransportClient client, ByteBuffer message, RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public StreamManager getStreamManager()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ChunkFetchRequestHandlerSuite",
			"org.apache.spark.network.TransportRequestHandlerSuite",
			"org.apache.spark.network.client.TransportClientFactorySuite",
			"org.apache.spark.network.util.NettyMemoryMetricsSuite",
			"org.apache.spark.network.shuffle.ExternalBlockStoreClient"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * StreamManager which allows registration of an Iterator\u0026lt;ManagedBuffer\u0026gt;, which are\n * individually fetched as chunks by the client. Each registered buffer is one chunk.\n */",
		"name": "org.apache.spark.network.server.OneForOneStreamManager",
		"extends": "org.apache.spark.network.server.StreamManager",
		"Methods": [
			{
				"signature": "public OneForOneStreamManager()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer getChunk(long streamId, int chunkIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer openStream(String streamChunkId)",
				"documentation": ""
			},
			{
				"signature": "public static String genStreamChunkId(long streamId, int chunkId)",
				"documentation": ""
			},
			{
				"signature": "public static Pair\u003cLong, Integer\u003e parseStreamChunkId(String streamChunkId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void connectionTerminated(Channel channel)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void checkAuthorization(TransportClient client, long streamId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void chunkBeingSent(long streamId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void streamBeingSent(String streamId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void chunkSent(long streamId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void streamSent(String streamId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long chunksBeingTransferred()",
				"documentation": ""
			},
			{
				"signature": "public long registerStream(\n      String appId,\n      Iterator\u003cManagedBuffer\u003e buffers,\n      Channel channel,\n      boolean isBufferMaterializedOnNext)",
				"documentation": "/**\n   * Registers a stream of ManagedBuffers which are served as individual chunks one at a time to\n   * callers. Each ManagedBuffer will be release()'d after it is transferred on the wire. If a\n   * client connection is closed before the iterator is fully drained, then the remaining\n   * materialized buffers will all be release()'d, but some buffers like\n   * ShuffleManagedBufferIterator, ShuffleChunkManagedBufferIterator, ManagedBufferIterator should\n   * not release, because they have not been materialized before requesting the iterator by\n   * the next method.\n   *\n   * If an app ID is provided, only callers who've authenticated with the given app ID will be\n   * allowed to fetch from this stream.\n   *\n   * This method also associates the stream with a single client connection, which is guaranteed\n   * to be the only reader of the stream. Once the connection is closed, the stream will never\n   * be used again, enabling cleanup by `connectionTerminated`.\n   */"
			},
			{
				"signature": "public long registerStream(String appId, Iterator\u003cManagedBuffer\u003e buffers, Channel channel)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public int numStreamStates()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.OneForOneStreamManager.StreamState"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockHandler",
			"org.apache.spark.network.shuffle.AppIsolationSuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.server.OneForOneStreamManager.StreamState"
		]
	},
	{
		"documentation": "/** State of a single stream. */",
		"name": "org.apache.spark.network.server.OneForOneStreamManager.StreamState",
		"extends": "",
		"Methods": [
			{
				"signature": "StreamState(\n        String appId,\n        Iterator\u003cManagedBuffer\u003e buffers,\n        Channel channel,\n        boolean isBufferMaterializedOnNext)",
				"documentation": "/** State of a single stream. */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.server.OneForOneStreamManager"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Handler for sendRPC() messages sent by {@link org.apache.spark.network.client.TransportClient}s.\n */",
		"name": "org.apache.spark.network.server.RpcHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "public StreamCallbackWithID receiveStream(\n      TransportClient client,\n      ByteBuffer messageHeader,\n      RpcResponseCallback callback)",
				"documentation": "/**\n   * Receive a single RPC message which includes data that is to be received as a stream. Any\n   * exception thrown while in this method will be sent back to the client in string form as a\n   * standard RPC failure.\n   *\n   * Neither this method nor #receive will be called in parallel for a single TransportClient\n   * (i.e., channel).\n   *\n   * An error while reading data from the stream\n   * ({@link org.apache.spark.network.client.StreamCallback#onData(String, ByteBuffer)})\n   * will fail the entire channel.  A failure in \"post-processing\" the stream in\n   * {@link org.apache.spark.network.client.StreamCallback#onComplete(String)} will result in an\n   * rpcFailure, but the channel will remain active.\n   *\n   * @param client A channel client which enables the handler to make requests back to the sender\n   *               of this RPC. This will always be the exact same object for a particular channel.\n   * @param messageHeader The serialized bytes of the header portion of the RPC.  This is in meant\n   *                      to be relatively small, and will be buffered entirely in memory, to\n   *                      facilitate how the streaming portion should be received.\n   * @param callback Callback which should be invoked exactly once upon success or failure of the\n   *                 RPC.\n   * @return a StreamCallback for handling the accompanying streaming data\n   */"
			},
			{
				"signature": "public void receive(TransportClient client, ByteBuffer message)",
				"documentation": "/**\n   * Receives an RPC message that does not expect a reply. The default implementation will\n   * call \"{@link #receive(TransportClient, ByteBuffer, RpcResponseCallback)}\" and log a warning if\n   * any of the callback methods are called.\n   *\n   * @param client A channel client which enables the handler to make requests back to the sender\n   *               of this RPC. This will always be the exact same object for a particular channel.\n   * @param message The serialized bytes of the RPC.\n   */"
			},
			{
				"signature": "public MergedBlockMetaReqHandler getMergedBlockMetaReqHandler()",
				"documentation": ""
			},
			{
				"signature": "public void channelActive(TransportClient client)",
				"documentation": "/**\n   * Invoked when the channel associated with the given client is active.\n   */"
			},
			{
				"signature": "public void channelInactive(TransportClient client)",
				"documentation": "/**\n   * Invoked when the channel associated with the given client is inactive.\n   * No further requests will come from this client.\n   */"
			},
			{
				"signature": "public void exceptionCaught(Throwable cause, TransportClient client)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.crypto.AuthIntegrationSuite.AuthTestCtx"
		],
		"subClasses": [
			"org.apache.spark.network.server.AbstractAuthRpcHandler",
			"org.apache.spark.network.server.NoOpRpcHandler"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.RpcHandler.OneWayRpcCallback",
			"org.apache.spark.network.server.RpcHandler.NoopMergedBlockMetaReqHandler"
		],
		"usedBy": [
			"org.apache.spark.network.ChunkFetchIntegrationSuite",
			"org.apache.spark.network.RequestTimeoutIntegrationSuite",
			"org.apache.spark.network.StreamSuite",
			"org.apache.spark.network.crypto.AuthIntegrationSuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.server.RpcHandler.OneWayRpcCallback",
			"org.apache.spark.network.server.MergedBlockMetaReqHandler",
			"org.apache.spark.network.server.RpcHandler.NoopMergedBlockMetaReqHandler"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.server.RpcHandler.OneWayRpcCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void onSuccess(ByteBuffer response)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(Throwable e)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.RpcResponseCallback"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.server.RpcHandler"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Handler for {@link MergedBlockMetaRequest}.\n   *\n   * @since 3.2.0\n   */",
		"name": "org.apache.spark.network.server.MergedBlockMetaReqHandler",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.server.RpcHandler.NoopMergedBlockMetaReqHandler"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A Noop implementation of {@link MergedBlockMetaReqHandler}. This Noop implementation is used\n   * by all the RPC handlers which don't eventually delegate the {@link MergedBlockMetaRequest} to\n   * ExternalBlockHandler in the network-shuffle module.\n   *\n   * @since 3.2.0\n   */",
		"name": "org.apache.spark.network.server.RpcHandler.NoopMergedBlockMetaReqHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void receiveMergeBlockMetaReq(TransportClient client,\n      MergedBlockMetaRequest mergedBlockMetaRequest, MergedBlockMetaResponseCallback callback)",
				"documentation": "/**\n   * A Noop implementation of {@link MergedBlockMetaReqHandler}. This Noop implementation is used\n   * by all the RPC handlers which don't eventually delegate the {@link MergedBlockMetaRequest} to\n   * ExternalBlockHandler in the network-shuffle module.\n   *\n   * @since 3.2.0\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.network.server.MergedBlockMetaReqHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.server.RpcHandler"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The StreamManager is used to fetch individual chunks from a stream. This is used in\n * {@link TransportRequestHandler} in order to respond to fetchChunk() requests. Creation of the\n * stream is outside the scope of the transport layer, but a given stream is guaranteed to be read\n * by only one client connection, meaning that getChunk() for a particular stream will be called\n * serially and that once the connection associated with the stream is closed, that stream will\n * never be used again.\n */",
		"name": "org.apache.spark.network.server.StreamManager",
		"extends": "",
		"Methods": [
			{
				"signature": "public ManagedBuffer openStream(String streamId)",
				"documentation": "/**\n   * Called in response to a stream() request. The returned data is streamed to the client\n   * through a single TCP connection.\n   *\n   * Note the \u003ccode\u003estreamId\u003c/code\u003e argument is not related to the similarly named argument in the\n   * {@link #getChunk(long, int)} method.\n   *\n   * @param streamId id of a stream that has been previously registered with the StreamManager.\n   * @return A managed buffer for the stream, or null if the stream was not found.\n   */"
			},
			{
				"signature": "public void connectionTerminated(Channel channel)",
				"documentation": "/**\n   * Indicates that the given channel has been terminated. After this occurs, we are guaranteed not\n   * to read from the associated streams again, so any state can be cleaned up.\n   */"
			},
			{
				"signature": "public void checkAuthorization(TransportClient client, long streamId)",
				"documentation": "/**\n   * Verify that the client is authorized to read from the given stream.\n   *\n   * @throws SecurityException If client is not authorized.\n   */"
			},
			{
				"signature": "public long chunksBeingTransferred()",
				"documentation": "/**\n   * Return the number of chunks being transferred and not finished yet in this StreamManager.\n   */"
			},
			{
				"signature": "public void chunkBeingSent(long streamId)",
				"documentation": "/**\n   * Called when start sending a chunk.\n   */"
			},
			{
				"signature": "public void streamBeingSent(String streamId)",
				"documentation": "/**\n   * Called when start sending a stream.\n   */"
			},
			{
				"signature": "public void chunkSent(long streamId)",
				"documentation": "/**\n   * Called when a chunk is successfully sent.\n   */"
			},
			{
				"signature": "public void streamSent(String streamId)",
				"documentation": "/**\n   * Called when a stream is successfully sent.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.server.OneForOneStreamManager"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ChunkFetchIntegrationSuite",
			"org.apache.spark.network.RequestTimeoutIntegrationSuite",
			"org.apache.spark.network.StreamSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The single Transport-level Channel handler which is used for delegating requests to the\n * {@link TransportRequestHandler} and responses to the {@link TransportResponseHandler}.\n *\n * All channels created in the transport layer are bidirectional. When the Client initiates a Netty\n * Channel with a RequestMessage (which gets handled by the Server's RequestHandler), the Server\n * will produce a ResponseMessage (handled by the Client's ResponseHandler). However, the Server\n * also gets a handle on the same Channel, so it may then begin to send RequestMessages to the\n * Client.\n * This means that the Client also needs a RequestHandler and the Server needs a ResponseHandler,\n * for the Client's responses to the Server's requests.\n *\n * This class also handles timeouts from a {@link io.netty.handler.timeout.IdleStateHandler}.\n * We consider a connection timed out if there are outstanding fetch or RPC requests but no traffic\n * on the channel for at least `requestTimeoutMs`. Note that this is duplex traffic; we will not\n * timeout if the client is continuously sending but getting no responses, for simplicity.\n */",
		"name": "org.apache.spark.network.server.TransportChannelHandler",
		"extends": "io.netty.channel.SimpleChannelInboundHandler",
		"Methods": [
			{
				"signature": "public TransportChannelHandler(\n      TransportClient client,\n      TransportResponseHandler responseHandler,\n      TransportRequestHandler requestHandler,\n      long requestTimeoutMs,\n      boolean skipChunkFetchRequest,\n      boolean closeIdleConnections,\n      TransportContext transportContext)",
				"documentation": "/**\n * The single Transport-level Channel handler which is used for delegating requests to the\n * {@link TransportRequestHandler} and responses to the {@link TransportResponseHandler}.\n *\n * All channels created in the transport layer are bidirectional. When the Client initiates a Netty\n * Channel with a RequestMessage (which gets handled by the Server's RequestHandler), the Server\n * will produce a ResponseMessage (handled by the Client's ResponseHandler). However, the Server\n * also gets a handle on the same Channel, so it may then begin to send RequestMessages to the\n * Client.\n * This means that the Client also needs a RequestHandler and the Server needs a ResponseHandler,\n * for the Client's responses to the Server's requests.\n *\n * This class also handles timeouts from a {@link io.netty.handler.timeout.IdleStateHandler}.\n * We consider a connection timed out if there are outstanding fetch or RPC requests but no traffic\n * on the channel for at least `requestTimeoutMs`. Note that this is duplex traffic; we will not\n * timeout if the client is continuously sending but getting no responses, for simplicity.\n */"
			},
			{
				"signature": "public TransportClient getClient()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelActive(ChannelHandlerContext ctx) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelInactive(ChannelHandlerContext ctx) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean acceptInboundMessage(Object msg) throws Exception",
				"documentation": "/**\n   * Overwrite acceptInboundMessage to properly delegate ChunkFetchRequest messages\n   * to ChunkFetchRequestHandler.\n   */"
			},
			{
				"signature": "@Override\n  public void channelRead0(ChannelHandlerContext ctx, Message request) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception",
				"documentation": "/** Triggered based on events from an {@link io.netty.handler.timeout.IdleStateHandler}. */"
			},
			{
				"signature": "public TransportResponseHandler getResponseHandler()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelRegistered(ChannelHandlerContext ctx) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelUnregistered(ChannelHandlerContext ctx) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.NettyUtils"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A handler that processes requests from clients and writes chunk data back. Each handler is\n * attached to a single Netty channel, and keeps track of which streams have been fetched via this\n * channel, in order to clean them up if the channel is terminated (see #channelUnregistered).\n *\n * The messages should have been processed by the pipeline setup by {@link TransportServer}.\n */",
		"name": "org.apache.spark.network.server.TransportRequestHandler",
		"extends": "org.apache.spark.network.server.MessageHandler",
		"Methods": [
			{
				"signature": "public TransportRequestHandler(\n      Channel channel,\n      TransportClient reverseClient,\n      RpcHandler rpcHandler,\n      Long maxChunksBeingTransferred,\n      ChunkFetchRequestHandler chunkFetchRequestHandler)",
				"documentation": "/** The dedicated ChannelHandler for ChunkFetchRequest messages. */"
			},
			{
				"signature": "@Override\n  public void exceptionCaught(Throwable cause)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelActive()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelInactive()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void handle(RequestMessage request) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void processStreamRequest(final StreamRequest req)",
				"documentation": ""
			},
			{
				"signature": "private void processRpcRequest(final RpcRequest req)",
				"documentation": ""
			},
			{
				"signature": "private void processStreamUpload(final UploadStream req)",
				"documentation": "/**\n   * Handle a request from the client to upload a stream of data.\n   */"
			},
			{
				"signature": "@Override\n        public void onSuccess(ByteBuffer response)",
				"documentation": "/**\n   * Handle a request from the client to upload a stream of data.\n   */"
			},
			{
				"signature": "@Override\n        public void onFailure(Throwable e)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onData(String streamId, ByteBuffer buf) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onComplete(String streamId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onFailure(String streamId, Throwable cause) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public String getID()",
				"documentation": ""
			},
			{
				"signature": "private void processOneWayMessage(OneWayMessage req)",
				"documentation": ""
			},
			{
				"signature": "private void processMergedBlockMetaRequest(final MergedBlockMetaRequest req)",
				"documentation": ""
			},
			{
				"signature": "private ChannelFuture respond(Encodable result)",
				"documentation": "/**\n   * Responds to a single message with some Encodable object. If a failure occurs while sending,\n   * it will be logged and the channel closed.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.util.NettyUtils"
		],
		"usedBy": [
			"org.apache.spark.network.TransportRequestHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Server for the efficient, low-level streaming service.\n */",
		"name": "org.apache.spark.network.server.TransportServer",
		"extends": "",
		"Methods": [
			{
				"signature": "public TransportServer(\n      TransportContext context,\n      String hostToBind,\n      int portToBind,\n      RpcHandler appRpcHandler,\n      List\u003cTransportServerBootstrap\u003e bootstraps)",
				"documentation": "/**\n   * Creates a TransportServer that binds to the given host and the given port, or to any available\n   * if 0. If you don't want to bind to any special host, set \"hostToBind\" to null.\n   * */"
			}
		],
		"interfaces": [
			"java.io.Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.server.TransportServerBootstrap",
			"org.apache.spark.network.util.AbstractFileRegion",
			"org.apache.spark.network.util.ByteArrayReadableChannel",
			"org.apache.spark.network.util.ByteArrayWritableChannel",
			"org.apache.spark.network.util.ByteUnit",
			"org.apache.spark.network.util.ConfigProvider",
			"org.apache.spark.network.util.CryptoUtils",
			"org.apache.spark.network.util.IOMode",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.LevelDBProvider",
			"org.apache.spark.network.util.LimitedInputStream",
			"org.apache.spark.network.util.MapConfigProvider",
			"org.apache.spark.network.util.NettyLogger",
			"org.apache.spark.network.util.NettyMemoryMetrics",
			"org.apache.spark.network.util.NettyUtils",
			"org.apache.spark.network.util.TimerWithCustomTimeUnit",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.util.TransportFrameDecoder",
			"org.apache.spark.network.ChunkFetchIntegrationSuite",
			"org.apache.spark.network.ChunkFetchRequestHandlerSuite",
			"org.apache.spark.network.ExtendedChannelPromise",
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.RequestTimeoutIntegrationSuite",
			"org.apache.spark.network.RpcIntegrationSuite",
			"org.apache.spark.network.StreamSuite",
			"org.apache.spark.network.StreamTestHelper",
			"org.apache.spark.network.TestManagedBuffer",
			"org.apache.spark.network.TestUtils",
			"org.apache.spark.network.TransportRequestHandlerSuite",
			"org.apache.spark.network.TransportResponseHandlerSuite",
			"org.apache.spark.network.client.TransportClientFactorySuite",
			"org.apache.spark.network.crypto.AuthEngineSuite",
			"org.apache.spark.network.crypto.AuthIntegrationSuite",
			"org.apache.spark.network.crypto.AuthMessagesSuite",
			"org.apache.spark.network.crypto.TransportCipherSuite",
			"org.apache.spark.network.protocol.EncodersSuite",
			"org.apache.spark.network.protocol.MergedBlockMetaSuccessSuite",
			"org.apache.spark.network.protocol.MessageWithHeaderSuite",
			"org.apache.spark.network.sasl.SparkSaslSuite",
			"org.apache.spark.network.server.OneForOneStreamManagerSuite",
			"org.apache.spark.network.util.CryptoUtilsSuite",
			"org.apache.spark.network.util.NettyMemoryMetricsSuite",
			"org.apache.spark.network.util.TimerWithCustomUnitSuite",
			"org.apache.spark.network.util.TransportFrameDecoderSuite",
			"org.apache.spark.network.sasl.ShuffleSecretManager",
			"org.apache.spark.network.shuffle.BlockFetchingListener",
			"org.apache.spark.network.shuffle.BlockPushingListener",
			"org.apache.spark.network.shuffle.BlockStoreClient",
			"org.apache.spark.network.shuffle.BlockTransferListener",
			"org.apache.spark.network.shuffle.Constants",
			"org.apache.spark.network.shuffle.DownloadFile",
			"org.apache.spark.network.shuffle.DownloadFileManager",
			"org.apache.spark.network.shuffle.DownloadFileWritableChannel",
			"org.apache.spark.network.shuffle.ErrorHandler",
			"org.apache.spark.network.shuffle.ExecutorDiskUtils",
			"org.apache.spark.network.shuffle.ExternalBlockHandler",
			"org.apache.spark.network.shuffle.ExternalBlockStoreClient",
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolver",
			"org.apache.spark.network.shuffle.MergeFinalizerListener",
			"org.apache.spark.network.shuffle.MergedBlockMeta",
			"org.apache.spark.network.shuffle.MergedBlocksMetaListener",
			"org.apache.spark.network.shuffle.MergedShuffleFileManager",
			"org.apache.spark.network.shuffle.NoOpMergedShuffleFileManager",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher",
			"org.apache.spark.network.shuffle.OneForOneBlockPusher",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver",
			"org.apache.spark.network.shuffle.RetryingBlockTransferor",
			"org.apache.spark.network.shuffle.ShuffleIndexInformation",
			"org.apache.spark.network.shuffle.ShuffleIndexRecord",
			"org.apache.spark.network.shuffle.SimpleDownloadFile",
			"org.apache.spark.network.shuffle.checksum.Cause",
			"org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper",
			"org.apache.spark.network.shuffle.protocol.AbstractFetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.BlockPushReturnCode",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
			"org.apache.spark.network.shuffle.protocol.BlocksRemoved",
			"org.apache.spark.network.shuffle.protocol.CorruptionCause",
			"org.apache.spark.network.shuffle.protocol.DiagnoseCorruption",
			"org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunks",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.FinalizeShuffleMerge",
			"org.apache.spark.network.shuffle.protocol.GetLocalDirsForExecutors",
			"org.apache.spark.network.shuffle.protocol.LocalDirsForExecutors",
			"org.apache.spark.network.shuffle.protocol.MergeStatuses",
			"org.apache.spark.network.shuffle.protocol.OpenBlocks",
			"org.apache.spark.network.shuffle.protocol.PushBlockStream",
			"org.apache.spark.network.shuffle.protocol.RegisterExecutor",
			"org.apache.spark.network.shuffle.protocol.RemoveBlocks",
			"org.apache.spark.network.shuffle.protocol.StreamHandle",
			"org.apache.spark.network.shuffle.protocol.UploadBlock",
			"org.apache.spark.network.shuffle.protocol.UploadBlockStream",
			"org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver",
			"org.apache.spark.network.shuffle.protocol.mesos.ShuffleServiceHeartbeat",
			"org.apache.spark.network.sasl.SaslIntegrationSuite",
			"org.apache.spark.network.sasl.ShuffleSecretManagerSuite",
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.BlockTransferMessagesSuite",
			"org.apache.spark.network.shuffle.CleanupNonShuffleServiceServedFilesSuite",
			"org.apache.spark.network.shuffle.ErrorHandlerSuite",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite",
			"org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite"
		]
	},
	{
		"documentation": "/**\n * A bootstrap which is executed on a TransportServer's client channel once a client connects\n * to the server. This allows customizing the client channel to allow for things such as SASL\n * authentication.\n */",
		"name": "org.apache.spark.network.server.TransportServerBootstrap",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.AbstractFileRegion",
		"extends": "io.netty.util.AbstractReferenceCounted",
		"Methods": [
			{
				"signature": "@Override\n  @SuppressWarnings(\"deprecation\")\n  public final long transfered()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public AbstractFileRegion retain()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public AbstractFileRegion retain(int increment)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public AbstractFileRegion touch()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public AbstractFileRegion touch(Object o)",
				"documentation": ""
			}
		],
		"interfaces": [
			"io.netty.channel.FileRegion"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.ByteArrayReadableChannel",
		"extends": "",
		"Methods": [
			{
				"signature": "public void feedData(ByteBuf buf) throws ClosedChannelException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int read(ByteBuffer dst) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isOpen()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.nio.channels.ReadableByteChannel"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.crypto.TransportCipher",
			"org.apache.spark.network.crypto.TransportCipher.DecryptionHandler"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A writable channel that stores the written data in a byte array in memory.\n */",
		"name": "org.apache.spark.network.util.ByteArrayWritableChannel",
		"extends": "",
		"Methods": [
			{
				"signature": "public ByteArrayWritableChannel(int size)",
				"documentation": "/**\n * A writable channel that stores the written data in a byte array in memory.\n */"
			},
			{
				"signature": "public byte[] getData()",
				"documentation": ""
			},
			{
				"signature": "public int length()",
				"documentation": ""
			},
			{
				"signature": "public void reset()",
				"documentation": "/** Resets the channel so that writing to it will overwrite the existing buffer. */"
			},
			{
				"signature": "@Override\n  public int write(ByteBuffer src)",
				"documentation": "/**\n   * Reads from the given buffer into the internal byte array.\n   */"
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isOpen()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.nio.channels.WritableByteChannel"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.crypto.TransportCipher",
			"org.apache.spark.network.crypto.TransportCipher.EncryptionHandler",
			"org.apache.spark.network.crypto.TransportCipher.EncryptedMessage",
			"org.apache.spark.network.sasl.SaslEncryption",
			"org.apache.spark.network.sasl.SaslEncryption.EncryptedMessage",
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.ProtocolSuite.FileRegionEncoder",
			"org.apache.spark.network.crypto.AuthEngineSuite",
			"org.apache.spark.network.sasl.SparkSaslSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.ByteUnit",
		"extends": "",
		"Methods": [
			{
				"signature": "ByteUnit(long multiplier)",
				"documentation": ""
			},
			{
				"signature": "public long convertFrom(long d, ByteUnit u)",
				"documentation": ""
			},
			{
				"signature": "public long convertTo(long d, ByteUnit u)",
				"documentation": ""
			},
			{
				"signature": "public long toBytes(long d)",
				"documentation": ""
			},
			{
				"signature": "public long toKiB(long d)",
				"documentation": ""
			},
			{
				"signature": "public long toMiB(long d)",
				"documentation": ""
			},
			{
				"signature": "public long toGiB(long d)",
				"documentation": ""
			},
			{
				"signature": "public long toTiB(long d)",
				"documentation": ""
			},
			{
				"signature": "public long toPiB(long d)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Provides a mechanism for constructing a {@link TransportConf} using some sort of configuration.\n */",
		"name": "org.apache.spark.network.util.ConfigProvider",
		"extends": "",
		"Methods": [
			{
				"signature": "public String get(String name, String defaultValue)",
				"documentation": "/** Returns all the config values in the provider. */"
			},
			{
				"signature": "public int getInt(String name, int defaultValue)",
				"documentation": ""
			},
			{
				"signature": "public long getLong(String name, long defaultValue)",
				"documentation": ""
			},
			{
				"signature": "public double getDouble(String name, double defaultValue)",
				"documentation": ""
			},
			{
				"signature": "public boolean getBoolean(String name, boolean defaultValue)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.util.MapConfigProvider"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.client.TransportClientFactorySuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Utility methods related to the commons-crypto library.\n */",
		"name": "org.apache.spark.network.util.CryptoUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static Properties toCryptoConf(String prefix, Iterable\u003cMap.Entry\u003cString, String\u003e\u003e conf)",
				"documentation": "/**\n   * Extract the commons-crypto configuration embedded in a list of config values.\n   *\n   * @param prefix Prefix in the given configuration that identifies the commons-crypto configs.\n   * @param conf List of configuration values.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Selector for which form of low-level IO we should use.\n * NIO is always available, while EPOLL is only available on Linux.\n * AUTO is used to select EPOLL if it's available, or NIO otherwise.\n */",
		"name": "org.apache.spark.network.util.IOMode",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * General utilities available in the network package. Many of these are sourced from Spark's\n * own Utils, just accessible within this package.\n */",
		"name": "org.apache.spark.network.util.JavaUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void closeQuietly(Closeable closeable)",
				"documentation": "/** Closes the given object, ignoring IOExceptions. */"
			},
			{
				"signature": "public static int nonNegativeHash(Object obj)",
				"documentation": "/** Returns a hash consistent with Spark's Utils.nonNegativeHash(). */"
			},
			{
				"signature": "public static ByteBuffer stringToBytes(String s)",
				"documentation": "/**\n   * Convert the given string to a byte buffer. The resulting buffer can be\n   * converted back to the same string through {@link #bytesToString(ByteBuffer)}.\n   */"
			},
			{
				"signature": "public static String bytesToString(ByteBuffer b)",
				"documentation": "/**\n   * Convert the given byte buffer to a string. The resulting string can be\n   * converted back to the same byte buffer through {@link #stringToBytes(String)}.\n   */"
			},
			{
				"signature": "public static void deleteRecursively(File file) throws IOException",
				"documentation": "/**\n   * Delete a file or directory and its contents recursively.\n   * Don't follow directories if they are symlinks.\n   *\n   * @param file Input file / dir to be deleted\n   * @throws IOException if deletion is unsuccessful\n   */"
			},
			{
				"signature": "public static void deleteRecursively(File file, FilenameFilter filter) throws IOException",
				"documentation": "/**\n   * Delete a file or directory and its contents recursively.\n   * Don't follow directories if they are symlinks.\n   *\n   * @param file Input file / dir to be deleted\n   * @param filter A filename filter that make sure only files / dirs with the satisfied filenames\n   *               are deleted.\n   * @throws IOException if deletion is unsuccessful\n   */"
			},
			{
				"signature": "private static void deleteRecursivelyUsingJavaIO(\n      File file,\n      FilenameFilter filter) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static void deleteRecursivelyUsingUnixNative(File file) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static File[] listFilesSafely(File file, FilenameFilter filter) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static boolean isSymlink(File file) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public static long timeStringAs(String str, TimeUnit unit)",
				"documentation": "/**\n   * Convert a passed time string (e.g. 50s, 100ms, or 250us) to a time count in the given unit.\n   * The unit is also considered the default if the given string does not specify a unit.\n   */"
			},
			{
				"signature": "public static long timeStringAsMs(String str)",
				"documentation": "/**\n   * Convert a time parameter such as (50s, 100ms, or 250us) to milliseconds for internal use. If\n   * no suffix is provided, the passed number is assumed to be in ms.\n   */"
			},
			{
				"signature": "public static long timeStringAsSec(String str)",
				"documentation": "/**\n   * Convert a time parameter such as (50s, 100ms, or 250us) to seconds for internal use. If\n   * no suffix is provided, the passed number is assumed to be in seconds.\n   */"
			},
			{
				"signature": "public static long byteStringAs(String str, ByteUnit unit)",
				"documentation": "/**\n   * Convert a passed byte string (e.g. 50b, 100kb, or 250mb) to the given. If no suffix is\n   * provided, a direct conversion to the provided unit is attempted.\n   */"
			},
			{
				"signature": "public static long byteStringAsBytes(String str)",
				"documentation": "/**\n   * Convert a passed byte string (e.g. 50b, 100k, or 250m) to bytes for\n   * internal use.\n   *\n   * If no suffix is provided, the passed number is assumed to be in bytes.\n   */"
			},
			{
				"signature": "public static long byteStringAsKb(String str)",
				"documentation": "/**\n   * Convert a passed byte string (e.g. 50b, 100k, or 250m) to kibibytes for\n   * internal use.\n   *\n   * If no suffix is provided, the passed number is assumed to be in kibibytes.\n   */"
			},
			{
				"signature": "public static long byteStringAsMb(String str)",
				"documentation": "/**\n   * Convert a passed byte string (e.g. 50b, 100k, or 250m) to mebibytes for\n   * internal use.\n   *\n   * If no suffix is provided, the passed number is assumed to be in mebibytes.\n   */"
			},
			{
				"signature": "public static long byteStringAsGb(String str)",
				"documentation": "/**\n   * Convert a passed byte string (e.g. 50b, 100k, or 250m) to gibibytes for\n   * internal use.\n   *\n   * If no suffix is provided, the passed number is assumed to be in gibibytes.\n   */"
			},
			{
				"signature": "public static byte[] bufferToArray(ByteBuffer buffer)",
				"documentation": "/**\n   * Returns a byte array with the buffer's contents, trying to avoid copying the data if\n   * possible.\n   */"
			},
			{
				"signature": "public static void readFully(ReadableByteChannel channel, ByteBuffer dst) throws IOException",
				"documentation": "/**\n   * Fills a buffer with data read from the channel.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.StreamTestHelper",
			"org.apache.spark.network.crypto.AuthIntegrationSuite.AuthTestCtx",
			"org.apache.spark.network.shuffle.TestShuffleDataContext"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.RpcIntegrationSuite",
			"org.apache.spark.network.client.TransportClientFactorySuite",
			"org.apache.spark.network.crypto.AuthIntegrationSuite",
			"org.apache.spark.network.sasl.SparkSaslSuite",
			"org.apache.spark.network.sasl.ShuffleSecretManager",
			"org.apache.spark.network.shuffle.ExecutorDiskUtils",
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolver",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver",
			"org.apache.spark.network.sasl.SaslIntegrationSuite",
			"org.apache.spark.unsafe.map.AbstractBytesToBytesMapSuite",
			"org.apache.spark.streaming.JavaWriteAheadLogSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * LevelDB utility class available in the network package.\n */",
		"name": "org.apache.spark.network.util.LevelDBProvider",
		"extends": "",
		"Methods": [
			{
				"signature": "public static DB initLevelDB(File dbFile, StoreVersion version, ObjectMapper mapper) throws\n      IOException",
				"documentation": "/**\n * LevelDB utility class available in the network package.\n */"
			},
			{
				"signature": "public static void checkVersion(DB db, StoreVersion newversion, ObjectMapper mapper) throws\n      IOException",
				"documentation": "/**\n   * Simple major.minor versioning scheme.  Any incompatible changes should be across major\n   * versions.  Minor version differences are allowed -- meaning we should be able to read\n   * dbs that are either earlier *or* later on the minor version.\n   */"
			},
			{
				"signature": "public static void storeVersion(DB db, StoreVersion version, ObjectMapper mapper)\n      throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.LevelDBProvider.LevelDBLogger"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolver"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.util.LevelDBProvider.LevelDBLogger",
			"org.apache.spark.network.util.LevelDBProvider.StoreVersion"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.LevelDBProvider.LevelDBLogger",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void log(String message)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.iq80.leveldb.Logger"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.util.LevelDBProvider"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.LevelDBProvider.StoreVersion",
		"extends": "",
		"Methods": [
			{
				"signature": "@JsonCreator\n    public StoreVersion(@JsonProperty(\"major\") int major, @JsonProperty(\"minor\") int minor)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolver"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Wraps a {@link InputStream}, limiting the number of bytes which can be read.\n *\n * This code is from Guava's 14.0 source code, because there is no compatible way to\n * use this functionality in both a Guava 11 environment and a Guava \u0026gt;14 environment.\n */",
		"name": "org.apache.spark.network.util.LimitedInputStream",
		"extends": "java.io.FilterInputStream",
		"Methods": [
			{
				"signature": "public LimitedInputStream(InputStream in, long limit)",
				"documentation": "/**\n * Wraps a {@link InputStream}, limiting the number of bytes which can be read.\n *\n * This code is from Guava's 14.0 source code, because there is no compatible way to\n * use this functionality in both a Guava 11 environment and a Guava \u0026gt;14 environment.\n */"
			},
			{
				"signature": "public LimitedInputStream(InputStream in, long limit, boolean closeWrappedStream)",
				"documentation": "/**\n   * Create a LimitedInputStream that will read {@code limit} bytes from {@code in}.\n   * \u003cp\u003e\n   * If {@code closeWrappedStream} is true, this will close {@code in} when it is closed.\n   * Otherwise, the stream is left open for reading its remaining content.\n   *\n   * @param in a {@link InputStream} to read from\n   * @param limit the number of bytes to read\n   * @param closeWrappedStream whether to close {@code in} when {@link #close} is called\n     */"
			},
			{
				"signature": "@Override public int available() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override public int read() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override public int read(byte[] b, int off, int len) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override public long skip(long n) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** ConfigProvider based on a Map (copied in the constructor). */",
		"name": "org.apache.spark.network.util.MapConfigProvider",
		"extends": "org.apache.spark.network.util.ConfigProvider",
		"Methods": [
			{
				"signature": "public MapConfigProvider(Map\u003cString, String\u003e config)",
				"documentation": "/** ConfigProvider based on a Map (copied in the constructor). */"
			},
			{
				"signature": "@Override\n  public String get(String name)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String get(String name, String defaultValue)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Iterable\u003cMap.Entry\u003cString, String\u003e\u003e getAll()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.sasl.SparkSaslSuite.SaslTestCtx"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.RequestTimeoutIntegrationSuite",
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.CleanupNonShuffleServiceServedFilesSuite",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite",
			"org.apache.spark.network.shuffle.RetryingBlockTransferorSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.NettyLogger",
		"extends": "",
		"Methods": [
			{
				"signature": "public NettyLogger()",
				"documentation": ""
			},
			{
				"signature": "public LoggingHandler getLoggingHandler()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.NettyLogger.NoContentLoggingHandler"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.util.NettyLogger.NoContentLoggingHandler"
		]
	},
	{
		"documentation": "/** A Netty LoggingHandler which does not dump the message contents. */",
		"name": "org.apache.spark.network.util.NettyLogger.NoContentLoggingHandler",
		"extends": "io.netty.handler.logging.LoggingHandler",
		"Methods": [
			{
				"signature": "NoContentLoggingHandler(Class\u003c?\u003e clazz, LogLevel level)",
				"documentation": "/** A Netty LoggingHandler which does not dump the message contents. */"
			},
			{
				"signature": "protected String format(ChannelHandlerContext ctx, String eventName, Object arg)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.util.NettyLogger"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Netty memory metrics class to collect metrics from Netty PooledByteBufAllocator.\n */",
		"name": "org.apache.spark.network.util.NettyMemoryMetrics",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * A Netty memory metrics class to collect metrics from Netty PooledByteBufAllocator.\n */"
			},
			{
				"signature": "public NettyMemoryMetrics(PooledByteBufAllocator pooledAllocator,\n      String metricPrefix,\n      TransportConf conf)",
				"documentation": ""
			},
			{
				"signature": "private void registerMetrics(PooledByteBufAllocator allocator)",
				"documentation": ""
			},
			{
				"signature": "private void registerArenaMetric(PoolArenaMetric arenaMetric, String arenaName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Map\u003cString, Metric\u003e getMetrics()",
				"documentation": ""
			}
		],
		"interfaces": [
			"com.codahale.metrics.MetricSet"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Utilities for creating various Netty constructs based on whether we're using EPOLL or NIO.\n */",
		"name": "org.apache.spark.network.util.NettyUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static long freeDirectMemory()",
				"documentation": "/**\n   * Specifies an upper bound on the number of Netty threads that Spark requires by default.\n   * In practice, only 2-4 cores should be required to transfer roughly 10 Gb/s, and each core\n   * that we use will have an initial overhead of roughly 32 MB of off-heap memory, which comes\n   * at a premium.\n   *\n   * Thus, this value should still retain maximum throughput and reduce wasted off-heap memory\n   * allocation. It can be overridden by setting the number of serverThreads and clientThreads\n   * manually in Spark's configuration.\n   */"
			},
			{
				"signature": "public static ThreadFactory createThreadFactory(String threadPoolPrefix)",
				"documentation": "/** Creates a new ThreadFactory which prefixes each thread with the given name. */"
			},
			{
				"signature": "public static EventLoopGroup createEventLoop(IOMode mode, int numThreads, String threadPrefix)",
				"documentation": "/** Creates a Netty EventLoopGroup based on the IOMode. */"
			},
			{
				"signature": "public static Class\u003c? extends Channel\u003e getClientChannelClass(IOMode mode)",
				"documentation": "/** Returns the correct (client) SocketChannel class based on IOMode. */"
			},
			{
				"signature": "public static Class\u003c? extends ServerChannel\u003e getServerChannelClass(IOMode mode)",
				"documentation": "/** Returns the correct ServerSocketChannel class based on IOMode. */"
			},
			{
				"signature": "public static TransportFrameDecoder createFrameDecoder()",
				"documentation": "/**\n   * Creates a LengthFieldBasedFrameDecoder where the first 8 bytes are the length of the frame.\n   * This is used before all decoders.\n   */"
			},
			{
				"signature": "public static String getRemoteAddress(Channel channel)",
				"documentation": "/** Returns the remote address on the channel or \"\u0026lt;unknown remote\u0026gt;\" if none exists. */"
			},
			{
				"signature": "public static int defaultNumThreads(int numUsableCores)",
				"documentation": "/**\n   * Returns the default number of threads for both the Netty client and server thread pools.\n   * If numUsableCores is 0, we will use Runtime get an approximate number of available cores.\n   */"
			},
			{
				"signature": "public static PooledByteBufAllocator createPooledByteBufAllocator(\n      boolean allowDirectBufs,\n      boolean allowCache,\n      int numCores)",
				"documentation": "/**\n   * Create a pooled ByteBuf allocator but disables the thread-local cache. Thread-local caches\n   * are disabled for TransportClients because the ByteBufs are allocated by the event loop thread,\n   * but released by the executor thread rather than the event loop thread. Those thread-local\n   * caches actually delay the recycling of buffers, leading to larger memory usage.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.client.TransportClient",
			"org.apache.spark.network.client.StdChannelListener",
			"org.apache.spark.network.client.TransportResponseHandler",
			"org.apache.spark.network.sasl.SaslEncryption",
			"org.apache.spark.network.server.TransportChannelHandler",
			"org.apache.spark.network.server.TransportRequestHandler",
			"org.apache.spark.network.ProtocolSuite",
			"org.apache.spark.network.shuffle.ExternalBlockHandler",
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolver",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver",
			"org.apache.spark.network.shuffle.RetryingBlockTransferor"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A custom version of a {@link Timer} which allows for specifying a specific {@link TimeUnit} to\n * be used when accessing timing values via {@link #getSnapshot()}. Normally, though the\n * {@link #update(long, TimeUnit)} method requires a unit, the extraction methods on the snapshot\n * do not specify a unit, and always return nanoseconds. It can be useful to specify that a timer\n * should use a different unit for its snapshot. Note that internally, all values are still stored\n * with nanosecond-precision; it is only before being returned to the caller that the nanosecond\n * value is converted to the custom time unit.\n */",
		"name": "org.apache.spark.network.util.TimerWithCustomTimeUnit",
		"extends": "com.codahale.metrics.Timer",
		"Methods": [
			{
				"signature": "public TimerWithCustomTimeUnit(TimeUnit timeUnit)",
				"documentation": "/**\n * A custom version of a {@link Timer} which allows for specifying a specific {@link TimeUnit} to\n * be used when accessing timing values via {@link #getSnapshot()}. Normally, though the\n * {@link #update(long, TimeUnit)} method requires a unit, the extraction methods on the snapshot\n * do not specify a unit, and always return nanoseconds. It can be useful to specify that a timer\n * should use a different unit for its snapshot. Note that internally, all values are still stored\n * with nanosecond-precision; it is only before being returned to the caller that the nanosecond\n * value is converted to the custom time unit.\n */"
			},
			{
				"signature": "TimerWithCustomTimeUnit(TimeUnit timeUnit, Clock clock)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Snapshot getSnapshot()",
				"documentation": ""
			},
			{
				"signature": "private double toUnit(double nanos)",
				"documentation": ""
			},
			{
				"signature": "private long toUnit(long nanos)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.SnapshotWithCustomTimeUnit"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockHandler",
			"org.apache.spark.network.shuffle.ShuffleMetrics"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.util.SnapshotWithCustomTimeUnit"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.SnapshotWithCustomTimeUnit",
		"extends": "com.codahale.metrics.Snapshot",
		"Methods": [
			{
				"signature": "SnapshotWithCustomTimeUnit(Snapshot wrappedSnapshot)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public double getValue(double v)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long[] getValues()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int size()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long getMax()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public double getMean()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long getMin()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public double getStdDev()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void dump(OutputStream outputStream)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.util.TimerWithCustomTimeUnit"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A central location that tracks all the settings we expose to users.\n */",
		"name": "org.apache.spark.network.util.TransportConf",
		"extends": "",
		"Methods": [
			{
				"signature": "public TransportConf(String module, ConfigProvider conf)",
				"documentation": "/**\n * A central location that tracks all the settings we expose to users.\n */"
			},
			{
				"signature": "public int getInt(String name, int defaultValue)",
				"documentation": ""
			},
			{
				"signature": "public String get(String name, String defaultValue)",
				"documentation": ""
			},
			{
				"signature": "private String getConfKey(String suffix)",
				"documentation": ""
			},
			{
				"signature": "public String getModuleName()",
				"documentation": ""
			},
			{
				"signature": "public String ioMode()",
				"documentation": "/** IO mode: nio or epoll */"
			},
			{
				"signature": "public boolean preferDirectBufs()",
				"documentation": "/** If true, we will prefer allocating off-heap byte buffers within Netty. */"
			},
			{
				"signature": "public int connectionTimeoutMs()",
				"documentation": "/** Connection idle timeout in milliseconds. Default 120 secs. */"
			},
			{
				"signature": "public int connectionCreationTimeoutMs()",
				"documentation": "/** Connect creation timeout in milliseconds. Default 30 secs. */"
			},
			{
				"signature": "public int numConnectionsPerPeer()",
				"documentation": "/** Number of concurrent connections between two nodes for fetching data. */"
			},
			{
				"signature": "public int backLog()",
				"documentation": "/**\n   * Requested maximum length of the queue of incoming connections. If  \u0026lt; 1,\n   * the default Netty value of {@link io.netty.util.NetUtil#SOMAXCONN} will be used.\n   * Default to -1.\n   */"
			},
			{
				"signature": "public int serverThreads()",
				"documentation": "/** Number of threads used in the server thread pool. Default to 0, which is 2x#cores. */"
			},
			{
				"signature": "public int clientThreads()",
				"documentation": "/** Number of threads used in the client thread pool. Default to 0, which is 2x#cores. */"
			},
			{
				"signature": "public int receiveBuf()",
				"documentation": "/**\n   * Receive buffer size (SO_RCVBUF).\n   * Note: the optimal size for receive buffer and send buffer should be\n   *  latency * network_bandwidth.\n   * Assuming latency = 1ms, network_bandwidth = 10Gbps\n   *  buffer size should be ~ 1.25MB\n   */"
			},
			{
				"signature": "public int sendBuf()",
				"documentation": "/** Send buffer size (SO_SNDBUF). */"
			},
			{
				"signature": "public int authRTTimeoutMs()",
				"documentation": "/** Timeout for a single round trip of auth message exchange, in milliseconds. */"
			},
			{
				"signature": "public int maxIORetries()",
				"documentation": "/**\n   * Max number of times we will try IO exceptions (such as connection timeouts) per request.\n   * If set to 0, we will not do any retries.\n   */"
			},
			{
				"signature": "public int ioRetryWaitTimeMs()",
				"documentation": "/**\n   * Time (in milliseconds) that we will wait in order to perform a retry after an IOException.\n   * Only relevant if maxIORetries \u0026gt; 0.\n   */"
			},
			{
				"signature": "public int memoryMapBytes()",
				"documentation": "/**\n   * Minimum size of a block that we should start using memory map rather than reading in through\n   * normal IO operations. This prevents Spark from memory mapping very small blocks. In general,\n   * memory mapping has high overhead for blocks close to or below the page size of the OS.\n   */"
			},
			{
				"signature": "public boolean lazyFileDescriptor()",
				"documentation": "/**\n   * Whether to initialize FileDescriptor lazily or not. If true, file descriptors are\n   * created only when data is going to be transferred. This can reduce the number of open files.\n   */"
			},
			{
				"signature": "public boolean verboseMetrics()",
				"documentation": "/**\n   * Whether to track Netty memory detailed metrics. If true, the detailed metrics of Netty\n   * PoolByteBufAllocator will be gotten, otherwise only general memory usage will be tracked.\n   */"
			},
			{
				"signature": "public boolean enableTcpKeepAlive()",
				"documentation": "/**\n   * Whether to enable TCP keep-alive. If true, the TCP keep-alives are enabled, which removes\n   * connections that are idle for too long.\n   */"
			},
			{
				"signature": "public int portMaxRetries()",
				"documentation": "/**\n   * Maximum number of retries when binding to a port before giving up.\n   */"
			},
			{
				"signature": "public boolean encryptionEnabled()",
				"documentation": "/**\n   * Enables strong encryption. Also enables the new auth protocol, used to negotiate keys.\n   */"
			},
			{
				"signature": "public String cipherTransformation()",
				"documentation": "/**\n   * The cipher transformation to use for encrypting session data.\n   */"
			},
			{
				"signature": "public boolean saslFallback()",
				"documentation": "/**\n   * Whether to fall back to SASL if the new auth protocol fails. Enabled by default for\n   * backwards compatibility.\n   */"
			},
			{
				"signature": "public boolean saslEncryption()",
				"documentation": "/**\n   * Whether to enable SASL-based encryption when authenticating using SASL.\n   */"
			},
			{
				"signature": "public int maxSaslEncryptedBlockSize()",
				"documentation": "/**\n   * Maximum number of bytes to be encrypted at a time when SASL encryption is used.\n   */"
			},
			{
				"signature": "public boolean saslServerAlwaysEncrypt()",
				"documentation": "/**\n   * Whether the server should enforce encryption on SASL-authenticated connections.\n   */"
			},
			{
				"signature": "public boolean sharedByteBufAllocators()",
				"documentation": "/**\n   * Flag indicating whether to share the pooled ByteBuf allocators between the different Netty\n   * channels. If enabled then only two pooled ByteBuf allocators are created: one where caching\n   * is allowed (for transport servers) and one where not (for transport clients).\n   * When disabled a new allocator is created for each transport servers and clients.\n   */"
			},
			{
				"signature": "public boolean preferDirectBufsForSharedByteBufAllocators()",
				"documentation": "/**\n  * If enabled then off-heap byte buffers will be preferred for the shared ByteBuf allocators.\n  */"
			},
			{
				"signature": "public Properties cryptoConf()",
				"documentation": "/**\n   * The commons-crypto configuration for the module.\n   */"
			},
			{
				"signature": "public long maxChunksBeingTransferred()",
				"documentation": "/**\n   * The max number of chunks allowed to be transferred at the same time on shuffle service.\n   * Note that new incoming connections will be closed when the max number is hit. The client will\n   * retry according to the shuffle retry configs (see `spark.shuffle.io.maxRetries` and\n   * `spark.shuffle.io.retryWait`), if those limits are reached the task will fail with fetch\n   * failure.\n   */"
			},
			{
				"signature": "public int chunkFetchHandlerThreads()",
				"documentation": "/**\n   * Percentage of io.serverThreads used by netty to process ChunkFetchRequest.\n   * When the config `spark.shuffle.server.chunkFetchHandlerThreadsPercent` is set,\n   * shuffle server will use a separate EventLoopGroup to process ChunkFetchRequest messages.\n   * Although when calling the async writeAndFlush on the underlying channel to send\n   * response back to client, the I/O on the channel is still being handled by\n   * {@link org.apache.spark.network.server.TransportServer}'s default EventLoopGroup\n   * that's registered with the Channel, by waiting inside the ChunkFetchRequest handler\n   * threads for the completion of sending back responses, we are able to put a limit on\n   * the max number of threads from TransportServer's default EventLoopGroup that are\n   * going to be consumed by writing response to ChunkFetchRequest, which are I/O intensive\n   * and could take long time to process due to disk contentions. By configuring a slightly\n   * higher number of shuffler server threads, we are able to reserve some threads for\n   * handling other RPC messages, thus making the Client less likely to experience timeout\n   * when sending RPC messages to the shuffle server. The number of threads used for handling\n   * chunked fetch requests are percentage of io.serverThreads (if defined) else it is a percentage\n   * of 2 * #cores. However, a percentage of 0 means netty default number of threads which\n   * is 2 * #cores ignoring io.serverThreads. The percentage here is configured via\n   * spark.shuffle.server.chunkFetchHandlerThreadsPercent. The returned value is rounded off to\n   * ceiling of the nearest integer.\n   */"
			},
			{
				"signature": "public boolean separateChunkFetchRequest()",
				"documentation": "/**\n   * Whether to use a separate EventLoopGroup to process ChunkFetchRequest messages, it is decided\n   * by the config `spark.shuffle.server.chunkFetchHandlerThreadsPercent` is set or not.\n   */"
			},
			{
				"signature": "public boolean useOldFetchProtocol()",
				"documentation": "/**\n   * Whether to use the old protocol while doing the shuffle block fetching.\n   * It is only enabled while we need the compatibility in the scenario of new spark version\n   * job fetching blocks from old version external shuffle service.\n   */"
			},
			{
				"signature": "public String mergedShuffleFileManagerImpl()",
				"documentation": "/**\n   * Class name of the implementation of MergedShuffleFileManager that merges the blocks\n   * pushed to it when push-based shuffle is enabled. By default, push-based shuffle is disabled at\n   * a cluster level because this configuration is set to\n   * 'org.apache.spark.network.shuffle.NoOpMergedShuffleFileManager'.\n   * To turn on push-based shuffle at a cluster level, set the configuration to\n   * 'org.apache.spark.network.shuffle.RemoteBlockPushResolver'.\n   */"
			},
			{
				"signature": "public int minChunkSizeInMergedShuffleFile()",
				"documentation": "/**\n   * The minimum size of a chunk when dividing a merged shuffle file into multiple chunks during\n   * push-based shuffle.\n   * A merged shuffle file consists of multiple small shuffle blocks. Fetching the complete\n   * merged shuffle file in a single disk I/O increases the memory requirements for both the\n   * clients and the external shuffle service. Instead, the external shuffle service serves\n   * the merged file in MB-sized chunks. This configuration controls how big a chunk can get.\n   * A corresponding index file for each merged shuffle file will be generated indicating chunk\n   * boundaries.\n   *\n   * Setting this too high would increase the memory requirements on both the clients and the\n   * external shuffle service.\n   *\n   * Setting this too low would increase the overall number of RPC requests to external shuffle\n   * service unnecessarily.\n   */"
			},
			{
				"signature": "public long mergedIndexCacheSize()",
				"documentation": "/**\n   * The maximum size of cache in memory which is used in push-based shuffle for storing merged\n   * index files. This cache is in addition to the one configured via\n   * spark.shuffle.service.index.cache.size.\n   */"
			},
			{
				"signature": "public int ioExceptionsThresholdDuringMerge()",
				"documentation": "/**\n   * The threshold for number of IOExceptions while merging shuffle blocks to a shuffle partition.\n   * When the number of IOExceptions while writing to merged shuffle data/index/meta file exceed\n   * this threshold then the shuffle server will respond back to client to stop pushing shuffle\n   * blocks for this shuffle partition.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.crypto.AuthIntegrationSuite.AuthTestCtx",
			"org.apache.spark.network.sasl.SparkSaslSuite.SaslTestCtx"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ChunkFetchIntegrationSuite",
			"org.apache.spark.network.RequestTimeoutIntegrationSuite",
			"org.apache.spark.network.RpcIntegrationSuite",
			"org.apache.spark.network.StreamSuite",
			"org.apache.spark.network.client.TransportClientFactorySuite",
			"org.apache.spark.network.crypto.AuthEngineSuite",
			"org.apache.spark.network.crypto.AuthIntegrationSuite",
			"org.apache.spark.network.crypto.TransportCipherSuite",
			"org.apache.spark.network.sasl.SparkSaslSuite",
			"org.apache.spark.network.sasl.SaslIntegrationSuite",
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.CleanupNonShuffleServiceServedFilesSuite",
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite",
			"org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite",
			"org.apache.spark.network.shuffle.RetryingBlockTransferorSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A customized frame decoder that allows intercepting raw data.\n * \u003cp\u003e\n * This behaves like Netty's frame decoder (with hard coded parameters that match this library's\n * needs), except it allows an interceptor to be installed to read data directly before it's\n * framed.\n * \u003cp\u003e\n * Unlike Netty's frame decoder, each frame is dispatched to child handlers as soon as it's\n * decoded, instead of building as many frames as the current buffer allows and dispatching\n * all of them. This allows a child handler to install an interceptor if needed.\n * \u003cp\u003e\n * If an interceptor is installed, framing stops, and data is instead fed directly to the\n * interceptor. When the interceptor indicates that it doesn't need to read any more data,\n * framing resumes. Interceptors should not hold references to the data buffers provided\n * to their handle() method.\n */",
		"name": "org.apache.spark.network.util.TransportFrameDecoder",
		"extends": "io.netty.channel.ChannelInboundHandlerAdapter",
		"Methods": [
			{
				"signature": "public TransportFrameDecoder()",
				"documentation": "/**\n * A customized frame decoder that allows intercepting raw data.\n * \u003cp\u003e\n * This behaves like Netty's frame decoder (with hard coded parameters that match this library's\n * needs), except it allows an interceptor to be installed to read data directly before it's\n * framed.\n * \u003cp\u003e\n * Unlike Netty's frame decoder, each frame is dispatched to child handlers as soon as it's\n * decoded, instead of building as many frames as the current buffer allows and dispatching\n * all of them. This allows a child handler to install an interceptor if needed.\n * \u003cp\u003e\n * If an interceptor is installed, framing stops, and data is instead fed directly to the\n * interceptor. When the interceptor indicates that it doesn't need to read any more data,\n * framing resumes. Interceptors should not hold references to the data buffers provided\n * to their handle() method.\n */"
			},
			{
				"signature": "@VisibleForTesting\n  TransportFrameDecoder(long consolidateThreshold)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelRead(ChannelHandlerContext ctx, Object data) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private long decodeFrameSize()",
				"documentation": ""
			},
			{
				"signature": "private ByteBuf decodeNext()",
				"documentation": ""
			},
			{
				"signature": "private ByteBuf consumeCurrentFrameBuf()",
				"documentation": ""
			},
			{
				"signature": "private ByteBuf nextBufferForFrame(int bytesToRead)",
				"documentation": "/**\n   * Takes the first buffer in the internal list, and either adjust it to fit in the frame\n   * (by taking a slice out of it) or remove it from the internal list.\n   */"
			},
			{
				"signature": "@Override\n  public void channelInactive(ChannelHandlerContext ctx) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void handlerRemoved(ChannelHandlerContext ctx) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public void setInterceptor(Interceptor interceptor)",
				"documentation": ""
			},
			{
				"signature": "private boolean feedInterceptor(ByteBuf buf) throws Exception",
				"documentation": "/**\n   * @return Whether the interceptor is still active after processing the data.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.TransportResponseHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.util.Interceptor"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.Interceptor",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.ChunkFetchIntegrationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void setUp() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ManagedBuffer getChunk(long streamId, int chunkIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void receive(\n          TransportClient client,\n          ByteBuffer message,\n          RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StreamManager getStreamManager()",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void tearDown()",
				"documentation": ""
			},
			{
				"signature": "private FetchResult fetchChunks(List\u003cInteger\u003e chunkIndices) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onSuccess(int chunkIndex, ManagedBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onFailure(int chunkIndex, Throwable e)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fetchBufferChunk() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fetchFileChunk() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fetchNonExistentChunk() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fetchBothChunks() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fetchChunkAndNonExistent() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private static void assertBufferListsEqual(List\u003cManagedBuffer\u003e list0, List\u003cManagedBuffer\u003e list1)\n      throws Exception",
				"documentation": ""
			},
			{
				"signature": "private static void assertBuffersEqual(ManagedBuffer buffer0, ManagedBuffer buffer1)\n      throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.client.ChunkReceivedCallback",
			"org.apache.spark.network.server.RpcHandler",
			"org.apache.spark.network.server.StreamManager",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.ChunkFetchIntegrationSuite.FetchResult"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.ChunkFetchIntegrationSuite.FetchResult"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.ChunkFetchIntegrationSuite.FetchResult",
		"extends": "",
		"Methods": [
			{
				"signature": "public void releaseBuffers()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.ChunkFetchIntegrationSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.ChunkFetchRequestHandlerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void handleChunkFetchRequest() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.ChunkFetchRequestHandler",
			"org.apache.spark.network.server.NoOpRpcHandler"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.ExtendedChannelPromise",
		"extends": "io.netty.channel.DefaultChannelPromise",
		"Methods": [
			{
				"signature": "ExtendedChannelPromise(Channel channel)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ChannelPromise addListener(\n      GenericFutureListener\u003c? extends Future\u003c? super Void\u003e\u003e listener)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isSuccess()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ChannelPromise await() throws InterruptedException",
				"documentation": ""
			},
			{
				"signature": "public void finish(boolean success)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.ProtocolSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private void testServerToClient(Message msg)",
				"documentation": ""
			},
			{
				"signature": "private void testClientToServer(Message msg)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void requests()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void responses()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.ChunkFetchFailure",
			"org.apache.spark.network.protocol.ChunkFetchRequest",
			"org.apache.spark.network.protocol.ChunkFetchSuccess",
			"org.apache.spark.network.protocol.OneWayMessage",
			"org.apache.spark.network.protocol.RpcFailure",
			"org.apache.spark.network.protocol.RpcRequest",
			"org.apache.spark.network.protocol.RpcResponse",
			"org.apache.spark.network.protocol.StreamChunkId",
			"org.apache.spark.network.protocol.StreamFailure",
			"org.apache.spark.network.protocol.StreamRequest",
			"org.apache.spark.network.protocol.StreamResponse",
			"org.apache.spark.network.util.ByteArrayWritableChannel",
			"org.apache.spark.network.util.NettyUtils",
			"org.apache.spark.network.ProtocolSuite.FileRegionEncoder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.ProtocolSuite.FileRegionEncoder"
		]
	},
	{
		"documentation": "/**\n   * Handler to transform a FileRegion into a byte buffer. EmbeddedChannel doesn't actually transfer\n   * bytes, but messages, so this is needed so that the frame decoder on the receiving side can\n   * understand what MessageWithHeader actually contains.\n   */",
		"name": "org.apache.spark.network.ProtocolSuite.FileRegionEncoder",
		"extends": "io.netty.handler.codec.MessageToMessageEncoder",
		"Methods": [
			{
				"signature": "@Override\n    public void encode(ChannelHandlerContext ctx, FileRegion in, List\u003cObject\u003e out)\n      throws Exception",
				"documentation": "/**\n   * Handler to transform a FileRegion into a byte buffer. EmbeddedChannel doesn't actually transfer\n   * bytes, but messages, so this is needed so that the frame decoder on the receiving side can\n   * understand what MessageWithHeader actually contains.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.ByteArrayWritableChannel"
		],
		"usedBy": [
			"org.apache.spark.network.ProtocolSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Suite which ensures that requests that go without a response for the network timeout period are\n * failed, and the connection closed.\n *\n * In this suite, we use 10 seconds as the connection timeout, with some slack given in the tests,\n * to ensure stability in different test environments.\n */",
		"name": "org.apache.spark.network.RequestTimeoutIntegrationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp() throws Exception",
				"documentation": "/**\n * Suite which ensures that requests that go without a response for the network timeout period are\n * failed, and the connection closed.\n *\n * In this suite, we use 10 seconds as the connection timeout, with some slack given in the tests,\n * to ensure stability in different test environments.\n */"
			},
			{
				"signature": "@Override\n      public ManagedBuffer getChunk(long streamId, int chunkIndex)",
				"documentation": "/**\n * Suite which ensures that requests that go without a response for the network timeout period are\n * failed, and the connection closed.\n *\n * In this suite, we use 10 seconds as the connection timeout, with some slack given in the tests,\n * to ensure stability in different test environments.\n */"
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void timeoutInactiveRequests() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void receive(\n          TransportClient client,\n          ByteBuffer message,\n          RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StreamManager getStreamManager()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void timeoutCleanlyClosesClient() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void receive(\n          TransportClient client,\n          ByteBuffer message,\n          RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StreamManager getStreamManager()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void furtherRequestsDelay() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ManagedBuffer getChunk(long streamId, int chunkIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void receive(\n          TransportClient client,\n          ByteBuffer message,\n          RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StreamManager getStreamManager()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.server.RpcHandler",
			"org.apache.spark.network.server.StreamManager",
			"org.apache.spark.network.util.MapConfigProvider",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.RequestTimeoutIntegrationSuite.TestCallback"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.RequestTimeoutIntegrationSuite.TestCallback"
		]
	},
	{
		"documentation": "/**\n   * Callback which sets 'success' or 'failure' on completion.\n   * Additionally notifies all waiters on this callback when invoked.\n   */",
		"name": "org.apache.spark.network.RequestTimeoutIntegrationSuite.TestCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void onSuccess(ByteBuffer response)",
				"documentation": "/**\n   * Callback which sets 'success' or 'failure' on completion.\n   * Additionally notifies all waiters on this callback when invoked.\n   */"
			},
			{
				"signature": "@Override\n    public void onFailure(Throwable e)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onSuccess(int chunkIndex, ManagedBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(int chunkIndex, Throwable e)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.RpcResponseCallback",
			"org.apache.spark.network.client.ChunkReceivedCallback"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.RequestTimeoutIntegrationSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.RpcIntegrationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void setUp() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void receive(\n          TransportClient client,\n          ByteBuffer message,\n          RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StreamCallbackWithID receiveStream(\n          TransportClient client,\n          ByteBuffer messageHeader,\n          RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void receive(TransportClient client, ByteBuffer message)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StreamManager getStreamManager()",
				"documentation": ""
			},
			{
				"signature": "private static StreamCallbackWithID receiveStreamHelper(String msg)",
				"documentation": ""
			},
			{
				"signature": "@Override\n              public void onData(String streamId, ByteBuffer buf) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n              public void onComplete(String streamId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n              public void onFailure(String streamId, Throwable cause) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n              public String getID()",
				"documentation": ""
			},
			{
				"signature": "@Override\n              public void onData(String streamId, ByteBuffer buf) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n              public void onComplete(String streamId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n              public void onFailure(String streamId, Throwable cause) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n              public String getID()",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void tearDown()",
				"documentation": ""
			},
			{
				"signature": "private RpcResult sendRPC(String ... commands) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void onSuccess(ByteBuffer message)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void onFailure(Throwable e)",
				"documentation": ""
			},
			{
				"signature": "private RpcResult sendRpcWithStream(String... streams) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void singleRPC() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void doubleRPC() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void returnErrorRPC() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void throwErrorRPC() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void doubleTrouble() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sendSuccessAndFailure() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sendOneWayMessage() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sendRpcWithStreamOneAtATime() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sendRpcWithStreamConcurrently() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sendRpcWithStreamFailures() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void assertErrorsContain(Set\u003cString\u003e errors, Set\u003cString\u003e contains)",
				"documentation": ""
			},
			{
				"signature": "private void assertErrorAndClosed(RpcResult result, String expectedError)",
				"documentation": ""
			},
			{
				"signature": "private Pair\u003cSet\u003cString\u003e, Set\u003cString\u003e\u003e checkErrorsContain(\n      Set\u003cString\u003e errors,\n      Set\u003cString\u003e contains)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.RpcIntegrationSuite.RpcResult",
			"org.apache.spark.network.RpcIntegrationSuite.RpcStreamCallback",
			"org.apache.spark.network.RpcIntegrationSuite.VerifyingStreamCallback"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.RpcIntegrationSuite.RpcResult",
			"org.apache.spark.network.RpcIntegrationSuite.RpcStreamCallback",
			"org.apache.spark.network.RpcIntegrationSuite.VerifyingStreamCallback"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.RpcIntegrationSuite.RpcResult",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.RpcIntegrationSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.RpcIntegrationSuite.RpcStreamCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "RpcStreamCallback(String streamId, RpcResult res, Semaphore sem)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onSuccess(ByteBuffer message)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(Throwable e)",
				"documentation": ""
			}
		],
		"interfaces": [
			"RpcResponseCallback"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.RpcIntegrationSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.RpcIntegrationSuite.VerifyingStreamCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "VerifyingStreamCallback(String streamId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "void verify() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onData(String streamId, ByteBuffer buf) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onComplete(String streamId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(String streamId, Throwable cause) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String getID()",
				"documentation": ""
			}
		],
		"interfaces": [
			"StreamCallbackWithID"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.RpcIntegrationSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.StreamSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private static ByteBuffer createBuffer(int bufSize)",
				"documentation": ""
			},
			{
				"signature": "@BeforeClass\n  public static void setUp() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ManagedBuffer getChunk(long streamId, int chunkIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ManagedBuffer openStream(String streamId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void receive(\n          TransportClient client,\n          ByteBuffer message,\n          RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StreamManager getStreamManager()",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testZeroLengthStream() throws Throwable",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSingleStream() throws Throwable",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMultipleStreams() throws Throwable",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testConcurrentStreams() throws Throwable",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.RpcHandler",
			"org.apache.spark.network.server.StreamManager",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.StreamSuite.StreamTask",
			"org.apache.spark.network.StreamSuite.TestCallback"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.StreamSuite.StreamTask",
			"org.apache.spark.network.StreamSuite.TestCallback"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.StreamSuite.StreamTask",
		"extends": "",
		"Methods": [
			{
				"signature": "StreamTask(TransportClient client, String streamId, long timeoutMs)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void run()",
				"documentation": ""
			},
			{
				"signature": "public void check() throws Throwable",
				"documentation": ""
			}
		],
		"interfaces": [
			"Runnable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.StreamSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.StreamSuite.TestCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "TestCallback(OutputStream out)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onData(String streamId, ByteBuffer buf) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onComplete(String streamId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(String streamId, Throwable cause)",
				"documentation": ""
			},
			{
				"signature": "void waitForCompletion(long timeoutMs)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.StreamCallback"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.StreamSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.StreamTestHelper",
		"extends": "",
		"Methods": [
			{
				"signature": "static final String[] STREAMS =",
				"documentation": ""
			},
			{
				"signature": "private static ByteBuffer createBuffer(int bufSize)",
				"documentation": ""
			},
			{
				"signature": "StreamTestHelper() throws Exception",
				"documentation": ""
			},
			{
				"signature": "public ByteBuffer srcBuffer(String name)",
				"documentation": ""
			},
			{
				"signature": "public ManagedBuffer openStream(TransportConf conf, String streamId)",
				"documentation": ""
			},
			{
				"signature": "void cleanup()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.util.JavaUtils"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A ManagedBuffer implementation that contains 0, 1, 2, 3, ..., (len-1).\n *\n * Used for testing.\n */",
		"name": "org.apache.spark.network.TestManagedBuffer",
		"extends": "org.apache.spark.network.buffer.ManagedBuffer",
		"Methods": [
			{
				"signature": "public TestManagedBuffer(int len)",
				"documentation": "/**\n * A ManagedBuffer implementation that contains 0, 1, 2, 3, ..., (len-1).\n *\n * Used for testing.\n */"
			},
			{
				"signature": "@Override\n  public long size()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ByteBuffer nioByteBuffer() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public InputStream createInputStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer retain()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer release()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object convertToNetty() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer"
		],
		"usedBy": [
			"org.apache.spark.network.protocol.MessageWithHeaderSuite",
			"org.apache.spark.network.server.OneForOneStreamManagerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.TestUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static String getLocalHost()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.AppIsolationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.TransportRequestHandlerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void handleStreamRequest() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void handleMergedBlockMetaRequest() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void receive(\n          TransportClient client,\n          ByteBuffer message,\n          RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StreamManager getStreamManager()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public MergedBlockMetaReqHandler getMergedBlockMetaReqHandler()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.NoOpRpcHandler",
			"org.apache.spark.network.server.TransportRequestHandler"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.TransportResponseHandlerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void handleSuccessfulFetch() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void handleFailedFetch() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void clearAllOutstandingRequests() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void handleSuccessfulRPC() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void handleFailedRPC() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testActiveStreams() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void failOutstandingStreamCallbackOnClose() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void failOutstandingStreamCallbackOnException() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void handleSuccessfulMergedBlockMeta() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void handleFailedMergedBlockMeta() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.client.TransportResponseHandler",
			"org.apache.spark.network.protocol.ChunkFetchFailure",
			"org.apache.spark.network.protocol.ChunkFetchSuccess",
			"org.apache.spark.network.protocol.MergedBlockMetaSuccess",
			"org.apache.spark.network.protocol.RpcFailure",
			"org.apache.spark.network.protocol.RpcResponse",
			"org.apache.spark.network.protocol.StreamChunkId",
			"org.apache.spark.network.protocol.StreamFailure",
			"org.apache.spark.network.protocol.StreamResponse",
			"org.apache.spark.network.util.TransportFrameDecoder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.client.TransportClientFactorySuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "private void testClientReuse(int maxConnections, boolean concurrent)\n    throws IOException, InterruptedException",
				"documentation": "/**\n   * Request a bunch of clients to a single server to test\n   * we create up to maxConnections of clients.\n   *\n   * If concurrent is true, create multiple threads to create clients in parallel.\n   */"
			},
			{
				"signature": "@Test\n  public void reuseClientsUpToConfigVariable() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void reuseClientsUpToConfigVariableConcurrent() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void returnDifferentClientsForDifferentServers() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void neverReturnInactiveClients() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void closeBlockClientsWithFactory() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void closeIdleConnectionForRequestTimeOut() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void closeFactoryBeforeCreateClient()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fastFailConnectionInTimeWindow()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.server.NoOpRpcHandler",
			"org.apache.spark.network.util.ConfigProvider",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.TransportConf"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.crypto.AuthEngineSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void setUp()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAuthEngine() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCorruptChallengeAppId() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCorruptChallengeSalt() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCorruptChallengeCiphertext() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCorruptResponseAppId() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCorruptResponseSalt() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCorruptServerCiphertext() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFixedChallenge() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFixedChallengeResponse() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMismatchedSecret() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEncryptedMessage() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEncryptedMessageWhenTransferringZeroBytes() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.ByteArrayWritableChannel",
			"org.apache.spark.network.util.TransportConf"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.crypto.AuthIntegrationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@After\n  public void cleanUp() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNewAuth() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAuthFailure() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSaslServerFallback() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSaslClientFallback() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAuthReplay() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLargeMessageEncryption() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.sasl.SaslServerBootstrap",
			"org.apache.spark.network.server.RpcHandler",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.crypto.AuthIntegrationSuite.AuthTestCtx"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.crypto.AuthIntegrationSuite.AuthTestCtx"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.crypto.AuthIntegrationSuite.AuthTestCtx",
		"extends": "",
		"Methods": [
			{
				"signature": "AuthTestCtx() throws Exception",
				"documentation": ""
			},
			{
				"signature": "AuthTestCtx(RpcHandler rpcHandler) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void createServer(String secret) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void createServer(String secret, boolean enableAes) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void createClient(String secret) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void createClient(String secret, boolean enableAes) throws Exception",
				"documentation": ""
			},
			{
				"signature": "void close()",
				"documentation": ""
			},
			{
				"signature": "private SecretKeyHolder createKeyHolder(String secret)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.sasl.SaslServerBootstrap",
			"org.apache.spark.network.server.RpcHandler",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.TransportConf"
		],
		"usedBy": [
			"org.apache.spark.network.crypto.AuthIntegrationSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.crypto.AuthMessagesSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private static String string()",
				"documentation": ""
			},
			{
				"signature": "private static byte[] byteArray()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPublicKeyEncodeDecode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.crypto.TransportCipherSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testBufferNotLeaksOnInternalError() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n      CryptoOutputStream createOutputStream(WritableByteChannel ch)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      CryptoInputStream createInputStream(ReadableByteChannel ch) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.TransportConf"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Tests for {@link Encoders}.\n */",
		"name": "org.apache.spark.network.protocol.EncodersSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testRoaringBitmapEncodeDecode()",
				"documentation": "/**\n * Tests for {@link Encoders}.\n */"
			},
			{
				"signature": "@Test (expected = java.nio.BufferOverflowException.class)\n  public void testRoaringBitmapEncodeShouldFailWhenBufferIsSmall()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBitmapArraysEncodeDecode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test for {@link MergedBlockMetaSuccess}.\n */",
		"name": "org.apache.spark.network.protocol.MergedBlockMetaSuccessSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testMergedBlocksMetaEncodeDecode() throws Exception",
				"documentation": "/**\n * Test for {@link MergedBlockMetaSuccess}.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.protocol.MessageWithHeaderSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testSingleWrite() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShortWrite() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testByteBufBody() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCompositeByteBufBodySingleBuffer() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCompositeByteBufBodyMultipleBuffers() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void testByteBufBody(ByteBuf header) throws Exception",
				"documentation": "/**\n   * Test writing a {@link MessageWithHeader} using the given {@link ByteBuf} as header.\n   *\n   * @param header the header to use.\n   * @throws Exception thrown on error.\n   */"
			},
			{
				"signature": "@Test\n  public void testDeallocateReleasesManagedBuffer() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void testFileRegionBody(int totalWrites, int writesPerCall) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private ByteBuf doWrite(MessageWithHeader msg, int minExpectedWrites) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer",
			"org.apache.spark.network.TestManagedBuffer",
			"org.apache.spark.network.protocol.MessageWithHeaderSuite.TestFileRegion"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.protocol.MessageWithHeaderSuite.TestFileRegion"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.protocol.MessageWithHeaderSuite.TestFileRegion",
		"extends": "org.apache.spark.network.util.AbstractFileRegion",
		"Methods": [
			{
				"signature": "TestFileRegion(int totalWrites, int writesPerCall)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long count()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long position()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long transferred()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long transferTo(WritableByteChannel target, long position) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void deallocate()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.protocol.MessageWithHeaderSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Jointly tests SparkSaslClient and SparkSaslServer, as both are black boxes.\n */",
		"name": "org.apache.spark.network.sasl.SparkSaslSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public String getSaslUser(String appId)",
				"documentation": "/** Provides a secret key holder which returns secret key == appId */"
			},
			{
				"signature": "@Override\n    public String getSecretKey(String appId)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMatching()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNonMatching()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSaslAuthentication() throws Throwable",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSaslEncryption() throws Throwable",
				"documentation": ""
			},
			{
				"signature": "private static void testBasicSasl(boolean encrypt) throws Throwable",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEncryptedMessage() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEncryptedMessageChunking() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFileRegionEncryption() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testServerAlwaysEncrypt()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDataEncryptionIsActuallyEnabled() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRpcHandlerDelegate() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDelegates() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.util.ByteArrayWritableChannel",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.sasl.SparkSaslSuite.SaslTestCtx",
			"org.apache.spark.network.sasl.SparkSaslSuite.EncryptionCheckerBootstrap",
			"org.apache.spark.network.sasl.SparkSaslSuite.EncryptionDisablerBootstrap"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.sasl.SparkSaslSuite.SaslTestCtx",
			"org.apache.spark.network.sasl.SparkSaslSuite.EncryptionCheckerBootstrap",
			"org.apache.spark.network.sasl.SparkSaslSuite.EncryptionDisablerBootstrap"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.SparkSaslSuite.SaslTestCtx",
		"extends": "",
		"Methods": [
			{
				"signature": "SaslTestCtx(\n        RpcHandler rpcHandler,\n        boolean encrypt,\n        boolean disableClientEncryption)\n      throws Exception",
				"documentation": ""
			},
			{
				"signature": "SaslTestCtx(\n        RpcHandler rpcHandler,\n        boolean encrypt,\n        boolean disableClientEncryption,\n        Map\u003cString, String\u003e extraConf)\n      throws Exception",
				"documentation": ""
			},
			{
				"signature": "public void close()",
				"documentation": ""
			}
		],
		"interfaces": [
			"AutoCloseable"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.util.MapConfigProvider",
			"org.apache.spark.network.util.TransportConf"
		],
		"usedBy": [
			"org.apache.spark.network.sasl.SparkSaslSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.SparkSaslSuite.EncryptionCheckerBootstrap",
		"extends": "io.netty.channel.ChannelOutboundHandlerAdapter",
		"Methods": [
			{
				"signature": "EncryptionCheckerBootstrap(String encryptHandlerName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)\n      throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public RpcHandler doBootstrap(Channel channel, RpcHandler rpcHandler)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.server.TransportServerBootstrap"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.sasl.SparkSaslSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.SparkSaslSuite.EncryptionDisablerBootstrap",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void doBootstrap(TransportClient client, Channel channel)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.TransportClientBootstrap"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.sasl.SparkSaslSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.server.OneForOneStreamManagerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "private ManagedBuffer getChunk(OneForOneStreamManager manager, long streamId, int chunkIndex)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMissingChunk()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void managedBuffersAreFreedWhenConnectionIsClosed()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void streamStatesAreFreedWhenConnectionIsClosedEvenIfBufferIteratorThrowsException()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void streamStatesAreFreeOrNotWhenConnectionIsClosed()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TestManagedBuffer"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.CryptoUtilsSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testConfConversion()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.NettyMemoryMetricsSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private void setUp(boolean enableVerboseMetrics)",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  @SuppressWarnings(\"unchecked\")\n  public void testGeneralNettyMemoryMetrics() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  @SuppressWarnings(\"unchecked\")\n  public void testAdditionalMetrics() throws IOException, InterruptedException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.server.NoOpRpcHandler"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Tests for {@link TimerWithCustomTimeUnit} */",
		"name": "org.apache.spark.network.util.TimerWithCustomUnitSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testTimerWithMillisecondTimeUnit()",
				"documentation": "/** Tests for {@link TimerWithCustomTimeUnit} */"
			},
			{
				"signature": "@Test\n  public void testTimerWithNanosecondTimeUnit()",
				"documentation": ""
			},
			{
				"signature": "private void testTimerWithCustomTimeUnit(TimeUnit timeUnit)",
				"documentation": ""
			},
			{
				"signature": "Duration[] durations =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTimingViaContext()",
				"documentation": ""
			},
			{
				"signature": "Duration[] durations =",
				"documentation": ""
			},
			{
				"signature": "private static long toTimeUnit(Duration duration, TimeUnit timeUnit)",
				"documentation": ""
			},
			{
				"signature": "private static double toTimeUnitFloating(Duration duration, TimeUnit timeUnit)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.TimerWithCustomUnitSuite.ManualClock"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.util.TimerWithCustomUnitSuite.ManualClock"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.TimerWithCustomUnitSuite.ManualClock",
		"extends": "com.codahale.metrics.Clock",
		"Methods": [
			{
				"signature": "void advance(long nanos)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long getTick()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.util.TimerWithCustomUnitSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.TransportFrameDecoderSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@AfterClass\n  public static void cleanup()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFrameDecoding() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testConsolidationPerf() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInterception() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRetainedFrames() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSplitLengthField() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNegativeFrameSize()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEmptyFrame()",
				"documentation": ""
			},
			{
				"signature": "private ByteBuf createAndFeedFrames(\n      int frameCount,\n      TransportFrameDecoder decoder,\n      ChannelHandlerContext ctx) throws Exception",
				"documentation": "/**\n   * Creates a number of randomly sized frames and feed them to the given decoder, verifying\n   * that the frames were read.\n   */"
			},
			{
				"signature": "private void verifyAndCloseDecoder(\n      TransportFrameDecoder decoder,\n      ChannelHandlerContext ctx,\n      ByteBuf data) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void testInvalidFrame(long size) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private ChannelHandlerContext mockChannelHandlerContext()",
				"documentation": ""
			},
			{
				"signature": "private void release(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.TransportFrameDecoderSuite.MockInterceptor"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.util.TransportFrameDecoderSuite.MockInterceptor"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.util.TransportFrameDecoderSuite.MockInterceptor",
		"extends": "",
		"Methods": [
			{
				"signature": "MockInterceptor(int readCount)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean handle(ByteBuf data) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void exceptionCaught(Throwable cause) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void channelInactive() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [
			"TransportFrameDecoder.Interceptor"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.util.TransportFrameDecoderSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A class that manages shuffle secret used by the external shuffle service.\n */",
		"name": "org.apache.spark.network.sasl.ShuffleSecretManager",
		"extends": "",
		"Methods": [
			{
				"signature": "public ShuffleSecretManager()",
				"documentation": "/**\n * A class that manages shuffle secret used by the external shuffle service.\n */"
			},
			{
				"signature": "public void registerApp(String appId, String shuffleSecret)",
				"documentation": "/**\n   * Register an application with its secret.\n   * Executors need to first authenticate themselves with the same secret before\n   * fetching shuffle files written by other executors in this application.\n   */"
			},
			{
				"signature": "public void registerApp(String appId, ByteBuffer shuffleSecret)",
				"documentation": "/**\n   * Register an application with its secret specified as a byte buffer.\n   */"
			},
			{
				"signature": "public void unregisterApp(String appId)",
				"documentation": "/**\n   * Unregister an application along with its secret.\n   * This is called when the application terminates.\n   */"
			},
			{
				"signature": "@Override\n  public String getSaslUser(String appId)",
				"documentation": "/**\n   * Return the Spark user for authenticating SASL connections.\n   */"
			},
			{
				"signature": "@Override\n  public String getSecretKey(String appId)",
				"documentation": "/**\n   * Return the secret key registered with the given application.\n   * This key is used to authenticate the executors before they can fetch shuffle files\n   * written by this application from the external shuffle service. If the specified\n   * application is not registered, return null.\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.network.sasl.SecretKeyHolder"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.JavaUtils"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.BlockFetchingListener",
		"extends": "org.apache.spark.network.shuffle.BlockTransferListener",
		"Methods": [
			{
				"signature": "@Override\n  default void onBlockTransferSuccess(String blockId, ManagedBuffer data)",
				"documentation": "/**\n   * Called at least once per block upon failures.\n   */"
			},
			{
				"signature": "@Override\n  default void onBlockTransferFailure(String blockId, Throwable exception)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  default String getTransferType()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.shuffle.RetryingBlockTransferListener"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Callback to handle block push success and failure. This interface and\n * {@link BlockFetchingListener} are unified under {@link BlockTransferListener} to allow\n * code reuse for handling block push and fetch retry.\n */",
		"name": "org.apache.spark.network.shuffle.BlockPushingListener",
		"extends": "org.apache.spark.network.shuffle.BlockTransferListener",
		"Methods": [
			{
				"signature": "@Override\n  default void onBlockTransferSuccess(String blockId, ManagedBuffer data)",
				"documentation": "/**\n   * Called at least once per block upon failures.\n   */"
			},
			{
				"signature": "@Override\n  default void onBlockTransferFailure(String blockId, Throwable exception)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  default String getTransferType()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.shuffle.RetryingBlockTransferListener"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Provides an interface for reading both shuffle files and RDD blocks, either from an Executor\n * or external service.\n */",
		"name": "org.apache.spark.network.shuffle.BlockStoreClient",
		"extends": "",
		"Methods": [
			{
				"signature": "public Cause diagnoseCorruption(\n      String host,\n      int port,\n      String execId,\n      int shuffleId,\n      long mapId,\n      int reduceId,\n      long checksum,\n      String algorithm)",
				"documentation": "/**\n   * Send the diagnosis request for the corrupted shuffle block to the server.\n   *\n   * @param host the host of the remote node.\n   * @param port the port of the remote node.\n   * @param execId the executor id.\n   * @param shuffleId the shuffleId of the corrupted shuffle block\n   * @param mapId the mapId of the corrupted shuffle block\n   * @param reduceId the reduceId of the corrupted shuffle block\n   * @param checksum the shuffle checksum which calculated at client side for the corrupted\n   *                 shuffle block\n   * @return The cause of the shuffle block corruption\n   */"
			},
			{
				"signature": "public MetricSet shuffleMetrics()",
				"documentation": "/**\n   * Get the shuffle MetricsSet from BlockStoreClient, this will be used in MetricsSystem to\n   * get the Shuffle related metrics.\n   */"
			},
			{
				"signature": "protected void checkInit()",
				"documentation": ""
			},
			{
				"signature": "public void setAppAttemptId(String appAttemptId)",
				"documentation": ""
			},
			{
				"signature": "public String getAppAttemptId()",
				"documentation": ""
			},
			{
				"signature": "public void getHostLocalDirs(\n      String host,\n      int port,\n      String[] execIds,\n      CompletableFuture\u003cMap\u003cString, String[]\u003e\u003e hostLocalDirsCompletable)",
				"documentation": "/**\n   * Request the local disk directories for executors which are located at the same host with\n   * the current BlockStoreClient(it can be ExternalBlockStoreClient or NettyBlockTransferService).\n   *\n   * @param host the host of BlockManager or ExternalShuffleService. It should be the same host\n   *             with current BlockStoreClient.\n   * @param port the port of BlockManager or ExternalShuffleService.\n   * @param execIds a collection of executor Ids, which specifies the target executors that we\n   *                want to get their local directories. There could be multiple executor Ids if\n   *                BlockStoreClient is implemented by ExternalBlockStoreClient since the request\n   *                handler, ExternalShuffleService, can serve multiple executors on the same node.\n   *                Or, only one executor Id if BlockStoreClient is implemented by\n   *                NettyBlockTransferService.\n   * @param hostLocalDirsCompletable a CompletableFuture which contains a map from executor Id\n   *                                 to its local directories if the request handler replies\n   *                                 successfully. Otherwise, it contains a specific error.\n   */"
			},
			{
				"signature": "public void pushBlocks(\n      String host,\n      int port,\n      String[] blockIds,\n      ManagedBuffer[] buffers,\n      BlockPushingListener listener)",
				"documentation": "/**\n   * Push a sequence of shuffle blocks in a best-effort manner to a remote node asynchronously.\n   * These shuffle blocks, along with blocks pushed by other clients, will be merged into\n   * per-shuffle partition merged shuffle files on the destination node.\n   *\n   * @param host the host of the remote node.\n   * @param port the port of the remote node.\n   * @param blockIds block ids to be pushed\n   * @param buffers buffers to be pushed\n   * @param listener the listener to receive block push status.\n   *\n   * @since 3.1.0\n   */"
			},
			{
				"signature": "public void finalizeShuffleMerge(\n      String host,\n      int port,\n      int shuffleId,\n      int shuffleMergeId,\n      MergeFinalizerListener listener)",
				"documentation": "/**\n   * Invoked by Spark driver to notify external shuffle services to finalize the shuffle merge\n   * for a given shuffle. This allows the driver to start the shuffle reducer stage after properly\n   * finishing the shuffle merge process associated with the shuffle mapper stage.\n   *\n   * @param host host of shuffle server\n   * @param port port of shuffle server.\n   * @param shuffleId shuffle ID of the shuffle to be finalized\n   * @param shuffleMergeId shuffleMergeId is used to uniquely identify merging process\n   *                       of shuffle by an indeterminate stage attempt.\n   * @param listener the listener to receive MergeStatuses\n   *\n   * @since 3.1.0\n   */"
			},
			{
				"signature": "public void getMergedBlockMeta(\n      String host,\n      int port,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId,\n      MergedBlocksMetaListener listener)",
				"documentation": "/**\n   * Get the meta information of a merged block from the remote shuffle service.\n   *\n   * @param host the host of the remote node.\n   * @param port the port of the remote node.\n   * @param shuffleId shuffle id.\n   * @param shuffleMergeId shuffleMergeId is used to uniquely identify merging process\n   *                       of shuffle by an indeterminate stage attempt.\n   * @param reduceId reduce id.\n   * @param listener the listener to receive chunk counts.\n   *\n   * @since 3.2.0\n   */"
			}
		],
		"interfaces": [
			"java.io.Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.shuffle.ExternalBlockStoreClient"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.client.RpcResponseCallback"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This interface unifies both {@link BlockFetchingListener} and {@link BlockPushingListener}\n * under a single interface to allow code reuse, while also keeping the existing public interface\n * to facilitate backward compatibility.\n */",
		"name": "org.apache.spark.network.shuffle.BlockTransferListener",
		"extends": "java.util.EventListener",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.shuffle.BlockFetchingListener",
			"org.apache.spark.network.shuffle.BlockPushingListener"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.Constants",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A handle on the file used when fetching remote data to disk.  Used to ensure the lifecycle of\n * writing the data, reading it back, and then cleaning it up is followed.  Specific implementations\n * may also handle encryption.  The data can be read only via DownloadFileWritableChannel,\n * which ensures data is not read until after the writer is closed.\n */",
		"name": "org.apache.spark.network.shuffle.DownloadFile",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.shuffle.SimpleDownloadFile"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A manager to create temp block files used when fetching remote data to reduce the memory usage.\n * It will clean files when they won't be used any more.\n */",
		"name": "org.apache.spark.network.shuffle.DownloadFileManager",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A channel for writing data which is fetched to disk, which allows access to the written data only\n * after the writer has been closed.  Used with DownloadFile and DownloadFileManager.\n */",
		"name": "org.apache.spark.network.shuffle.DownloadFileWritableChannel",
		"extends": "java.nio.channels.WritableByteChannel",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.shuffle.SimpleDownloadWritableChannel"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Plugs into {@link RetryingBlockTransferor} to further control when an exception should be retried\n * and logged.\n * Note: {@link RetryingBlockTransferor} will delegate the exception to this handler only when\n * - remaining retries \u003c max retries\n * - exception is an IOException\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.ErrorHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "default boolean shouldLogError(Throwable t)",
				"documentation": "/**\n * Plugs into {@link RetryingBlockTransferor} to further control when an exception should be retried\n * and logged.\n * Note: {@link RetryingBlockTransferor} will delegate the exception to this handler only when\n * - remaining retries \u003c max retries\n * - exception is an IOException\n *\n * @since 3.1.0\n */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.shuffle.BlockPushErrorHandler",
			"org.apache.spark.network.shuffle.BlockFetchErrorHandler"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.BlockPushErrorHandler",
			"org.apache.spark.network.shuffle.BlockFetchErrorHandler"
		]
	},
	{
		"documentation": "/**\n   * The error handler for pushing shuffle blocks to remote shuffle services.\n   *\n   * @since 3.1.0\n   */",
		"name": "org.apache.spark.network.shuffle.BlockPushErrorHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public boolean shouldRetryError(Throwable t)",
				"documentation": "/**\n     * String constant used for generating exception messages indicating the server rejecting a\n     * shuffle finalize request since shuffle blocks of a higher shuffleMergeId for a shuffle is\n     * already being pushed. This typically happens in the case of indeterminate stage retries\n     * where if a stage attempt fails then the entirety of the shuffle output needs to be rolled\n     * back. For more details refer SPARK-23243, SPARK-25341 and SPARK-32923.\n     */"
			},
			{
				"signature": "@Override\n    public boolean shouldLogError(Throwable t)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.shuffle.ErrorHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.BlockFetchErrorHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public boolean shouldRetryError(Throwable t)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean shouldLogError(Throwable t)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.shuffle.ErrorHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ExecutorDiskUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static String getFilePath(String[] localDirs, int subDirsPerLocalDir, String filename)",
				"documentation": "/**\n   * Hashes a filename into the corresponding local directory, in a manner consistent with\n   * Spark's DiskBlockManager.getFile().\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.JavaUtils"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * RPC Handler for a server which can serve both RDD blocks and shuffle blocks from outside\n * of an Executor process.\n *\n * Handles registering executors and opening shuffle or disk persisted RDD blocks from them.\n * Blocks are registered with the \"one-for-one\" strategy, meaning each Transport-layer Chunk\n * is equivalent to one block.\n */",
		"name": "org.apache.spark.network.shuffle.ExternalBlockHandler",
		"extends": "org.apache.spark.network.server.RpcHandler",
		"Methods": [
			{
				"signature": "public ExternalBlockHandler(TransportConf conf, File registeredExecutorFile)\n    throws IOException",
				"documentation": "/**\n * RPC Handler for a server which can serve both RDD blocks and shuffle blocks from outside\n * of an Executor process.\n *\n * Handles registering executors and opening shuffle or disk persisted RDD blocks from them.\n * Blocks are registered with the \"one-for-one\" strategy, meaning each Transport-layer Chunk\n * is equivalent to one block.\n */"
			},
			{
				"signature": "public ExternalBlockHandler(\n      TransportConf conf,\n      File registeredExecutorFile,\n      MergedShuffleFileManager mergeManager) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public ExternalShuffleBlockResolver getBlockResolver()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public ExternalBlockHandler(\n      OneForOneStreamManager streamManager,\n      ExternalShuffleBlockResolver blockManager)",
				"documentation": "/** Enables mocking out the StreamManager and BlockManager. */"
			},
			{
				"signature": "@VisibleForTesting\n  public ExternalBlockHandler(\n      OneForOneStreamManager streamManager,\n      ExternalShuffleBlockResolver blockManager,\n      MergedShuffleFileManager mergeManager)",
				"documentation": "/** Enables mocking out the StreamManager, BlockManager, and MergeManager. */"
			},
			{
				"signature": "@Override\n  public void receive(TransportClient client, ByteBuffer message, RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public StreamCallbackWithID receiveStream(\n      TransportClient client,\n      ByteBuffer messageHeader,\n      RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "protected void handleMessage(\n      BlockTransferMessage msgObj,\n      TransportClient client,\n      RpcResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void receiveMergeBlockMetaReq(\n      TransportClient client,\n      MergedBlockMetaRequest metaRequest,\n      MergedBlockMetaResponseCallback callback)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MergedBlockMetaReqHandler getMergedBlockMetaReqHandler()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void exceptionCaught(Throwable cause, TransportClient client)",
				"documentation": ""
			},
			{
				"signature": "public MetricSet getAllMetrics()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public StreamManager getStreamManager()",
				"documentation": ""
			},
			{
				"signature": "public void applicationRemoved(String appId, boolean cleanupLocalDirs)",
				"documentation": "/**\n   * Removes an application (once it has been terminated), and optionally will clean up any\n   * local directories associated with the executors of that application in a separate thread.\n   */"
			},
			{
				"signature": "public void executorRemoved(String executorId, String appId)",
				"documentation": "/**\n   * Clean up any non-shuffle files in any local directories associated with an finished executor.\n   */"
			},
			{
				"signature": "public void close()",
				"documentation": ""
			},
			{
				"signature": "private void checkAuth(TransportClient client, String appId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelActive(TransportClient client)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void channelInactive(TransportClient client)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.server.RpcHandler.MergedBlockMetaReqHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.OneForOneStreamManager",
			"org.apache.spark.network.util.NettyUtils",
			"org.apache.spark.network.util.TimerWithCustomTimeUnit",
			"org.apache.spark.network.shuffle.ShuffleMetrics",
			"org.apache.spark.network.shuffle.ManagedBufferIterator"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.ShuffleMetrics",
			"org.apache.spark.network.shuffle.ManagedBufferIterator",
			"org.apache.spark.network.shuffle.ShuffleManagedBufferIterator",
			"org.apache.spark.network.shuffle.ShuffleChunkManagedBufferIterator"
		]
	},
	{
		"documentation": "/**\n   * A simple class to wrap all shuffle service wrapper metrics\n   */",
		"name": "org.apache.spark.network.shuffle.ShuffleMetrics",
		"extends": "",
		"Methods": [
			{
				"signature": "public ShuffleMetrics()",
				"documentation": "/**\n   * A simple class to wrap all shuffle service wrapper metrics\n   */"
			},
			{
				"signature": "@Override\n    public Map\u003cString, Metric\u003e getMetrics()",
				"documentation": ""
			}
		],
		"interfaces": [
			"com.codahale.metrics.MetricSet"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.TimerWithCustomTimeUnit"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockHandler"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ManagedBufferIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "ManagedBufferIterator(OpenBlocks msg)",
				"documentation": ""
			},
			{
				"signature": "private int[] rddAndSplitIds(String[] blockIds)",
				"documentation": ""
			},
			{
				"signature": "private int[] shuffleMapIdAndReduceIds(String[] blockIds, int shuffleId)",
				"documentation": "/**\n     * @param blockIds Regular shuffle blockIds starts with SHUFFLE_BLOCK_ID to be parsed\n     * @param shuffleId shuffle blocks shuffleId\n     * @return mapId and reduceIds of the shuffle blocks in the same order as that of the blockIds\n     *\n     * Regular shuffle blocks format should be shuffle_$shuffleId_$mapId_$reduceId\n     */"
			},
			{
				"signature": "private int[] shuffleReduceIdAndChunkIds(\n        String[] blockIds,\n        int shuffleId,\n        int shuffleMergeId)",
				"documentation": "/**\n     * @param blockIds Shuffle merged chunks starts with SHUFFLE_CHUNK_ID to be parsed\n     * @param shuffleId shuffle blocks shuffleId\n     * @param shuffleMergeId shuffleMergeId is used to uniquely identify merging process\n     *                       of shuffle by an indeterminate stage attempt.\n     * @return reduceId and chunkIds of the shuffle chunks in the same order as that of the\n     *         blockIds\n     *\n     * Shuffle merged chunks format should be\n     * shuffleChunk_$shuffleId_$shuffleMergeId_$reduceId_$chunkId\n     */"
			},
			{
				"signature": "@Override\n    public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public ManagedBuffer next()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Iterator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockHandler"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ShuffleManagedBufferIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "ShuffleManagedBufferIterator(FetchShuffleBlocks msg)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public ManagedBuffer next()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Iterator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ShuffleChunkManagedBufferIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "ShuffleChunkManagedBufferIterator(FetchShuffleBlockChunks msg)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public ManagedBuffer next()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Iterator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Client for reading both RDD blocks and shuffle blocks which points to an external\n * (outside of executor) server. This is instead of reading blocks directly from other executors\n * (via BlockTransferService), which has the downside of losing the data if we lose the executors.\n */",
		"name": "org.apache.spark.network.shuffle.ExternalBlockStoreClient",
		"extends": "org.apache.spark.network.shuffle.BlockStoreClient",
		"Methods": [
			{
				"signature": "public ExternalBlockStoreClient(\n      TransportConf conf,\n      SecretKeyHolder secretKeyHolder,\n      boolean authEnabled,\n      long registrationTimeoutMs)",
				"documentation": "/**\n   * Creates an external shuffle client, with SASL optionally enabled. If SASL is not enabled,\n   * then secretKeyHolder may be null.\n   */"
			},
			{
				"signature": "public void init(String appId)",
				"documentation": "/**\n   * Initializes the BlockStoreClient, specifying this Executor's appId.\n   * Must be called before any other method on the BlockStoreClient.\n   */"
			},
			{
				"signature": "@Override\n  public void setAppAttemptId(String appAttemptId)",
				"documentation": ""
			},
			{
				"signature": "private void setComparableAppAttemptId(String appAttemptId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void fetchBlocks(\n      String host,\n      int port,\n      String execId,\n      String[] blockIds,\n      BlockFetchingListener listener,\n      DownloadFileManager downloadFileManager)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void pushBlocks(\n      String host,\n      int port,\n      String[] blockIds,\n      ManagedBuffer[] buffers,\n      BlockPushingListener listener)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void finalizeShuffleMerge(\n      String host,\n      int port,\n      int shuffleId,\n      int shuffleMergeId,\n      MergeFinalizerListener listener)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void getMergedBlockMeta(\n      String host,\n      int port,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId,\n      MergedBlocksMetaListener listener)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MetricSet shuffleMetrics()",
				"documentation": ""
			},
			{
				"signature": "public void registerWithShuffleServer(\n      String host,\n      int port,\n      String execId,\n      ExecutorShuffleInfo executorInfo) throws IOException, InterruptedException",
				"documentation": "/**\n   * Registers this executor with an external shuffle server. This registration is required to\n   * inform the shuffle server about where and how we store our shuffle files.\n   *\n   * @param host Host of shuffle server.\n   * @param port Port of shuffle server.\n   * @param execId This Executor's id.\n   * @param executorInfo Contains all info necessary for the service to find our shuffle files.\n   */"
			},
			{
				"signature": "public Future\u003cInteger\u003e removeBlocks(\n      String host,\n      int port,\n      String execId,\n      String[] blockIds) throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.client.MergedBlockMetaResponseCallback",
			"org.apache.spark.network.client.RpcResponseCallback",
			"org.apache.spark.network.crypto.AuthClientBootstrap",
			"org.apache.spark.network.server.NoOpRpcHandler"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Manages converting shuffle BlockIds into physical segments of local files, from a process outside\n * of Executors. Each Executor must register its own configuration about where it stores its files\n * (local dirs) and how (shuffle manager). The logic for retrieval of individual files is replicated\n * from Spark's IndexShuffleBlockResolver.\n */",
		"name": "org.apache.spark.network.shuffle.ExternalShuffleBlockResolver",
		"extends": "",
		"Methods": [
			{
				"signature": "public ExternalShuffleBlockResolver(TransportConf conf, File registeredExecutorFile)\n      throws IOException",
				"documentation": "/**\n   *  Caches index file information so that we can avoid open/close the index files\n   *  for each block fetch.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  ExternalShuffleBlockResolver(\n      TransportConf conf,\n      File registeredExecutorFile,\n      Executor directoryCleaner) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public ShuffleIndexInformation load(String filePath) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public int getRegisteredExecutorsSize()",
				"documentation": ""
			},
			{
				"signature": "public void registerExecutor(\n      String appId,\n      String execId,\n      ExecutorShuffleInfo executorInfo)",
				"documentation": "/** Registers a new Executor with all the configuration we need to find its shuffle files. */"
			},
			{
				"signature": "public ManagedBuffer getBlockData(\n      String appId,\n      String execId,\n      int shuffleId,\n      long mapId,\n      int reduceId)",
				"documentation": "/**\n   * Obtains a FileSegmentManagedBuffer from a single block (shuffleId, mapId, reduceId).\n   */"
			},
			{
				"signature": "public ManagedBuffer getContinuousBlocksData(\n      String appId,\n      String execId,\n      int shuffleId,\n      long mapId,\n      int startReduceId,\n      int endReduceId)",
				"documentation": "/**\n   * Obtains a FileSegmentManagedBuffer from (shuffleId, mapId, [startReduceId, endReduceId)).\n   * We make assumptions about how the hash and sort based shuffles store their data.\n   */"
			},
			{
				"signature": "public ManagedBuffer getRddBlockData(\n      String appId,\n      String execId,\n      int rddId,\n      int splitIndex)",
				"documentation": ""
			},
			{
				"signature": "public void applicationRemoved(String appId, boolean cleanupLocalDirs)",
				"documentation": "/**\n   * Removes our metadata of all executors registered for the given application, and optionally\n   * also deletes the local directories associated with the executors of that application in a\n   * separate thread.\n   *\n   * It is not valid to call registerExecutor() for an executor with this appId after invoking\n   * this method.\n   */"
			},
			{
				"signature": "public void executorRemoved(String executorId, String appId)",
				"documentation": "/**\n   * Removes all the files which cannot be served by the external shuffle service (non-shuffle and\n   * non-RDD files) in any local directories associated with the finished executor.\n   */"
			},
			{
				"signature": "private void deleteExecutorDirs(String[] dirs)",
				"documentation": "/**\n   * Synchronously deletes each directory one at a time.\n   * Should be executed in its own thread, as this may take a long time.\n   */"
			},
			{
				"signature": "private void deleteNonShuffleServiceServedFiles(String[] dirs)",
				"documentation": "/**\n   * Synchronously deletes files not served by shuffle service in each directory recursively.\n   * Should be executed in its own thread, as this may take a long time.\n   */"
			},
			{
				"signature": "private ManagedBuffer getSortBasedShuffleBlockData(\n    ExecutorShuffleInfo executor, int shuffleId, long mapId, int startReduceId, int endReduceId)",
				"documentation": "/**\n   * Sort-based shuffle data uses an index called \"shuffle_ShuffleId_MapId_0.index\" into a data file\n   * called \"shuffle_ShuffleId_MapId_0.data\". This logic is from IndexShuffleBlockResolver,\n   * and the block id format is from ShuffleDataBlockId and ShuffleIndexBlockId.\n   */"
			},
			{
				"signature": "public ManagedBuffer getDiskPersistedRddBlockData(\n      ExecutorShuffleInfo executor, int rddId, int splitIndex)",
				"documentation": ""
			},
			{
				"signature": "void close()",
				"documentation": ""
			},
			{
				"signature": "public int removeBlocks(String appId, String execId, String[] blockIds)",
				"documentation": ""
			},
			{
				"signature": "public Map\u003cString, String[]\u003e getLocalDirs(String appId, Set\u003cString\u003e execIds)",
				"documentation": ""
			},
			{
				"signature": "public Cause diagnoseShuffleBlockCorruption(\n      String appId,\n      String execId,\n      int shuffleId,\n      long mapId,\n      int reduceId,\n      long checksumByReader,\n      String algorithm)",
				"documentation": "/**\n   * Diagnose the possible cause of the shuffle data corruption by verifying the shuffle checksums\n   */"
			},
			{
				"signature": "private static byte[] dbAppExecKey(AppExecId appExecId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static AppExecId parseDbAppExecKey(String s) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  static ConcurrentMap\u003cAppExecId, ExecutorShuffleInfo\u003e reloadRegisteredExecutors(DB db)\n      throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.LevelDBProvider",
			"org.apache.spark.network.util.LevelDBProvider.StoreVersion",
			"org.apache.spark.network.util.NettyUtils",
			"org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.AppExecId"
		]
	},
	{
		"documentation": "/** Simply encodes an executor's full ID, which is appId + execId. */",
		"name": "org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.AppExecId",
		"extends": "",
		"Methods": [
			{
				"signature": "@JsonCreator\n    public AppExecId(@JsonProperty(\"appId\") String appId, @JsonProperty(\"execId\") String execId)",
				"documentation": "/** Simply encodes an executor's full ID, which is appId + execId. */"
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n *\n * Listener providing a callback function to invoke when driver receives the response for the\n * finalize shuffle merge request sent to remote shuffle service.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.MergeFinalizerListener",
		"extends": "java.util.EventListener",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Contains meta information for a merged block. Currently this information constitutes:\n * 1. Number of chunks in a merged shuffle block.\n * 2. Bitmaps for each chunk in the merged block. A chunk bitmap contains all the mapIds that were\n *    merged to that merged block chunk.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.MergedBlockMeta",
		"extends": "",
		"Methods": [
			{
				"signature": "public MergedBlockMeta(int numChunks, ManagedBuffer chunksBitmapBuffer)",
				"documentation": "/**\n * Contains meta information for a merged block. Currently this information constitutes:\n * 1. Number of chunks in a merged shuffle block.\n * 2. Bitmaps for each chunk in the merged block. A chunk bitmap contains all the mapIds that were\n *    merged to that merged block chunk.\n *\n * @since 3.1.0\n */"
			},
			{
				"signature": "public int getNumChunks()",
				"documentation": ""
			},
			{
				"signature": "public ManagedBuffer getChunksBitmapBuffer()",
				"documentation": ""
			},
			{
				"signature": "public RoaringBitmap[] readChunkBitmaps() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Listener for receiving success or failure events when fetching meta of merged blocks.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.network.shuffle.MergedBlocksMetaListener",
		"extends": "java.util.EventListener",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The MergedShuffleFileManager is used to process push based shuffle when enabled. It works\n * along side {@link ExternalBlockHandler} and serves as an RPCHandler for\n * {@link org.apache.spark.network.server.RpcHandler#receiveStream}, where it processes the\n * remotely pushed streams of shuffle blocks to merge them into merged shuffle files. Right\n * now, support for push based shuffle is only implemented for external shuffle service in\n * YARN mode.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.MergedShuffleFileManager",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.network.shuffle.NoOpMergedShuffleFileManager",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Dummy implementation of merged shuffle file manager. Suitable for when push-based shuffle\n * is not enabled.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.NoOpMergedShuffleFileManager",
		"extends": "",
		"Methods": [
			{
				"signature": "public NoOpMergedShuffleFileManager(TransportConf transportConf)",
				"documentation": "/**\n * Dummy implementation of merged shuffle file manager. Suitable for when push-based shuffle\n * is not enabled.\n *\n * @since 3.1.0\n */"
			},
			{
				"signature": "@Override\n  public StreamCallbackWithID receiveBlockDataAsStream(PushBlockStream msg)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MergeStatuses finalizeShuffleMerge(FinalizeShuffleMerge msg) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void registerExecutor(String appId, ExecutorShuffleInfo executorInfo)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void applicationRemoved(String appId, boolean cleanupLocalDirs)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ManagedBuffer getMergedBlockData(\n      String appId,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId,\n      int chunkId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MergedBlockMeta getMergedBlockMeta(\n      String appId,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String[] getMergedBlockDirs(String appId)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.shuffle.MergedShuffleFileManager"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Simple wrapper on top of a TransportClient which interprets each chunk as a whole block, and\n * invokes the BlockFetchingListener appropriately. This class is agnostic to the actual RPC\n * handler, as long as there is a single \"open blocks\" message which returns a ShuffleStreamHandle,\n * and Java serialization is used.\n *\n * Note that this typically corresponds to a\n * {@link org.apache.spark.network.server.OneForOneStreamManager} on the server side.\n */",
		"name": "org.apache.spark.network.shuffle.OneForOneBlockFetcher",
		"extends": "",
		"Methods": [
			{
				"signature": "public OneForOneBlockFetcher(\n    TransportClient client,\n    String appId,\n    String execId,\n    String[] blockIds,\n    BlockFetchingListener listener,\n    TransportConf transportConf)",
				"documentation": "/**\n * Simple wrapper on top of a TransportClient which interprets each chunk as a whole block, and\n * invokes the BlockFetchingListener appropriately. This class is agnostic to the actual RPC\n * handler, as long as there is a single \"open blocks\" message which returns a ShuffleStreamHandle,\n * and Java serialization is used.\n *\n * Note that this typically corresponds to a\n * {@link org.apache.spark.network.server.OneForOneStreamManager} on the server side.\n */"
			},
			{
				"signature": "public OneForOneBlockFetcher(\n      TransportClient client,\n      String appId,\n      String execId,\n      String[] blockIds,\n      BlockFetchingListener listener,\n      TransportConf transportConf,\n      DownloadFileManager downloadFileManager)",
				"documentation": ""
			},
			{
				"signature": "private boolean areShuffleBlocksOrChunks(String[] blockIds)",
				"documentation": "/**\n   * Check if the array of block IDs are all shuffle block IDs. With push based shuffle,\n   * the shuffle block ID could be either unmerged shuffle block IDs or merged shuffle chunk\n   * IDs. For a given stream of shuffle blocks to be fetched in one request, they would be either\n   * all unmerged shuffle blocks or all merged shuffle chunks.\n   * @param blockIds block ID array\n   * @return whether the array contains only shuffle block IDs\n   */"
			},
			{
				"signature": "private AbstractFetchShuffleBlocks createFetchShuffleBlocksOrChunksMsg(\n      String appId,\n      String execId,\n      String[] blockIds)",
				"documentation": "/** Creates either a {@link FetchShuffleBlocks} or {@link FetchShuffleBlockChunks} message. */"
			},
			{
				"signature": "private AbstractFetchShuffleBlocks createFetchShuffleBlocksMsg(\n      String appId,\n      String execId,\n      String[] blockIds)",
				"documentation": ""
			},
			{
				"signature": "private AbstractFetchShuffleBlocks createFetchShuffleChunksMsg(\n      String appId,\n      String execId,\n      String[] blockIds)",
				"documentation": ""
			},
			{
				"signature": "private int[][] getSecondaryIds(Map\u003c? extends Number, BlocksInfo\u003e primaryIdsToBlockInfo)",
				"documentation": ""
			},
			{
				"signature": "private String[] splitBlockId(String blockId)",
				"documentation": "/**\n   * Split the blockId and return accordingly\n   * shuffleChunk - return shuffleId, shuffleMergeId, reduceId and chunkIds\n   * shuffle block - return shuffleId, mapId, reduceId\n   * shuffle batch block - return shuffleId, mapId, begin reduceId and end reduceId\n   */"
			},
			{
				"signature": "public void start()",
				"documentation": "/**\n   * Begins the fetching process, calling the listener with every block fetched.\n   * The given message will be serialized with the Java serializer, and the RPC must return a\n   * {@link StreamHandle}. We will send all fetch requests immediately, without throttling.\n   */"
			},
			{
				"signature": "private void failRemainingBlocks(String[] failedBlockIds, Throwable e)",
				"documentation": "/** Invokes the \"onBlockFetchFailure\" callback for every listed block id. */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.client.RpcResponseCallback",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher.BlocksInfo",
			"org.apache.spark.network.shuffle.ChunkCallback",
			"org.apache.spark.network.shuffle.DownloadCallback",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunks",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.OpenBlocks"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher.BlocksInfo",
			"org.apache.spark.network.shuffle.ChunkCallback",
			"org.apache.spark.network.shuffle.DownloadCallback"
		]
	},
	{
		"documentation": "/** The reduceIds and blocks in a single mapId */",
		"name": "org.apache.spark.network.shuffle.OneForOneBlockFetcher.BlocksInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "BlocksInfo()",
				"documentation": "/**\n     * For {@link FetchShuffleBlocks} message, the ids are reduceIds.\n     * For {@link FetchShuffleBlockChunks} message, the ids are chunkIds.\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/** Callback invoked on receipt of each chunk. We equate a single chunk to a single block. */",
		"name": "org.apache.spark.network.shuffle.ChunkCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void onSuccess(int chunkIndex, ManagedBuffer buffer)",
				"documentation": "/** Callback invoked on receipt of each chunk. We equate a single chunk to a single block. */"
			},
			{
				"signature": "@Override\n    public void onFailure(int chunkIndex, Throwable e)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.ChunkReceivedCallback"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.DownloadCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "DownloadCallback(int chunkIndex) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onData(String streamId, ByteBuffer buf) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onComplete(String streamId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(String streamId, Throwable cause) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.StreamCallback"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Similar to {@link OneForOneBlockFetcher}, but for pushing blocks to remote shuffle service to\n * be merged instead of for fetching them from remote shuffle services. This is used by\n * ShuffleWriter when the block push process is initiated. The supplied BlockFetchingListener\n * is used to handle the success or failure in pushing each blocks.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.OneForOneBlockPusher",
		"extends": "",
		"Methods": [
			{
				"signature": "public OneForOneBlockPusher(\n      TransportClient client,\n      String appId,\n      int appAttemptId,\n      String[] blockIds,\n      BlockPushingListener listener,\n      Map\u003cString, ManagedBuffer\u003e buffers)",
				"documentation": "/**\n * Similar to {@link OneForOneBlockFetcher}, but for pushing blocks to remote shuffle service to\n * be merged instead of for fetching them from remote shuffle services. This is used by\n * ShuffleWriter when the block push process is initiated. The supplied BlockFetchingListener\n * is used to handle the success or failure in pushing each blocks.\n *\n * @since 3.1.0\n */"
			},
			{
				"signature": "private void checkAndFailRemainingBlocks(int index, Throwable e)",
				"documentation": ""
			},
			{
				"signature": "private void failRemainingBlocks(String[] failedBlockIds, Throwable e)",
				"documentation": ""
			},
			{
				"signature": "public void start()",
				"documentation": "/**\n   * Begins the block pushing process, calling the listener with every block pushed.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.server.BlockPushNonFatalFailure",
			"org.apache.spark.network.shuffle.BlockPushCallback",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.protocol.PushBlockStream"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.BlockPushCallback"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.BlockPushCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "BlockPushCallback(int index, String blockId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onSuccess(ByteBuffer response)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(Throwable e)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.RpcResponseCallback"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.server.BlockPushNonFatalFailure",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockPusher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An implementation of {@link MergedShuffleFileManager} that provides the most essential shuffle\n * service processing logic to support push based shuffle.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolver",
		"extends": "",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"UnstableApiUsage\")\n  public RemoteBlockPushResolver(TransportConf conf)",
				"documentation": "/**\n   * A concurrent hashmap where the key is the applicationId, and the value includes\n   * all the merged shuffle information for this application. AppShuffleInfo stores\n   * the application attemptId, merged shuffle local directories and the metadata\n   * for actively being merged shuffle partitions.\n   */"
			},
			{
				"signature": "public ShuffleIndexInformation load(String filePath) throws IOException",
				"documentation": "/**\n   * A concurrent hashmap where the key is the applicationId, and the value includes\n   * all the merged shuffle information for this application. AppShuffleInfo stores\n   * the application attemptId, merged shuffle local directories and the metadata\n   * for actively being merged shuffle partitions.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  protected static ErrorHandler.BlockPushErrorHandler createErrorHandler()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public boolean shouldLogError(Throwable t)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  protected AppShuffleInfo validateAndGetAppShuffleInfo(String appId)",
				"documentation": ""
			},
			{
				"signature": "private AppShufflePartitionInfo getOrCreateAppShufflePartitionInfo(\n      AppShuffleInfo appShuffleInfo,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId,\n      String blockId) throws BlockPushNonFatalFailure",
				"documentation": "/**\n   * Given the appShuffleInfo, shuffleId, shuffleMergeId and reduceId that uniquely identifies\n   * a given shuffle partition of an application, retrieves the associated metadata. If not\n   * present and the corresponding merged shuffle does not exist, initializes the metadata.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  AppShufflePartitionInfo newAppShufflePartitionInfo(\n      String appId,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId,\n      File dataFile,\n      File indexFile,\n      File metaFile) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MergedBlockMeta getMergedBlockMeta(\n      String appId,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"UnstableApiUsage\")\n  @Override\n  public ManagedBuffer getMergedBlockData(\n      String appId, int shuffleId, int shuffleMergeId, int reduceId, int chunkId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String[] getMergedBlockDirs(String appId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void applicationRemoved(String appId, boolean cleanupLocalDirs)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"SynchronizationOnLocalVariableOrMethodParameter\")\n  @VisibleForTesting\n  void closeAndDeletePartitionFilesIfNeeded(\n      AppShuffleInfo appShuffleInfo,\n      boolean cleanupLocalDirs)",
				"documentation": "/**\n   * Clean up the AppShufflePartitionInfo for a specific AppShuffleInfo.\n   * If cleanupLocalDirs is true, the merged shuffle files will also be deleted.\n   * The cleanup will be executed in a separate thread.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  void closeAndDeletePartitionFiles(Map\u003cInteger, AppShufflePartitionInfo\u003e partitions)",
				"documentation": "/**\n   * Clean up all the AppShufflePartitionInfo for a specific shuffleMergeId. This is done\n   * since there is a higher shuffleMergeId request made for a shuffleId, therefore clean\n   * up older shuffleMergeId partitions. The cleanup will be executed in a separate thread.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  void deleteExecutorDirs(AppShuffleInfo appShuffleInfo)",
				"documentation": "/**\n   * Serially delete local dirs.\n   */"
			},
			{
				"signature": "@Override\n  public StreamCallbackWithID receiveBlockDataAsStream(PushBlockStream msg)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public String getID()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onData(String streamId, ByteBuffer buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onComplete(String streamId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onFailure(String streamId, Throwable cause)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public ByteBuffer getCompletionResponse()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"SynchronizationOnLocalVariableOrMethodParameter\")\n  @Override\n  public MergeStatuses finalizeShuffleMerge(FinalizeShuffleMerge msg)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void registerExecutor(String appId, ExecutorShuffleInfo executorInfo)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.shuffle.MergedShuffleFileManager"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.client.StreamCallbackWithID",
			"org.apache.spark.network.server.BlockPushNonFatalFailure",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.NettyUtils",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.PushBlockStreamCallback",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppShuffleMergePartitionsInfo",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppShufflePartitionInfo",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppPathsInfo",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.MergeShuffleFile",
			"org.apache.spark.network.shuffle.protocol.BlockPushReturnCode",
			"org.apache.spark.network.shuffle.protocol.MergeStatuses"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.PushBlockStreamCallback",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppShuffleMergePartitionsInfo",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppShufflePartitionInfo",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppPathsInfo",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppShuffleInfo",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.MergeShuffleFile"
		]
	},
	{
		"documentation": "/**\n   * Callback for push stream that handles blocks which are not already merged.\n   */",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolver.PushBlockStreamCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "private PushBlockStreamCallback(\n        RemoteBlockPushResolver mergeManager,\n        AppShuffleInfo appShuffleInfo,\n        String streamId,\n        AppShufflePartitionInfo partitionInfo,\n        int mapIndex)",
				"documentation": "/**\n   * Callback for push stream that handles blocks which are not already merged.\n   */"
			},
			{
				"signature": "@Override\n    public String getID()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public ByteBuffer getCompletionResponse()",
				"documentation": ""
			},
			{
				"signature": "private void writeBuf(ByteBuffer buf) throws IOException",
				"documentation": "/**\n     * Write a ByteBuffer to the merged shuffle file. Here we keep track of the length of the\n     * block data written to file. In case of failure during writing block to file, we use the\n     * information tracked in partitionInfo to overwrite the corrupt block when writing the new\n     * block.\n     */"
			},
			{
				"signature": "private boolean allowedToWrite()",
				"documentation": "/**\n     * There will be multiple streams of map blocks belonging to the same reduce partition. At any\n     * given point of time, only a single map stream can write its data to the merged file. Until\n     * this stream is completed, the other streams defer writing. This prevents corruption of\n     * merged data. This returns whether this stream is the active stream that can write to the\n     * merged file.\n     */"
			},
			{
				"signature": "private boolean isDuplicateBlock()",
				"documentation": "/**\n     * Returns if this is a duplicate block generated by speculative tasks. With speculative\n     * tasks, we could receive the same block from 2 different sources at the same time. One of\n     * them is going to be the first to set the currentMapIndex. When that block does so, it's\n     * going to see the currentMapIndex initially as -1. After it sets the currentMapIndex, it's\n     * going to write some data to disk, thus increasing the length counter. The other duplicate\n     * block is going to see the currentMapIndex already set to its mapIndex. However, it hasn't\n     * written any data yet. If the first block gets written completely and resets the\n     * currentMapIndex to -1 before the processing for the second block finishes, we can just\n     * check the bitmap to identify the second as a duplicate.\n     */"
			},
			{
				"signature": "private void writeDeferredBufs() throws IOException",
				"documentation": "/**\n     * This is only invoked when the stream is able to write. The stream first writes any deferred\n     * block parts buffered in memory.\n     */"
			},
			{
				"signature": "private void abortIfNecessary()",
				"documentation": "/**\n     * @throws IllegalStateException if the number of IOExceptions have exceeded threshold.\n     */"
			},
			{
				"signature": "private void incrementIOExceptionsAndAbortIfNecessary()",
				"documentation": "/**\n     * This increments the number of IOExceptions and throws RuntimeException if it exceeds the\n     * threshold which will abort the merge of a particular shuffle partition.\n     */"
			},
			{
				"signature": "private boolean isStale(\n        AppShuffleMergePartitionsInfo appShuffleMergePartitionsInfo,\n        int shuffleMergeId)",
				"documentation": "/**\n     * If appShuffleMergePartitionsInfo is null or shuffleMergeId is\n     * greater than the request shuffleMergeId then it is a stale block push.\n     */"
			},
			{
				"signature": "private boolean isTooLate(\n        AppShuffleMergePartitionsInfo appShuffleMergePartitionsInfo,\n        int reduceId)",
				"documentation": "/**\n     * If appShuffleMergePartitionsInfo is null or shuffleMergePartitions is set to\n     * INDETERMINATE_SHUFFLE_FINALIZED or if the reduceId is not in the map then the\n     * shuffle is already finalized. Therefore the block push is too late.\n     */"
			},
			{
				"signature": "@Override\n    public void onData(String streamId, ByteBuffer buf) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onComplete(String streamId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(String streamId, Throwable throwable) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    AppShufflePartitionInfo getPartitionInfo()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.StreamCallbackWithID"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.BlockPushNonFatalFailure",
			"org.apache.spark.network.shuffle.protocol.BlockPushReturnCode"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Wrapper class to hold merged Shuffle related information for a specific shuffleMergeId\n   * required for the shuffles of indeterminate stages.\n   */",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppShuffleMergePartitionsInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "public AppShuffleMergePartitionsInfo(int shuffleMergeId, boolean shuffleFinalized)",
				"documentation": "/**\n   * Wrapper class to hold merged Shuffle related information for a specific shuffleMergeId\n   * required for the shuffles of indeterminate stages.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n    public Map\u003cInteger, AppShufflePartitionInfo\u003e getShuffleMergePartitions()",
				"documentation": ""
			},
			{
				"signature": "public boolean isFinalized()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Metadata tracked for an actively merged shuffle partition */",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppShufflePartitionInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "AppShufflePartitionInfo(\n        String appId,\n        int shuffleId,\n        int shuffleMergeId,\n        int reduceId,\n        File dataFile,\n        MergeShuffleFile indexFile,\n        MergeShuffleFile metaFile) throws IOException",
				"documentation": "/** Metadata tracked for an actively merged shuffle partition */"
			},
			{
				"signature": "public long getDataFilePos()",
				"documentation": ""
			},
			{
				"signature": "public void setDataFilePos(long dataFilePos)",
				"documentation": ""
			},
			{
				"signature": "int getCurrentMapIndex()",
				"documentation": ""
			},
			{
				"signature": "void setCurrentMapIndex(int mapIndex)",
				"documentation": ""
			},
			{
				"signature": "long getLastChunkOffset()",
				"documentation": ""
			},
			{
				"signature": "void blockMerged(int mapIndex)",
				"documentation": ""
			},
			{
				"signature": "void resetChunkTracker()",
				"documentation": ""
			},
			{
				"signature": "void updateChunkInfo(long chunkOffset, int mapIndex) throws IOException",
				"documentation": "/**\n     * Appends the chunk offset to the index file and adds the map index to the chunk tracker.\n     *\n     * @param chunkOffset the offset of the chunk in the data file.\n     * @param mapIndex the map index to be added to chunk tracker.\n     */"
			},
			{
				"signature": "private void writeChunkTracker(int mapIndex) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void incrementIOExceptions()",
				"documentation": ""
			},
			{
				"signature": "private boolean shouldAbort(int ioExceptionsThresholdDuringMerge)",
				"documentation": ""
			},
			{
				"signature": "private void finalizePartition() throws IOException",
				"documentation": ""
			},
			{
				"signature": "void closeAllFilesAndDeleteIfNeeded(boolean delete)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void finalize() throws Throwable",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    MergeShuffleFile getIndexFile()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    MergeShuffleFile getMetaFile()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    FileChannel getDataChannel()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    int getNumIOExceptions()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Wraps all the information related to the merge directory of an application.\n   */",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppPathsInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "private AppPathsInfo(\n        String appId,\n        String[] localDirs,\n        String mergeDirectory,\n        int subDirsPerLocalDir)",
				"documentation": "/**\n   * Wraps all the information related to the merge directory of an application.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/** Merged Shuffle related information tracked for a specific application attempt */",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolver.AppShuffleInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "AppShuffleInfo(\n        String appId,\n        int attemptId,\n        AppPathsInfo appPathsInfo)",
				"documentation": "/**\n     * 1. Key tracks shuffleId for an application\n     * 2. Value tracks the AppShuffleMergePartitionsInfo having shuffleMergeId and\n     * a Map tracking AppShufflePartitionInfo for all the shuffle partitions.\n     */"
			},
			{
				"signature": "@VisibleForTesting\n    public ConcurrentMap\u003cInteger, AppShuffleMergePartitionsInfo\u003e getShuffles()",
				"documentation": ""
			},
			{
				"signature": "private String getFilePath(String filename)",
				"documentation": "/**\n     * The logic here is consistent with\n     * @see [[org.apache.spark.storage.DiskBlockManager#getMergedShuffleFile(\n     *      org.apache.spark.storage.BlockId, scala.Option)]]\n     */"
			},
			{
				"signature": "private String generateFileName(\n        String appId,\n        int shuffleId,\n        int shuffleMergeId,\n        int reduceId)",
				"documentation": ""
			},
			{
				"signature": "public File getMergedShuffleDataFile(\n        int shuffleId,\n        int shuffleMergeId,\n        int reduceId)",
				"documentation": ""
			},
			{
				"signature": "public String getMergedShuffleIndexFilePath(\n        int shuffleId,\n        int shuffleMergeId,\n        int reduceId)",
				"documentation": ""
			},
			{
				"signature": "public File getMergedShuffleMetaFile(\n        int shuffleId,\n        int shuffleMergeId,\n        int reduceId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolver.MergeShuffleFile",
		"extends": "",
		"Methods": [
			{
				"signature": "@VisibleForTesting\n    MergeShuffleFile(File file) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    MergeShuffleFile(FileChannel channel, DataOutputStream dos)",
				"documentation": ""
			},
			{
				"signature": "private void updatePos(long numBytes)",
				"documentation": ""
			},
			{
				"signature": "void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "void delete() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    DataOutputStream getDos()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    FileChannel getChannel()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    long getPos()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Wraps another BlockFetcher or BlockPusher with the ability to automatically retry block\n * transfers which fail due to IOExceptions, which we hope are due to transient network conditions.\n *\n * This transferor provides stronger guarantees regarding the parent BlockTransferListener. In\n * particular, the listener will be invoked exactly once per blockId, with a success or failure.\n */",
		"name": "org.apache.spark.network.shuffle.RetryingBlockTransferor",
		"extends": "",
		"Methods": [
			{
				"signature": "public RetryingBlockTransferor(\n      TransportConf conf,\n      BlockTransferStarter transferStarter,\n      String[] blockIds,\n      BlockTransferListener listener,\n      ErrorHandler errorHandler)",
				"documentation": "/**\n   * The BlockTransferListener that is active with our current BlockFetcher.\n   * When we start a retry, we immediately replace this with a new Listener, which causes all any\n   * old Listeners to ignore all further responses.\n   */"
			},
			{
				"signature": "public RetryingBlockTransferor(\n      TransportConf conf,\n      BlockTransferStarter transferStarter,\n      String[] blockIds,\n      BlockFetchingListener listener)",
				"documentation": ""
			},
			{
				"signature": "public void start()",
				"documentation": "/**\n   * Initiates the transfer of all blocks provided in the constructor, with possible retries\n   * in the event of transient IOExceptions.\n   */"
			},
			{
				"signature": "private void transferAllOutstanding()",
				"documentation": "/**\n   * Fires off a request to transfer all blocks that have not been transferred successfully or\n   * permanently failed (i.e., by a non-IOException).\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.NettyUtils"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.BlockTransferStarter",
			"org.apache.spark.network.shuffle.RetryingBlockTransferListener"
		]
	},
	{
		"documentation": "/**\n   * Used to initiate the first transfer for all blocks, and subsequently for retrying the\n   * transfer on any remaining blocks.\n   */",
		"name": "org.apache.spark.network.shuffle.BlockTransferStarter",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Our RetryListener intercepts block transfer responses and forwards them to our parent\n   * listener. Note that in the event of a retry, we will immediately replace the 'currentListener'\n   * field, indicating that any responses from non-current Listeners should be ignored.\n   */",
		"name": "org.apache.spark.network.shuffle.RetryingBlockTransferListener",
		"extends": "",
		"Methods": [
			{
				"signature": "private void handleBlockTransferSuccess(String blockId, ManagedBuffer data)",
				"documentation": "/**\n   * Our RetryListener intercepts block transfer responses and forwards them to our parent\n   * listener. Note that in the event of a retry, we will immediately replace the 'currentListener'\n   * field, indicating that any responses from non-current Listeners should be ignored.\n   */"
			},
			{
				"signature": "private void handleBlockTransferFailure(String blockId, Throwable exception)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onBlockFetchSuccess(String blockId, ManagedBuffer data)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onBlockFetchFailure(String blockId, Throwable exception)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onBlockPushSuccess(String blockId, ManagedBuffer data)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onBlockPushFailure(String blockId, Throwable exception)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onBlockTransferSuccess(String blockId, ManagedBuffer data)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onBlockTransferFailure(String blockId, Throwable exception)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String getTransferType()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.shuffle.BlockFetchingListener",
			"org.apache.spark.network.shuffle.BlockPushingListener"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Keeps the index information for a particular map output\n * as an in-memory LongBuffer.\n */",
		"name": "org.apache.spark.network.shuffle.ShuffleIndexInformation",
		"extends": "",
		"Methods": [
			{
				"signature": "public ShuffleIndexInformation(String indexFilePath) throws IOException",
				"documentation": "/** offsets as long buffer */"
			},
			{
				"signature": "public int getRetainedMemorySize()",
				"documentation": ""
			},
			{
				"signature": "public ShuffleIndexRecord getIndex(int reduceId)",
				"documentation": "/**\n   * Get index offset for a particular reducer.\n   */"
			},
			{
				"signature": "public ShuffleIndexRecord getIndex(int startReduceId, int endReduceId)",
				"documentation": "/**\n   * Get index offset for the reducer range of [startReduceId, endReduceId).\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Contains offset and length of the shuffle block data.\n */",
		"name": "org.apache.spark.network.shuffle.ShuffleIndexRecord",
		"extends": "",
		"Methods": [
			{
				"signature": "public ShuffleIndexRecord(long offset, long length)",
				"documentation": "/**\n * Contains offset and length of the shuffle block data.\n */"
			},
			{
				"signature": "public long getOffset()",
				"documentation": ""
			},
			{
				"signature": "public long getLength()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A DownloadFile that does not take any encryption settings into account for reading and\n * writing data.\n *\n * This does *not* mean the data in the file is unencrypted -- it could be that the data is\n * already encrypted when its written, and subsequent layer is responsible for decrypting.\n */",
		"name": "org.apache.spark.network.shuffle.SimpleDownloadFile",
		"extends": "",
		"Methods": [
			{
				"signature": "public SimpleDownloadFile(File file, TransportConf transportConf)",
				"documentation": "/**\n * A DownloadFile that does not take any encryption settings into account for reading and\n * writing data.\n *\n * This does *not* mean the data in the file is unencrypted -- it could be that the data is\n * already encrypted when its written, and subsequent layer is responsible for decrypting.\n */"
			},
			{
				"signature": "@Override\n  public boolean delete()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public DownloadFileWritableChannel openForWriting() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String path()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.shuffle.DownloadFile"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer",
			"org.apache.spark.network.shuffle.SimpleDownloadWritableChannel"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.SimpleDownloadWritableChannel"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.SimpleDownloadWritableChannel",
		"extends": "",
		"Methods": [
			{
				"signature": "SimpleDownloadWritableChannel() throws FileNotFoundException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public ManagedBuffer closeAndRead()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int write(ByteBuffer src) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean isOpen()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.shuffle.DownloadFileWritableChannel"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.FileSegmentManagedBuffer"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.SimpleDownloadFile"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The cause of shuffle data corruption.\n */",
		"name": "org.apache.spark.network.shuffle.checksum.Cause",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A set of utility functions for the shuffle checksum.\n */",
		"name": "org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper",
		"extends": "",
		"Methods": [
			{
				"signature": "public static Checksum[] createPartitionChecksums(int numPartitions, String algorithm)",
				"documentation": "/**\n * A set of utility functions for the shuffle checksum.\n */"
			},
			{
				"signature": "private static Checksum[] getChecksumsByAlgorithm(int num, String algorithm)",
				"documentation": ""
			},
			{
				"signature": "public static Checksum getChecksumByAlgorithm(String algorithm)",
				"documentation": ""
			},
			{
				"signature": "public static String getChecksumFileName(String blockName, String algorithm)",
				"documentation": ""
			},
			{
				"signature": "private static long readChecksumByReduceId(File checksumFile, int reduceId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static long calculateChecksumForPartition(\n      ManagedBuffer partitionData,\n      Checksum checksumAlgo) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public static Cause diagnoseCorruption(\n      String algorithm,\n      File checksumFile,\n      int reduceId,\n      ManagedBuffer partitionData,\n      long checksumByReader)",
				"documentation": "/**\n   * Diagnose the possible cause of the shuffle data corruption by verifying the shuffle checksums.\n   *\n   * There're 3 different kinds of checksums for the same shuffle partition:\n   *   - checksum (c1) that is calculated by the shuffle data reader\n   *   - checksum (c2) that is calculated by the shuffle data writer and stored in the checksum file\n   *   - checksum (c3) that is recalculated during diagnosis\n   *\n   * And the diagnosis mechanism works like this:\n   * If c2 != c3, we suspect the corruption is caused by the DISK_ISSUE. Otherwise, if c1 != c3,\n   * we suspect the corruption is caused by the NETWORK_ISSUE. Otherwise, the cause remains\n   * CHECKSUM_VERIFY_PASS. In case of the any other failures, the cause remains UNKNOWN_ISSUE.\n   *\n   * @param algorithm The checksum algorithm that is used for calculating checksum value\n   *                  of partitionData\n   * @param checksumFile The checksum file that written by the shuffle writer\n   * @param reduceId The reduceId of the shuffle block\n   * @param partitionData The partition data of the shuffle block\n   * @param checksumByReader The checksum value that calculated by the shuffle data reader\n   * @return The cause of data corruption\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalShuffleBlockResolver",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.shuffle.checksum.ShuffleChecksumSupport",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base class for fetch shuffle blocks and chunks.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.network.shuffle.protocol.AbstractFetchShuffleBlocks",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "protected AbstractFetchShuffleBlocks(\n      String appId,\n      String execId,\n      int shuffleId)",
				"documentation": "/**\n * Base class for fetch shuffle blocks and chunks.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "public ToStringBuilder toStringHelper()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": "/**\n   * Returns number of blocks in the request.\n   */"
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunks",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Error code indicating a non-fatal failure of a block push request.\n * Due to the best-effort nature of push-based shuffle, these failures\n * do not impact the completion of the block push process. The list of\n * such errors is in\n * {@link org.apache.spark.network.server.BlockPushNonFatalFailure.ReturnCode}.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.network.shuffle.protocol.BlockPushReturnCode",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public BlockPushReturnCode(byte returnCode, String failureBlockId)",
				"documentation": "/**\n * Error code indicating a non-fatal failure of a block push request.\n * Due to the best-effort nature of push-based shuffle, these failures\n * do not impact the completion of the block push process. The list of\n * such errors is in\n * {@link org.apache.spark.network.server.BlockPushNonFatalFailure.ReturnCode}.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static BlockPushReturnCode decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.PushBlockStreamCallback",
			"org.apache.spark.network.shuffle.OneForOneBlockPusherSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Messages handled by the {@link ExternalBlockHandler}, or\n * by Spark's NettyBlockTransferService.\n *\n * At a high level:\n *   - OpenBlock is logically only handled by the NettyBlockTransferService, but for the capability\n *     for old version Spark, we still keep it in external shuffle service.\n *     It returns a StreamHandle.\n *   - UploadBlock is only handled by the NettyBlockTransferService.\n *   - RegisterExecutor is only handled by the external shuffle service.\n *   - RemoveBlocks is only handled by the external shuffle service.\n *   - FetchShuffleBlocks is handled by both services for shuffle files. It returns a StreamHandle.\n */",
		"name": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"extends": "",
		"Methods": [
			{
				"signature": "public ByteBuffer toByteBuffer()",
				"documentation": "/** Serializes the 'type' byte followed by the message itself. */"
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.Encodable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.network.shuffle.protocol.AbstractFetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.BlockPushReturnCode",
			"org.apache.spark.network.shuffle.protocol.BlocksRemoved",
			"org.apache.spark.network.shuffle.protocol.CorruptionCause",
			"org.apache.spark.network.shuffle.protocol.DiagnoseCorruption",
			"org.apache.spark.network.shuffle.protocol.FinalizeShuffleMerge",
			"org.apache.spark.network.shuffle.protocol.GetLocalDirsForExecutors",
			"org.apache.spark.network.shuffle.protocol.LocalDirsForExecutors",
			"org.apache.spark.network.shuffle.protocol.MergeStatuses",
			"org.apache.spark.network.shuffle.protocol.OpenBlocks",
			"org.apache.spark.network.shuffle.protocol.PushBlockStream",
			"org.apache.spark.network.shuffle.protocol.RegisterExecutor",
			"org.apache.spark.network.shuffle.protocol.RemoveBlocks",
			"org.apache.spark.network.shuffle.protocol.StreamHandle",
			"org.apache.spark.network.shuffle.protocol.UploadBlock",
			"org.apache.spark.network.shuffle.protocol.UploadBlockStream"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.shuffle.protocol.Type"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.protocol.Type",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder"
		]
	},
	{
		"documentation": "/** Preceding every serialized message is its type, which allows us to deserialize it. */",
		"name": "org.apache.spark.network.shuffle.protocol.Type",
		"extends": "",
		"Methods": [
			{
				"signature": "Type(int id)",
				"documentation": "/** Preceding every serialized message is its type, which allows us to deserialize it. */"
			},
			{
				"signature": "public byte id()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
		"extends": "",
		"Methods": [
			{
				"signature": "public static BlockTransferMessage fromByteBuffer(ByteBuffer msg)",
				"documentation": "/** Deserializes the 'type' byte followed by the message itself. */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver",
			"org.apache.spark.network.shuffle.protocol.mesos.ShuffleServiceHeartbeat"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher",
			"org.apache.spark.network.shuffle.OneForOneBlockPusher",
			"org.apache.spark.network.shuffle.BlockPushCallback",
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockPusherSuite",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** The reply to remove blocks giving back the number of removed blocks. */",
		"name": "org.apache.spark.network.shuffle.protocol.BlocksRemoved",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public BlocksRemoved(int numRemovedBlocks)",
				"documentation": "/** The reply to remove blocks giving back the number of removed blocks. */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static BlocksRemoved decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Response to the {@link DiagnoseCorruption} */",
		"name": "org.apache.spark.network.shuffle.protocol.CorruptionCause",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public CorruptionCause(Cause cause)",
				"documentation": "/** Response to the {@link DiagnoseCorruption} */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static CorruptionCause decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Request to get the cause of a corrupted block. Returns {@link CorruptionCause} */",
		"name": "org.apache.spark.network.shuffle.protocol.DiagnoseCorruption",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public DiagnoseCorruption(\n      String appId,\n      String execId,\n      int shuffleId,\n      long mapId,\n      int reduceId,\n      long checksum,\n      String algorithm)",
				"documentation": "/** Request to get the cause of a corrupted block. Returns {@link CorruptionCause} */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static DiagnoseCorruption decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Contains all configuration necessary for locating the shuffle files of an executor. */",
		"name": "org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "@JsonCreator\n  public ExecutorShuffleInfo(\n      @JsonProperty(\"localDirs\") String[] localDirs,\n      @JsonProperty(\"subDirsPerLocalDir\") int subDirsPerLocalDir,\n      @JsonProperty(\"shuffleManager\") String shuffleManager)",
				"documentation": "/**\n   * Shuffle manager (SortShuffleManager) that the executor is using.\n   * If this string contains semicolon, it will also include the meta information\n   * for push based shuffle in JSON format. Example of the string with semicolon would be:\n   * SortShuffleManager:{\"mergeDir\": \"mergeDirectory_1\", \"attemptId\": 1}\n   */"
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static ExecutorShuffleInfo decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.protocol.Encodable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.network.shuffle.TestShuffleDataContext"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.StringArrays"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Request to read a set of block chunks. Returns {@link StreamHandle}.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunks",
		"extends": "org.apache.spark.network.shuffle.protocol.AbstractFetchShuffleBlocks",
		"Methods": [
			{
				"signature": "public FetchShuffleBlockChunks(\n      String appId,\n      String execId,\n      int shuffleId,\n      int shuffleMergeId,\n      int[] reduceIds,\n      int[][] chunkIds)",
				"documentation": "/**\n * Request to read a set of block chunks. Returns {@link StreamHandle}.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getNumBlocks()",
				"documentation": ""
			},
			{
				"signature": "public static FetchShuffleBlockChunks decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings",
			"org.apache.spark.network.protocol.Encoders.IntArrays"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Request to read a set of blocks. Returns {@link StreamHandle}. */",
		"name": "org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks",
		"extends": "org.apache.spark.network.shuffle.protocol.AbstractFetchShuffleBlocks",
		"Methods": [
			{
				"signature": "public FetchShuffleBlocks(\n      String appId,\n      String execId,\n      int shuffleId,\n      long[] mapIds,\n      int[][] reduceIds,\n      boolean batchFetchEnabled)",
				"documentation": "/** Request to read a set of blocks. Returns {@link StreamHandle}. */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getNumBlocks()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static FetchShuffleBlocks decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings",
			"org.apache.spark.network.protocol.Encoders.IntArrays",
			"org.apache.spark.network.protocol.Encoders.LongArrays"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Request to finalize merge for a given shuffle.\n * Returns {@link MergeStatuses}\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.protocol.FinalizeShuffleMerge",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public FinalizeShuffleMerge(\n      String appId,\n      int appAttemptId,\n      int shuffleId,\n      int shuffleMergeId)",
				"documentation": "/**\n * Request to finalize merge for a given shuffle.\n * Returns {@link MergeStatuses}\n *\n * @since 3.1.0\n */"
			},
			{
				"signature": "@Override\n  protected BlockTransferMessage.Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static FinalizeShuffleMerge decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Request to get the local dirs for the given executors. */",
		"name": "org.apache.spark.network.shuffle.protocol.GetLocalDirsForExecutors",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public GetLocalDirsForExecutors(String appId, String[] execIds)",
				"documentation": "/** Request to get the local dirs for the given executors. */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static GetLocalDirsForExecutors decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** The reply to get local dirs giving back the dirs for each of the requested executors. */",
		"name": "org.apache.spark.network.shuffle.protocol.LocalDirsForExecutors",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public LocalDirsForExecutors(Map\u003cString, String[]\u003e localDirsByExec)",
				"documentation": "/** The reply to get local dirs giving back the dirs for each of the requested executors. */"
			},
			{
				"signature": "private LocalDirsForExecutors(String[] execIds, int[] numLocalDirsByExec, String[] allLocalDirs)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static LocalDirsForExecutors decode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public Map\u003cString, String[]\u003e getLocalDirsByExec()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.StringArrays",
			"org.apache.spark.network.protocol.Encoders.IntArrays"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Result returned by an ExternalShuffleService to the DAGScheduler. This represents the result\n * of all the remote shuffle block merge operations performed by an ExternalShuffleService\n * for a given shuffle ID. It includes the shuffle ID, an array of bitmaps each representing\n * the set of mapper partition blocks that are merged for a given reducer partition, an array\n * of reducer IDs, and an array of merged shuffle partition sizes. The 3 arrays list information\n * about all the reducer partitions merged by the ExternalShuffleService in the same order.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.protocol.MergeStatuses",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public MergeStatuses(\n      int shuffleId,\n      int shuffleMergeId,\n      RoaringBitmap[] bitmaps,\n      int[] reduceIds,\n      long[] sizes)",
				"documentation": "/**\n   * Array of merged shuffle partition block size. Each represents the total size of all\n   * merged shuffle partition blocks for one reducer partition.\n   * **/"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static MergeStatuses decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.IntArrays",
			"org.apache.spark.network.protocol.Encoders.LongArrays",
			"org.apache.spark.network.protocol.Encoders.BitmapArrays"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Request to read a set of blocks. Returns {@link StreamHandle}. */",
		"name": "org.apache.spark.network.shuffle.protocol.OpenBlocks",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public OpenBlocks(String appId, String execId, String[] blockIds)",
				"documentation": "/** Request to read a set of blocks. Returns {@link StreamHandle}. */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static OpenBlocks decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings",
			"org.apache.spark.network.protocol.Encoders.StringArrays"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcher",
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Request to push a block to a remote shuffle service to be merged in push based shuffle.\n * The remote shuffle service will also include this message when responding the push requests.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.network.shuffle.protocol.PushBlockStream",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public PushBlockStream(\n      String appId,\n      int appAttemptId,\n      int shuffleId,\n      int shuffleMergeId,\n      int mapIndex,\n      int reduceId,\n      int index)",
				"documentation": "/**\n * Request to push a block to a remote shuffle service to be merged in push based shuffle.\n * The remote shuffle service will also include this message when responding the push requests.\n *\n * @since 3.1.0\n */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static PushBlockStream decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockPusher",
			"org.apache.spark.network.shuffle.OneForOneBlockPusherSuite",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Initial registration message between an executor and its local shuffle server.\n * Returns nothing (empty byte array).\n */",
		"name": "org.apache.spark.network.shuffle.protocol.RegisterExecutor",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public RegisterExecutor(\n      String appId,\n      String execId,\n      ExecutorShuffleInfo executorInfo)",
				"documentation": "/**\n * Initial registration message between an executor and its local shuffle server.\n * Returns nothing (empty byte array).\n */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static RegisterExecutor decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.AppIsolationSuite",
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Request to remove a set of blocks. */",
		"name": "org.apache.spark.network.shuffle.protocol.RemoveBlocks",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public RemoveBlocks(String appId, String execId, String[] blockIds)",
				"documentation": "/** Request to remove a set of blocks. */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static RemoveBlocks decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings",
			"org.apache.spark.network.protocol.Encoders.StringArrays"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Identifier for a fixed number of chunks to read from a stream created by an \"open blocks\"\n * message. This is used by {@link org.apache.spark.network.shuffle.OneForOneBlockFetcher}.\n */",
		"name": "org.apache.spark.network.shuffle.protocol.StreamHandle",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public StreamHandle(long streamId, int numChunks)",
				"documentation": "/**\n * Identifier for a fixed number of chunks to read from a stream created by an \"open blocks\"\n * message. This is used by {@link org.apache.spark.network.shuffle.OneForOneBlockFetcher}.\n */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static StreamHandle decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Request to upload a block with a certain StorageLevel. Returns nothing (empty byte array). */",
		"name": "org.apache.spark.network.shuffle.protocol.UploadBlock",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public UploadBlock(\n      String appId,\n      String execId,\n      String blockId,\n      byte[] metadata,\n      byte[] blockData)",
				"documentation": "/**\n   * @param metadata Meta-information about block, typically StorageLevel.\n   * @param blockData The actual block's bytes.\n   */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static UploadBlock decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings",
			"org.apache.spark.network.protocol.Encoders.ByteArrays"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalBlockHandlerSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A request to Upload a block, which the destination should receive as a stream.\n *\n * The actual block data is not contained here.  It will be passed to the StreamCallbackWithID\n * that is returned from RpcHandler.receiveStream()\n */",
		"name": "org.apache.spark.network.shuffle.protocol.UploadBlockStream",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public UploadBlockStream(String blockId, byte[] metadata)",
				"documentation": "/**\n * A request to Upload a block, which the destination should receive as a stream.\n *\n * The actual block data is not contained here.  It will be passed to the StreamCallbackWithID\n * that is returned from RpcHandler.receiveStream()\n */"
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static UploadBlockStream decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A message sent from the driver to register with the MesosExternalShuffleService.\n */",
		"name": "org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public RegisterDriver(String appId, long heartbeatTimeoutMs)",
				"documentation": "/**\n * A message sent from the driver to register with the MesosExternalShuffleService.\n */"
			},
			{
				"signature": "public String getAppId()",
				"documentation": ""
			},
			{
				"signature": "public long getHeartbeatTimeoutMs()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "public static RegisterDriver decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.mesos.MesosExternalBlockStoreClient"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A heartbeat sent from the driver to the MesosExternalShuffleService.\n */",
		"name": "org.apache.spark.network.shuffle.protocol.mesos.ShuffleServiceHeartbeat",
		"extends": "org.apache.spark.network.shuffle.protocol.BlockTransferMessage",
		"Methods": [
			{
				"signature": "public ShuffleServiceHeartbeat(String appId)",
				"documentation": "/**\n * A heartbeat sent from the driver to the MesosExternalShuffleService.\n */"
			},
			{
				"signature": "public String getAppId()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected Type type()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int encodedLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void encode(ByteBuf buf)",
				"documentation": ""
			},
			{
				"signature": "public static ShuffleServiceHeartbeat decode(ByteBuf buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.protocol.Encoders.Strings"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.mesos.MesosExternalBlockStoreClient",
			"org.apache.spark.network.shuffle.mesos.Heartbeater"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.SaslIntegrationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void beforeAll() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void afterAll()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void afterEach()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGoodClient() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBadClient()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoSaslClient() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoSaslServer()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.sasl.SaslIntegrationSuite.TestRpcHandler"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.sasl.SaslIntegrationSuite.TestRpcHandler"
		]
	},
	{
		"documentation": "/** RPC handler which simply responds with the message it received. */",
		"name": "org.apache.spark.network.sasl.SaslIntegrationSuite.TestRpcHandler",
		"extends": "org.apache.spark.network.server.RpcHandler",
		"Methods": [
			{
				"signature": "@Override\n    public void receive(TransportClient client, ByteBuffer message, RpcResponseCallback callback)",
				"documentation": "/** RPC handler which simply responds with the message it received. */"
			},
			{
				"signature": "@Override\n    public StreamManager getStreamManager()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.sasl.SaslIntegrationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.sasl.ShuffleSecretManagerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testMultipleRegisters()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.AppIsolationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void beforeAll()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSaslAppIsolation() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAuthEngineAppIsolation() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void testAppIsolation(\n      Supplier\u003cTransportServerBootstrap\u003e serverBootstrap,\n      Function\u003cString, TransportClientBootstrap\u003e clientBootstrapFactory) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onBlockFetchSuccess(String blockId, ManagedBuffer data)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void onBlockFetchFailure(String blockId, Throwable t)",
				"documentation": ""
			},
			{
				"signature": "String[] blockIds =",
				"documentation": ""
			},
			{
				"signature": "@Override\n          public void onSuccess(int chunkIndex, ManagedBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override\n          public void onFailure(int chunkIndex, Throwable t)",
				"documentation": ""
			},
			{
				"signature": "private static void checkSecurityException(Throwable t)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.client.ChunkReceivedCallback",
			"org.apache.spark.network.crypto.AuthClientBootstrap",
			"org.apache.spark.network.crypto.AuthServerBootstrap",
			"org.apache.spark.network.sasl.SaslClientBootstrap",
			"org.apache.spark.network.sasl.SaslServerBootstrap",
			"org.apache.spark.network.server.OneForOneStreamManager",
			"org.apache.spark.network.util.MapConfigProvider",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.TestUtils",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo",
			"org.apache.spark.network.shuffle.protocol.OpenBlocks",
			"org.apache.spark.network.shuffle.protocol.RegisterExecutor"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Verifies that all BlockTransferMessages can be serialized correctly. */",
		"name": "org.apache.spark.network.shuffle.BlockTransferMessagesSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void serializeOpenShuffleBlocks()",
				"documentation": "/** Verifies that all BlockTransferMessages can be serialized correctly. */"
			},
			{
				"signature": "@Test\n  public void testLocalDirsMessages()",
				"documentation": ""
			},
			{
				"signature": "private BlockTransferMessage checkSerializeDeserialize(BlockTransferMessage msg)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.CleanupNonShuffleServiceServedFilesSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private TransportConf getConf(boolean isFetchRddEnabled)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnRemovedExecutorWithoutFilesToKeep() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void cleanupOnRemovedExecutor(\n      boolean withFilesToKeep,\n      TransportConf conf,\n      Set\u003cString\u003e expectedFilesKept) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupUsesExecutorWithFilesToKeep() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupUsesExecutorWithoutFilesToKeep() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void cleanupUsesExecutor(boolean withFilesToKeep) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnlyRemovedExecutorWithFilesToKeepFetchRddEnabled() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnlyRemovedExecutorWithFilesToKeepFetchRddDisabled() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnlyRemovedExecutorWithoutFilesToKeep() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void cleanupOnlyRemovedExecutor(\n      boolean withFilesToKeep,\n      TransportConf conf,\n      Set\u003cString\u003e expectedFilesKept) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnlyRegisteredExecutorWithFilesToKeepFetchRddEnabled() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnlyRegisteredExecutorWithFilesToKeepFetchRddDisabled() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cleanupOnlyRegisteredExecutorWithoutFilesToKeep() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void cleanupOnlyRegisteredExecutor(\n      boolean withFilesToKeep,\n      TransportConf conf,\n      Set\u003cString\u003e expectedFilesKept) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static void assertStillThere(TestShuffleDataContext dataContext)",
				"documentation": ""
			},
			{
				"signature": "private static Set\u003cString\u003e collectFilenames(File[] files) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static void assertContainedFilenames(\n      TestShuffleDataContext dataContext,\n      Set\u003cString\u003e expectedFilenames) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static TestShuffleDataContext initDataContext(boolean withFilesToKeep)\n      throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static void createFilesToKeep(TestShuffleDataContext dataContext) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static void createRemovableTestFiles(TestShuffleDataContext dataContext)\n      throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.MapConfigProvider",
			"org.apache.spark.network.util.TransportConf"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test suite for {@link ErrorHandler}\n */",
		"name": "org.apache.spark.network.shuffle.ErrorHandlerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testErrorRetry()",
				"documentation": "/**\n * Test suite for {@link ErrorHandler}\n */"
			},
			{
				"signature": "@Test\n  public void testErrorLogging()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.server.BlockPushNonFatalFailure"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ExternalBlockHandlerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "ManagedBuffer[] blockMarkers =",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void beforeEach()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRegisterExecutor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCompatibilityWithOldVersion()",
				"documentation": ""
			},
			{
				"signature": "private void checkDiagnosisResult(\n      String algorithm,\n      Cause expectedCaused) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShuffleCorruptionDiagnosisDiskIssue() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShuffleCorruptionDiagnosisNetworkIssue() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShuffleCorruptionDiagnosisUnknownIssue() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShuffleCorruptionDiagnosisChecksumVerifyPass() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShuffleCorruptionDiagnosisUnSupportedAlgorithm() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShuffleCorruptionDiagnosisCRC32() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFetchShuffleBlocks()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFetchShuffleBlocksInBatch()",
				"documentation": ""
			},
			{
				"signature": "ManagedBuffer[] batchBlockMarkers =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOpenDiskPersistedRDDBlocks()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOpenDiskPersistedRDDBlocksWithMissingBlock()",
				"documentation": ""
			},
			{
				"signature": "ManagedBuffer[] blockMarkersWithMissingBlock =",
				"documentation": ""
			},
			{
				"signature": "private void checkOpenBlocksReceive(BlockTransferMessage msg, ManagedBuffer[] blockMarkers)",
				"documentation": ""
			},
			{
				"signature": "private void verifyOpenBlockLatencyMetrics(\n      int blockTransferCount,\n      int blockTransferMessageCount)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBadMessages()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFinalizeShuffleMerge() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFetchMergedBlocksMeta()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOpenBlocksWithShuffleChunks()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFetchShuffleChunks()",
				"documentation": ""
			},
			{
				"signature": "private void verifyBlockChunkFetches(boolean useOpenBlocks)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.protocol.MergedBlockMetaRequest",
			"org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.protocol.DiagnoseCorruption",
			"org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunks",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.FinalizeShuffleMerge",
			"org.apache.spark.network.shuffle.protocol.MergeStatuses",
			"org.apache.spark.network.shuffle.protocol.OpenBlocks",
			"org.apache.spark.network.shuffle.protocol.RegisterExecutor",
			"org.apache.spark.network.shuffle.protocol.UploadBlock"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void beforeAll() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void afterAll()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBadRequests() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSortShuffleBlocks() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void jsonSerializationOfExecutorRegistration() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.TransportConf"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void noCleanupAndCleanup() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void beforeAll() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void afterAll()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void afterEach()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite",
			"org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite",
			"org.apache.spark.network.shuffle.OneForOneBlockPusherSuite",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite",
			"org.apache.spark.network.shuffle.RetryingBlockTransferorSuite",
			"org.apache.spark.network.shuffle.ShuffleIndexInformationSuite",
			"org.apache.spark.network.shuffle.TestShuffleDataContext",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunksSuite",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocksSuite",
			"org.apache.spark.network.yarn.YarnShuffleService"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void beforeEach() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void afterEach()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testValid() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBadAppId()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBadSecret()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEncryption() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "private void validate(String appId, String secretKey, boolean encrypt)\n        throws IOException, InterruptedException",
				"documentation": "/** Creates an ExternalBlockStoreClient and attempts to register with the server. */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.TransportContext",
			"org.apache.spark.network.sasl.SaslServerBootstrap",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo",
			"org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite.TestSecretKeyHolder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite.TestSecretKeyHolder"
		]
	},
	{
		"documentation": "/** Provides a secret key holder which always returns the given secret key, for a single appId. */",
		"name": "org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite.TestSecretKeyHolder",
		"extends": "",
		"Methods": [
			{
				"signature": "TestSecretKeyHolder(String appId, String secretKey)",
				"documentation": "/** Provides a secret key holder which always returns the given secret key, for a single appId. */"
			},
			{
				"signature": "@Override\n    public String getSaslUser(String appId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String getSecretKey(String appId)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.sasl.SecretKeyHolder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testFetchOne()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUseOldProtocol()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFetchThreeShuffleBlocks()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBatchFetchThreeShuffleBlocks()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFetchThree()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailure()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailureAndSuccess()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEmptyBlockFetch()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFetchShuffleBlocksOrder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBatchFetchShuffleBlocksOrder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShuffleBlockChunksFetch()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShuffleBlockChunkFetchFailure()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInvalidShuffleBlockIds()",
				"documentation": ""
			},
			{
				"signature": "private static BlockFetchingListener fetchBlocks(\n      LinkedHashMap\u003cString, ManagedBuffer\u003e blocks,\n      String[] blockIds,\n      BlockTransferMessage expectMessage,\n      TransportConf transportConf)",
				"documentation": "/**\n   * Begins a fetch on the given set of blocks by mocking out the server side of the RPC which\n   * simply returns the given (BlockId, Block) pairs.\n   * As \"blocks\" is a LinkedHashMap, the blocks are guaranteed to be returned in the same order\n   * that they were inserted in.\n   *\n   * If a block's buffer is \"null\", an exception will be thrown instead.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer",
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunks",
			"org.apache.spark.network.shuffle.protocol.FetchShuffleBlocks",
			"org.apache.spark.network.shuffle.protocol.OpenBlocks",
			"org.apache.spark.network.shuffle.protocol.StreamHandle"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.OneForOneBlockPusherSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testPushOne()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPushThree()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testServerFailures()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testHandlingRetriableFailures()",
				"documentation": ""
			},
			{
				"signature": "private static BlockPushingListener pushBlocks(\n      LinkedHashMap\u003cString, ManagedBuffer\u003e blocks,\n      String[] blockIds,\n      Iterable\u003cBlockTransferMessage\u003e expectMessages)",
				"documentation": "/**\n   * Begins a push on the given set of blocks by mocking the response from server side.\n   * If a block is an empty byte, a server side retriable exception will be thrown.\n   * If a block is null, a non-retriable exception will be thrown.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NettyManagedBuffer",
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.server.BlockPushNonFatalFailure",
			"org.apache.spark.network.shuffle.protocol.BlockPushReturnCode",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.protocol.PushBlockStream"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Tests for {@link RemoteBlockPushResolver}.\n */",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void before() throws IOException",
				"documentation": "/**\n * Tests for {@link RemoteBlockPushResolver}.\n */"
			},
			{
				"signature": "@After\n  public void after()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testErrorLogging()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoIndexFile()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBasicBlockMerge() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDividingMergedBlocksIntoChunks() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFinalizeWithMultipleReducePartitions() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDeferredBufsAreWrittenDuringOnData() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDeferredBufsAreWrittenDuringOnComplete() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDuplicateBlocksAreIgnoredWhenPrevStreamHasCompleted() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDuplicateBlocksAreIgnoredWhenPrevStreamIsInProgress() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailureAfterData() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailureAfterMultipleDataBlocks() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailureAfterComplete() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBlockReceivedAfterMergeFinalize() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIncompleteStreamsAreOverwritten() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCollision() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailureInAStreamDoesNotInterfereWithStreamWhichIsWriting() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUpdateLocalDirsOnlyOnce() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExecutorRegisterWithInvalidJsonForPushShuffle() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExecutorRegistrationFromTwoAppAttempts() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCleanUpDirectory() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n      void deleteExecutorDirs(AppShuffleInfo appShuffleInfo)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRecoverIndexFileAfterIOExceptions() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRecoverIndexFileAfterIOExceptionsInFinalize() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRecoverMetaFileAfterIOExceptions() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRecoverMetaFileAfterIOExceptionsInFinalize() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIOExceptionsExceededThreshold() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIOExceptionsDuringMetaUpdateIncreasesExceptionCount() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRequestForAbortedShufflePartitionThrowsException() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPendingBlockIsAbortedImmediately() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testWritingPendingBufsIsAbortedImmediatelyDuringComplete() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailureWhileTruncatingFiles() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOnFailureInvokedMoreThanOncePerBlock() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailureAfterDuplicateBlockDoesNotInterfereActiveStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPushBlockFromPreviousAttemptIsRejected()\n      throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n      void closeAndDeletePartitionFilesIfNeeded(\n        AppShuffleInfo appShuffleInfo,\n        boolean cleanupLocalDirs)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFinalizeShuffleMergeFromPreviousAttemptIsAborted()\n    throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOngoingMergeOfBlockFromPreviousAttemptIsAborted()\n      throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n      void closeAndDeletePartitionFilesIfNeeded(\n          AppShuffleInfo appShuffleInfo,\n          boolean cleanupLocalDirs)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBlockPushWithOlderShuffleMergeId() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFinalizeWithOlderShuffleMergeId() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFinalizeOfDeterminateShuffle() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBlockFetchWithOlderShuffleMergeId() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCleanupOlderShuffleMergeId() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n      void closeAndDeletePartitionFiles(Map\u003cInteger, AppShufflePartitionInfo\u003e partitions)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFinalizationResultIsEmptyWhenTheServerDidNotReceiveAnyBlocks()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEmptyMergePartitionsAreNotReported() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAllBlocksAreRejectedWhenReceivedAfterFinalization() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void useTestFiles(boolean useTestIndexFile, boolean useTestMetaFile) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n      AppShufflePartitionInfo newAppShufflePartitionInfo(\n          String appId,\n          int shuffleId,\n          int shuffleMergeId,\n          int reduceId,\n          File dataFile,\n          File indexFile,\n          File metaFile) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private Path[] createLocalDirs(int numLocalDirs) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void registerExecutor(String appId, String[] localDirs, String shuffleManagerMeta)",
				"documentation": ""
			},
			{
				"signature": "private String[] prepareLocalDirs(Path[] localDirs, String mergeDir) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void removeApplication(String appId)",
				"documentation": ""
			},
			{
				"signature": "private void validateMergeStatuses(\n      MergeStatuses mergeStatuses,\n      int[] expectedReduceIds,\n      long[] expectedSizes)",
				"documentation": ""
			},
			{
				"signature": "private void validateChunks(\n      String appId,\n      int shuffleId,\n      int shuffleMergeId,\n      int reduceId,\n      MergedBlockMeta meta,\n      int[] expectedSizes,\n      int[][] expectedMapsPerChunk) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void pushBlockHelper(\n      String appId,\n      int attemptId,\n      PushBlock[] blocks) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.MapConfigProvider",
			"org.apache.spark.network.util.TransportConf",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolver.MergeShuffleFile",
			"org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder",
			"org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo",
			"org.apache.spark.network.shuffle.protocol.FinalizeShuffleMerge",
			"org.apache.spark.network.shuffle.protocol.PushBlockStream",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite.PushBlock",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite.TestMergeShuffleFile"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite.PushBlock",
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite.TestMergeShuffleFile"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite.PushBlock",
		"extends": "",
		"Methods": [
			{
				"signature": "PushBlock(int shuffleId, int shuffleMergeId, int mapIndex, int reduceId, ByteBuffer buffer)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite.TestMergeShuffleFile",
		"extends": "org.apache.spark.network.shuffle.RemoteBlockPushResolver.MergeShuffleFile",
		"Methods": [
			{
				"signature": "private TestMergeShuffleFile(File file) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    DataOutputStream getDos()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    FileChannel getChannel()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "void restore() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.RemoteBlockPushResolverSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Tests retry logic by throwing IOExceptions and ensuring that subsequent attempts are made to\n * fetch the lost blocks.\n */",
		"name": "org.apache.spark.network.shuffle.RetryingBlockTransferorSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testNoFailures() throws IOException, InterruptedException",
				"documentation": "/**\n * Tests retry logic by throwing IOExceptions and ensuring that subsequent attempts are made to\n * fetch the lost blocks.\n */"
			},
			{
				"signature": "@Test\n  public void testUnrecoverableFailure() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSingleIOExceptionOnFirst() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSingleIOExceptionOnSecond() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTwoIOExceptions() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testThreeIOExceptions() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRetryAndUnrecoverable() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "private static void performInteractions(List\u003c? extends Map\u003cString, Object\u003e\u003e interactions,\n                                          BlockFetchingListener listener)\n    throws IOException, InterruptedException",
				"documentation": "/**\n   * Performs a set of interactions in response to block requests from a RetryingBlockFetcher.\n   * Each interaction is a Map from BlockId to either ManagedBuffer or Exception. This interaction\n   * means \"respond to the next block fetch request with these Successful buffers and these Failure\n   * exceptions\". We verify that the expected block ids are exactly the ones requested.\n   *\n   * If multiple interactions are supplied, they will be used in order. This is useful for encoding\n   * retries -- the first interaction may include an IOException, which causes a retry of some\n   * subset of the original blocks in a second interaction.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.buffer.NioManagedBuffer",
			"org.apache.spark.network.util.MapConfigProvider",
			"org.apache.spark.network.util.TransportConf"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.ShuffleIndexInformationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void before() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void afterAll()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void test() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Manages some sort-shuffle data, including the creation\n * and cleanup of directories that can be read by the {@link ExternalShuffleBlockResolver}.\n */",
		"name": "org.apache.spark.network.shuffle.TestShuffleDataContext",
		"extends": "",
		"Methods": [
			{
				"signature": "public TestShuffleDataContext(int numLocalDirs, int subDirsPerLocalDir)",
				"documentation": "/**\n * Manages some sort-shuffle data, including the creation\n * and cleanup of directories that can be read by the {@link ExternalShuffleBlockResolver}.\n */"
			},
			{
				"signature": "public void create()",
				"documentation": ""
			},
			{
				"signature": "public void cleanup()",
				"documentation": ""
			},
			{
				"signature": "public String insertSortShuffleData(int shuffleId, int mapId, byte[][] blocks)\n      throws IOException",
				"documentation": "/** Creates reducer blocks in a sort-based data format within our local dirs. */"
			},
			{
				"signature": "public void insertSpillData() throws IOException",
				"documentation": "/** Creates spill file(s) within the local dirs. */"
			},
			{
				"signature": "public void insertBroadcastData() throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void insertTempShuffleData() throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void insertCachedRddData(int rddId, int splitId, byte[] block) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void insertFile(String filename) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void insertFile(String filename, byte[] block) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public ExecutorShuffleInfo createExecutorInfo(String shuffleManager)",
				"documentation": "/**\n   * Creates an ExecutorShuffleInfo object based on the given shuffle manager which targets this\n   * context's directories.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.protocol.FetchShuffleBlockChunksSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testFetchShuffleBlockChunksEncodeDecode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.protocol.FetchShuffleBlocksSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testFetchShuffleBlockEncodeDecode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An external shuffle service used by Spark on Yarn.\n *\n * This is intended to be a long-running auxiliary service that runs in the NodeManager process.\n * A Spark application may connect to this service by setting `spark.shuffle.service.enabled`.\n * The application also automatically derives the service port through `spark.shuffle.service.port`\n * specified in the Yarn configuration. This is so that both the clients and the server agree on\n * the same port to communicate on.\n *\n * The service also optionally supports authentication. This ensures that executors from one\n * application cannot read the shuffle files written by those from another. This feature can be\n * enabled by setting `spark.authenticate` in the Yarn configuration before starting the NM.\n * Note that the Spark application must also set `spark.authenticate` manually and, unlike in\n * the case of the service port, will not inherit this setting from the Yarn configuration. This\n * is because an application running on the same Yarn cluster may choose to not use the external\n * shuffle service, in which case its setting of `spark.authenticate` should be independent of\n * the service's.\n *\n * The shuffle service will produce metrics via the YARN NodeManager's {@code metrics2} system\n * under a namespace specified by the {@value SPARK_SHUFFLE_SERVICE_METRICS_NAMESPACE_KEY} config.\n *\n * By default, all configurations for the shuffle service will be taken directly from the\n * Hadoop {@link Configuration} passed by the YARN NodeManager. It is also possible to configure\n * the shuffle service by placing a resource named\n * {@value SHUFFLE_SERVICE_CONF_OVERLAY_RESOURCE_NAME} into the classpath, which should be an\n * XML file in the standard Hadoop Configuration resource format. Note that when the shuffle\n * service is loaded in the default manner, without configuring\n * {@code yarn.nodemanager.aux-services.\u003cservice\u003e.classpath}, this file must be on the classpath\n * of the NodeManager itself. When using the {@code classpath} configuration, it can be present\n * either on the NodeManager's classpath, or specified in the classpath configuration.\n * This {@code classpath} configuration is only supported on YARN versions \u003e= 2.9.0.\n */",
		"name": "org.apache.spark.network.yarn.YarnShuffleService",
		"extends": "AuxiliaryService",
		"Methods": [
			{
				"signature": "public YarnShuffleService()",
				"documentation": "/**\n   * The name of the resource to search for on the classpath to find a shuffle service-specific\n   * configuration overlay. If found, this will be parsed as a standard Hadoop\n   * {@link Configuration config} file and will override the configs passed from the NodeManager.\n   */"
			},
			{
				"signature": "private boolean isAuthenticationEnabled()",
				"documentation": "/**\n   * Return whether authentication is enabled as specified by the configuration.\n   * If so, fetch requests will fail unless the appropriate authentication secret\n   * for the application is provided.\n   */"
			},
			{
				"signature": "@Override\n  protected void serviceInit(Configuration externalConf) throws Exception",
				"documentation": "/**\n   * Start the shuffle server with the given configuration.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  static MergedShuffleFileManager newMergedShuffleFileManagerInstance(TransportConf conf)",
				"documentation": ""
			},
			{
				"signature": "private void loadSecretsFromDb() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static String parseDbAppKey(String s) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static byte[] dbAppKey(AppId appExecId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initializeApplication(ApplicationInitializationContext context)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void stopApplication(ApplicationTerminationContext context)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.yarn.YarnShuffleServiceMetrics",
			"org.apache.spark.network.yarn.util.HadoopConfigProvider",
			"org.apache.spark.util.sketch.BitArray",
			"org.apache.spark.util.sketch.BloomFilter",
			"org.apache.spark.util.sketch.BloomFilterImpl",
			"org.apache.spark.util.sketch.CountMinSketch",
			"org.apache.spark.util.sketch.CountMinSketchImpl",
			"org.apache.spark.util.sketch.IncompatibleMergeException",
			"org.apache.spark.util.sketch.Murmur3_x86_32",
			"org.apache.spark.util.sketch.Platform",
			"org.apache.spark.util.sketch.Utils",
			"org.apache.spark.annotation.AlphaComponent",
			"org.apache.spark.annotation.DeveloperApi",
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.annotation.Experimental",
			"org.apache.spark.annotation.Private",
			"org.apache.spark.annotation.Stable",
			"org.apache.spark.annotation.Unstable",
			"org.apache.spark.tags.ChromeUITest",
			"org.apache.spark.tags.DockerTest",
			"org.apache.spark.tags.ExtendedHiveTest",
			"org.apache.spark.tags.ExtendedLevelDBTest",
			"org.apache.spark.tags.ExtendedSQLTest",
			"org.apache.spark.tags.ExtendedYarnTest",
			"org.apache.spark.tags.SlowHiveTest",
			"org.apache.spark.sql.catalyst.expressions.HiveHasher",
			"org.apache.spark.sql.catalyst.util.DateTimeConstants",
			"org.apache.spark.unsafe.KVIterator",
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UTF8StringBuilder",
			"org.apache.spark.unsafe.UnsafeAlignedOffset",
			"org.apache.spark.unsafe.array.ByteArrayMethods",
			"org.apache.spark.unsafe.array.LongArray",
			"org.apache.spark.unsafe.bitset.BitSetMethods",
			"org.apache.spark.unsafe.hash.Murmur3_x86_32",
			"org.apache.spark.unsafe.memory.HeapMemoryAllocator",
			"org.apache.spark.unsafe.memory.MemoryAllocator",
			"org.apache.spark.unsafe.memory.MemoryBlock",
			"org.apache.spark.unsafe.memory.MemoryLocation",
			"org.apache.spark.unsafe.memory.UnsafeMemoryAllocator",
			"org.apache.spark.unsafe.types.ByteArray",
			"org.apache.spark.unsafe.types.CalendarInterval",
			"org.apache.spark.unsafe.types.UTF8String",
			"org.apache.spark.unsafe.PlatformUtilSuite",
			"org.apache.spark.unsafe.array.ByteArraySuite",
			"org.apache.spark.unsafe.array.LongArraySuite",
			"org.apache.spark.unsafe.hash.Murmur3_x86_32Suite",
			"org.apache.spark.unsafe.types.CalendarIntervalSuite",
			"org.apache.spark.unsafe.types.UTF8StringSuite",
			"org.apache.spark.sql.avro.SparkAvroKeyOutputFormat",
			"org.apache.spark.sql.avro.SparkAvroKeyRecordWriter",
			"org.apache.spark.sql.avro.JavaAvroFunctionsSuite",
			"org.apache.spark.streaming.kafka010.JavaConsumerStrategySuite",
			"org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite",
			"org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite",
			"org.apache.spark.streaming.kafka010.JavaLocationStrategySuite",
			"org.apache.spark.examples.streaming.JavaKinesisWordCountASL",
			"org.apache.spark.streaming.kinesis.KinesisInitialPosition",
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions",
			"org.apache.spark.streaming.kinesis.JavaKinesisInputDStreamBuilderSuite",
			"com.codahale.metrics.ganglia.GangliaReporter",
			"org.apache.spark.JobExecutionStatus",
			"org.apache.spark.SparkExecutorInfo",
			"org.apache.spark.SparkFirehoseListener",
			"org.apache.spark.SparkJobInfo",
			"org.apache.spark.SparkStageInfo",
			"org.apache.spark.SparkThrowable",
			"org.apache.spark.api.java.JavaFutureAction",
			"org.apache.spark.api.java.Optional",
			"org.apache.spark.api.java.StorageLevels",
			"org.apache.spark.api.java.function.CoGroupFunction",
			"org.apache.spark.api.java.function.DoubleFlatMapFunction",
			"org.apache.spark.api.java.function.DoubleFunction",
			"org.apache.spark.api.java.function.FilterFunction",
			"org.apache.spark.api.java.function.FlatMapFunction",
			"org.apache.spark.api.java.function.FlatMapFunction2",
			"org.apache.spark.api.java.function.FlatMapGroupsFunction",
			"org.apache.spark.api.java.function.ForeachFunction",
			"org.apache.spark.api.java.function.ForeachPartitionFunction",
			"org.apache.spark.api.java.function.Function",
			"org.apache.spark.api.java.function.Function0",
			"org.apache.spark.api.java.function.Function2",
			"org.apache.spark.api.java.function.Function3",
			"org.apache.spark.api.java.function.Function4",
			"org.apache.spark.api.java.function.MapFunction",
			"org.apache.spark.api.java.function.MapGroupsFunction",
			"org.apache.spark.api.java.function.MapPartitionsFunction",
			"org.apache.spark.api.java.function.PairFlatMapFunction",
			"org.apache.spark.api.java.function.PairFunction",
			"org.apache.spark.api.java.function.ReduceFunction",
			"org.apache.spark.api.java.function.VoidFunction",
			"org.apache.spark.api.java.function.VoidFunction2",
			"org.apache.spark.api.plugin.DriverPlugin",
			"org.apache.spark.api.plugin.ExecutorPlugin",
			"org.apache.spark.api.plugin.PluginContext",
			"org.apache.spark.api.plugin.SparkPlugin",
			"org.apache.spark.api.resource.ResourceDiscoveryPlugin",
			"org.apache.spark.io.NioBufferedFileInputStream",
			"org.apache.spark.io.ReadAheadInputStream",
			"org.apache.spark.memory.MemoryConsumer",
			"org.apache.spark.memory.MemoryMode",
			"org.apache.spark.memory.SparkOutOfMemoryError",
			"org.apache.spark.memory.TaskMemoryManager",
			"org.apache.spark.memory.TooLargePageException",
			"org.apache.spark.serializer.DummySerializerInstance",
			"org.apache.spark.shuffle.api.ShuffleDataIO",
			"org.apache.spark.shuffle.api.ShuffleDriverComponents",
			"org.apache.spark.shuffle.api.ShuffleExecutorComponents",
			"org.apache.spark.shuffle.api.ShuffleMapOutputWriter",
			"org.apache.spark.shuffle.api.ShufflePartitionWriter",
			"org.apache.spark.shuffle.api.SingleSpillShuffleMapOutputWriter",
			"org.apache.spark.shuffle.api.WritableByteChannelWrapper",
			"org.apache.spark.shuffle.api.metadata.MapOutputCommitMessage",
			"org.apache.spark.shuffle.api.metadata.MapOutputMetadata",
			"org.apache.spark.shuffle.checksum.ShuffleChecksumSupport",
			"org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter",
			"org.apache.spark.shuffle.sort.PackedRecordPointer",
			"org.apache.spark.shuffle.sort.ShuffleExternalSorter",
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter",
			"org.apache.spark.shuffle.sort.ShuffleSortDataFormat",
			"org.apache.spark.shuffle.sort.SpillInfo",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter",
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleDataIO",
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleDriverComponents",
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents",
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter",
			"org.apache.spark.shuffle.sort.io.LocalDiskSingleSpillMapOutputWriter",
			"org.apache.spark.status.api.v1.ApplicationStatus",
			"org.apache.spark.status.api.v1.StageStatus",
			"org.apache.spark.status.api.v1.TaskSorting",
			"org.apache.spark.status.api.v1.TaskStatus",
			"org.apache.spark.storage.TimeTrackingOutputStream",
			"org.apache.spark.unsafe.map.BytesToBytesMap",
			"org.apache.spark.unsafe.map.HashMapGrowthStrategy",
			"org.apache.spark.util.ChildFirstURLClassLoader",
			"org.apache.spark.util.EnumUtil",
			"org.apache.spark.util.MutableURLClassLoader",
			"org.apache.spark.util.ParentClassLoader",
			"org.apache.spark.util.collection.TimSort",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators",
			"org.apache.spark.util.collection.unsafe.sort.RadixSort",
			"org.apache.spark.util.collection.unsafe.sort.RecordComparator",
			"org.apache.spark.util.collection.unsafe.sort.RecordPointerAndKeyPrefix",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter"
		]
	},
	{
		"documentation": "/**\n * Forward {@link org.apache.spark.network.shuffle.ExternalBlockHandler.ShuffleMetrics}\n * to hadoop metrics system.\n * NodeManager by default exposes JMX endpoint where can be collected.\n */",
		"name": "org.apache.spark.network.yarn.YarnShuffleServiceMetrics",
		"extends": "",
		"Methods": [
			{
				"signature": "YarnShuffleServiceMetrics(String metricsNamespace, MetricSet metricSet)",
				"documentation": "/**\n * Forward {@link org.apache.spark.network.shuffle.ExternalBlockHandler.ShuffleMetrics}\n * to hadoop metrics system.\n * NodeManager by default exposes JMX endpoint where can be collected.\n */"
			},
			{
				"signature": "@Override\n  public void getMetrics(MetricsCollector collector, boolean all)",
				"documentation": "/**\n   * Get metrics from the source\n   *\n   * @param collector to contain the resulting metrics snapshot\n   * @param all       if true, return all metrics even if unchanged.\n   */"
			},
			{
				"signature": "public static void collectMetric(\n    MetricsRecordBuilder metricsRecordBuilder, String name, Metric metric)",
				"documentation": "/**\n   * The metric types used in\n   * {@link org.apache.spark.network.shuffle.ExternalBlockHandler.ShuffleMetrics}.\n   * Visible for testing.\n   */"
			},
			{
				"signature": "private static MetricsInfo getShuffleServiceMetricsInfoForGauge(String name)",
				"documentation": ""
			},
			{
				"signature": "private static ShuffleServiceMetricsInfo getShuffleServiceMetricsInfoForCounter(String name)",
				"documentation": ""
			},
			{
				"signature": "private static ShuffleServiceMetricsInfo getShuffleServiceMetricsInfoForGenericValue(\n      String baseName, String valueName)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hadoop.metrics2.MetricsSource"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.yarn.YarnShuffleServiceMetrics.ShuffleServiceMetricsInfo"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.network.yarn.YarnShuffleServiceMetrics.ShuffleServiceMetricsInfo"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.yarn.YarnShuffleServiceMetrics.ShuffleServiceMetricsInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "ShuffleServiceMetricsInfo(String name, String description)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String description()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hadoop.metrics2.MetricsInfo"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.yarn.YarnShuffleServiceMetrics"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/** Use the Hadoop configuration to obtain config values. */",
		"name": "org.apache.spark.network.yarn.util.HadoopConfigProvider",
		"extends": "org.apache.spark.network.util.ConfigProvider",
		"Methods": [
			{
				"signature": "public HadoopConfigProvider(Configuration conf)",
				"documentation": "/** Use the Hadoop configuration to obtain config values. */"
			},
			{
				"signature": "@Override\n  public String get(String name)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String get(String name, String defaultValue)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Iterable\u003cMap.Entry\u003cString, String\u003e\u003e getAll()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.sketch.BitArray",
		"extends": "",
		"Methods": [
			{
				"signature": "static int numWords(long numBits)",
				"documentation": ""
			},
			{
				"signature": "BitArray(long numBits)",
				"documentation": ""
			},
			{
				"signature": "private BitArray(long[] data)",
				"documentation": ""
			},
			{
				"signature": "boolean set(long index)",
				"documentation": "/** Returns true if the bit changed value. */"
			},
			{
				"signature": "boolean get(long index)",
				"documentation": ""
			},
			{
				"signature": "long bitSize()",
				"documentation": "/** Number of bits */"
			},
			{
				"signature": "long cardinality()",
				"documentation": "/** Number of set bits (1s) */"
			},
			{
				"signature": "void putAll(BitArray array)",
				"documentation": "/** Combines the two BitArrays using bitwise OR. */"
			},
			{
				"signature": "void and(BitArray array)",
				"documentation": "/** Combines the two BitArrays using bitwise AND. */"
			},
			{
				"signature": "void writeTo(DataOutputStream out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "static BitArray readFrom(DataInputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Bloom filter is a space-efficient probabilistic data structure that offers an approximate\n * containment test with one-sided error: if it claims that an item is contained in it, this\n * might be in error, but if it claims that an item is \u003ci\u003enot\u003c/i\u003e contained in it, then this is\n * definitely true. Currently supported data types include:\n * \u003cul\u003e\n *   \u003cli\u003e{@link Byte}\u003c/li\u003e\n *   \u003cli\u003e{@link Short}\u003c/li\u003e\n *   \u003cli\u003e{@link Integer}\u003c/li\u003e\n *   \u003cli\u003e{@link Long}\u003c/li\u003e\n *   \u003cli\u003e{@link String}\u003c/li\u003e\n * \u003c/ul\u003e\n * The false positive probability ({@code FPP}) of a Bloom filter is defined as the probability that\n * {@linkplain #mightContain(Object)} will erroneously return {@code true} for an object that has\n * not actually been put in the {@code BloomFilter}.\n *\n * The implementation is largely based on the {@code BloomFilter} class from Guava.\n */",
		"name": "org.apache.spark.util.sketch.BloomFilter",
		"extends": "",
		"Methods": [
			{
				"signature": "public long cardinality()",
				"documentation": "/**\n   * @return the number of set bits in this {@link BloomFilter}.\n   */"
			},
			{
				"signature": "public static BloomFilter readFrom(InputStream in) throws IOException",
				"documentation": "/**\n   * Reads in a {@link BloomFilter} from an input stream. It is the caller's responsibility to close\n   * the stream.\n   */"
			},
			{
				"signature": "private static int optimalNumOfHashFunctions(long n, long m)",
				"documentation": "/**\n   * Computes the optimal k (number of hashes per item inserted in Bloom filter), given the\n   * expected insertions and total number of bits in the Bloom filter.\n   *\n   * See http://en.wikipedia.org/wiki/File:Bloom_filter_fp_probability.svg for the formula.\n   *\n   * @param n expected insertions (must be positive)\n   * @param m total number of bits in Bloom filter (must be positive)\n   */"
			},
			{
				"signature": "private static long optimalNumOfBits(long n, double p)",
				"documentation": "/**\n   * Computes m (total bits of Bloom filter) which is expected to achieve, for the specified\n   * expected insertions, the required false positive probability.\n   *\n   * See http://en.wikipedia.org/wiki/Bloom_filter#Probability_of_false_positives for the formula.\n   *\n   * @param n expected insertions (must be positive)\n   * @param p false positive rate (must be 0 \u003c p \u003c 1)\n   */"
			},
			{
				"signature": "public static BloomFilter create(long expectedNumItems)",
				"documentation": "/**\n   * Creates a {@link BloomFilter} with the expected number of insertions and a default expected\n   * false positive probability of 3%.\n   *\n   * Note that overflowing a {@code BloomFilter} with significantly more elements than specified,\n   * will result in its saturation, and a sharp deterioration of its false positive probability.\n   */"
			},
			{
				"signature": "public static BloomFilter create(long expectedNumItems, double fpp)",
				"documentation": "/**\n   * Creates a {@link BloomFilter} with the expected number of insertions and expected false\n   * positive probability.\n   *\n   * Note that overflowing a {@code BloomFilter} with significantly more elements than specified,\n   * will result in its saturation, and a sharp deterioration of its false positive probability.\n   */"
			},
			{
				"signature": "public static BloomFilter create(long expectedNumItems, long numBits)",
				"documentation": "/**\n   * Creates a {@link BloomFilter} with given {@code expectedNumItems} and {@code numBits}, it will\n   * pick an optimal {@code numHashFunctions} which can minimize {@code fpp} for the bloom filter.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.sketch.BloomFilterImpl"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.sketch.Version"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.sketch.Version"
		]
	},
	{
		"documentation": "/**\n * A Bloom filter is a space-efficient probabilistic data structure that offers an approximate\n * containment test with one-sided error: if it claims that an item is contained in it, this\n * might be in error, but if it claims that an item is \u003ci\u003enot\u003c/i\u003e contained in it, then this is\n * definitely true. Currently supported data types include:\n * \u003cul\u003e\n *   \u003cli\u003e{@link Byte}\u003c/li\u003e\n *   \u003cli\u003e{@link Short}\u003c/li\u003e\n *   \u003cli\u003e{@link Integer}\u003c/li\u003e\n *   \u003cli\u003e{@link Long}\u003c/li\u003e\n *   \u003cli\u003e{@link String}\u003c/li\u003e\n * \u003c/ul\u003e\n * The false positive probability ({@code FPP}) of a Bloom filter is defined as the probability that\n * {@linkplain #mightContain(Object)} will erroneously return {@code true} for an object that has\n * not actually been put in the {@code BloomFilter}.\n *\n * The implementation is largely based on the {@code BloomFilter} class from Guava.\n */",
		"name": "org.apache.spark.util.sketch.Version",
		"extends": "",
		"Methods": [
			{
				"signature": "Version(int versionNumber)",
				"documentation": "/**\n     * {@code BloomFilter} binary format version 1. All values written in big-endian order:\n     * \u003cul\u003e\n     *   \u003cli\u003eVersion number, always 1 (32 bit)\u003c/li\u003e\n     *   \u003cli\u003eNumber of hash functions (32 bit)\u003c/li\u003e\n     *   \u003cli\u003eTotal number of words of the underlying bit array (32 bit)\u003c/li\u003e\n     *   \u003cli\u003eThe words/longs (numWords * 64 bit)\u003c/li\u003e\n     * \u003c/ul\u003e\n     */"
			},
			{
				"signature": "int getVersionNumber()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.sketch.BloomFilter",
			"org.apache.spark.util.sketch.CountMinSketch"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.sketch.BloomFilterImpl",
		"extends": "org.apache.spark.util.sketch.BloomFilter",
		"Methods": [
			{
				"signature": "BloomFilterImpl(int numHashFunctions, long numBits)",
				"documentation": ""
			},
			{
				"signature": "private BloomFilterImpl(BitArray bits, int numHashFunctions)",
				"documentation": ""
			},
			{
				"signature": "private BloomFilterImpl()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double expectedFpp()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long bitSize()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean put(Object item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean putString(String item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean putBinary(byte[] item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean mightContainString(String item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean mightContainBinary(byte[] item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean putLong(long item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean mightContainLong(long item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean mightContain(Object item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isCompatible(BloomFilter other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public BloomFilter mergeInPlace(BloomFilter other) throws IncompatibleMergeException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public BloomFilter intersectInPlace(BloomFilter other) throws IncompatibleMergeException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long cardinality()",
				"documentation": ""
			},
			{
				"signature": "private BloomFilterImpl checkCompatibilityForMerge(BloomFilter other)\n          throws IncompatibleMergeException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void writeTo(OutputStream out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void readFrom0(InputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public static BloomFilterImpl readFrom(InputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void writeObject(ObjectOutputStream out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void readObject(ObjectInputStream in) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Count-min sketch is a probabilistic data structure used for cardinality estimation using\n * sub-linear space.  Currently, supported data types include:\n * \u003cul\u003e\n *   \u003cli\u003e{@link Byte}\u003c/li\u003e\n *   \u003cli\u003e{@link Short}\u003c/li\u003e\n *   \u003cli\u003e{@link Integer}\u003c/li\u003e\n *   \u003cli\u003e{@link Long}\u003c/li\u003e\n *   \u003cli\u003e{@link String}\u003c/li\u003e\n * \u003c/ul\u003e\n * A {@link CountMinSketch} is initialized with a random seed, and a pair of parameters:\n * \u003col\u003e\n *   \u003cli\u003erelative error (or {@code eps}), and\n *   \u003cli\u003econfidence (or {@code delta})\n * \u003c/ol\u003e\n * Suppose you want to estimate the number of times an element {@code x} has appeared in a data\n * stream so far.  With probability {@code delta}, the estimate of this frequency is within the\n * range {@code true frequency \u003c= estimate \u003c= true frequency + eps * N}, where {@code N} is the\n * total count of items have appeared the data stream so far.\n *\n * Under the cover, a {@link CountMinSketch} is essentially a two-dimensional {@code long} array\n * with depth {@code d} and width {@code w}, where\n * \u003cul\u003e\n *   \u003cli\u003e{@code d = ceil(2 / eps)}\u003c/li\u003e\n *   \u003cli\u003e{@code w = ceil(-log(1 - confidence) / log(2))}\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * This implementation is largely based on the {@code CountMinSketch} class from stream-lib.\n */",
		"name": "org.apache.spark.util.sketch.CountMinSketch",
		"extends": "",
		"Methods": [
			{
				"signature": "public static CountMinSketch readFrom(InputStream in) throws IOException",
				"documentation": "/**\n   * Reads in a {@link CountMinSketch} from an input stream. It is the caller's responsibility to\n   * close the stream.\n   */"
			},
			{
				"signature": "public static CountMinSketch readFrom(byte[] bytes) throws IOException",
				"documentation": "/**\n   * Reads in a {@link CountMinSketch} from a byte array.\n   */"
			},
			{
				"signature": "public static CountMinSketch create(int depth, int width, int seed)",
				"documentation": "/**\n   * Creates a {@link CountMinSketch} with given {@code depth}, {@code width}, and random\n   * {@code seed}.\n   *\n   * @param depth depth of the Count-min Sketch, must be positive\n   * @param width width of the Count-min Sketch, must be positive\n   * @param seed random seed\n   */"
			},
			{
				"signature": "public static CountMinSketch create(double eps, double confidence, int seed)",
				"documentation": "/**\n   * Creates a {@link CountMinSketch} with given relative error ({@code eps}), {@code confidence},\n   * and random {@code seed}.\n   *\n   * @param eps relative error, must be positive\n   * @param confidence confidence, must be positive and less than 1.0\n   * @param seed random seed\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.sketch.CountMinSketchImpl"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.sketch.Version"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.sketch.Version"
		]
	},
	{
		"documentation": "/**\n * A Count-min sketch is a probabilistic data structure used for cardinality estimation using\n * sub-linear space.  Currently, supported data types include:\n * \u003cul\u003e\n *   \u003cli\u003e{@link Byte}\u003c/li\u003e\n *   \u003cli\u003e{@link Short}\u003c/li\u003e\n *   \u003cli\u003e{@link Integer}\u003c/li\u003e\n *   \u003cli\u003e{@link Long}\u003c/li\u003e\n *   \u003cli\u003e{@link String}\u003c/li\u003e\n * \u003c/ul\u003e\n * A {@link CountMinSketch} is initialized with a random seed, and a pair of parameters:\n * \u003col\u003e\n *   \u003cli\u003erelative error (or {@code eps}), and\n *   \u003cli\u003econfidence (or {@code delta})\n * \u003c/ol\u003e\n * Suppose you want to estimate the number of times an element {@code x} has appeared in a data\n * stream so far.  With probability {@code delta}, the estimate of this frequency is within the\n * range {@code true frequency \u003c= estimate \u003c= true frequency + eps * N}, where {@code N} is the\n * total count of items have appeared the data stream so far.\n *\n * Under the cover, a {@link CountMinSketch} is essentially a two-dimensional {@code long} array\n * with depth {@code d} and width {@code w}, where\n * \u003cul\u003e\n *   \u003cli\u003e{@code d = ceil(2 / eps)}\u003c/li\u003e\n *   \u003cli\u003e{@code w = ceil(-log(1 - confidence) / log(2))}\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * This implementation is largely based on the {@code CountMinSketch} class from stream-lib.\n */",
		"name": "org.apache.spark.util.sketch.Version",
		"extends": "",
		"Methods": [
			{
				"signature": "Version(int versionNumber)",
				"documentation": "/**\n     * {@code CountMinSketch} binary format version 1.  All values written in big-endian order:\n     * \u003cul\u003e\n     *   \u003cli\u003eVersion number, always 1 (32 bit)\u003c/li\u003e\n     *   \u003cli\u003eTotal count of added items (64 bit)\u003c/li\u003e\n     *   \u003cli\u003eDepth (32 bit)\u003c/li\u003e\n     *   \u003cli\u003eWidth (32 bit)\u003c/li\u003e\n     *   \u003cli\u003eHash functions (depth * 64 bit)\u003c/li\u003e\n     *   \u003cli\u003e\n     *     Count table\n     *     \u003cul\u003e\n     *       \u003cli\u003eRow 0 (width * 64 bit)\u003c/li\u003e\n     *       \u003cli\u003eRow 1 (width * 64 bit)\u003c/li\u003e\n     *       \u003cli\u003e...\u003c/li\u003e\n     *       \u003cli\u003eRow {@code depth - 1} (width * 64 bit)\u003c/li\u003e\n     *     \u003c/ul\u003e\n     *   \u003c/li\u003e\n     * \u003c/ul\u003e\n     */"
			},
			{
				"signature": "int getVersionNumber()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.sketch.BloomFilter",
			"org.apache.spark.util.sketch.CountMinSketch"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.sketch.CountMinSketchImpl",
		"extends": "org.apache.spark.util.sketch.CountMinSketch",
		"Methods": [
			{
				"signature": "private CountMinSketchImpl()",
				"documentation": ""
			},
			{
				"signature": "CountMinSketchImpl(int depth, int width, int seed)",
				"documentation": ""
			},
			{
				"signature": "CountMinSketchImpl(double eps, double confidence, int seed)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "private void initTablesWith(int depth, int width, int seed)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double relativeError()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double confidence()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int depth()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int width()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long totalCount()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void add(Object item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void add(Object item, long count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void addString(String item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void addString(String item, long count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void addLong(long item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void addLong(long item, long count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void addBinary(byte[] item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void addBinary(byte[] item, long count)",
				"documentation": ""
			},
			{
				"signature": "private int hash(long item, int count)",
				"documentation": ""
			},
			{
				"signature": "private static int[] getHashBuckets(String key, int hashCount, int max)",
				"documentation": ""
			},
			{
				"signature": "private static int[] getHashBuckets(byte[] b, int hashCount, int max)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long estimateCount(Object item)",
				"documentation": ""
			},
			{
				"signature": "private long estimateCountForLongItem(long item)",
				"documentation": ""
			},
			{
				"signature": "private long estimateCountForStringItem(String item)",
				"documentation": ""
			},
			{
				"signature": "private long estimateCountForBinaryItem(byte[] item)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public CountMinSketch mergeInPlace(CountMinSketch other) throws IncompatibleMergeException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void writeTo(OutputStream out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] toByteArray() throws IOException",
				"documentation": ""
			},
			{
				"signature": "public static CountMinSketchImpl readFrom(InputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void readFrom0(InputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void writeObject(ObjectOutputStream out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void readObject(ObjectInputStream in) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.sketch.IncompatibleMergeException",
		"extends": "Exception",
		"Methods": [
			{
				"signature": "public IncompatibleMergeException(String message)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * 32-bit Murmur3 hasher.  This is based on Guava's Murmur3_32HashFunction.\n */",
		"name": "org.apache.spark.util.sketch.Murmur3_x86_32",
		"extends": "",
		"Methods": [
			{
				"signature": "Murmur3_x86_32(int seed)",
				"documentation": "/**\n * 32-bit Murmur3 hasher.  This is based on Guava's Murmur3_32HashFunction.\n */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "public int hashInt(int input)",
				"documentation": ""
			},
			{
				"signature": "public static int hashInt(int input, int seed)",
				"documentation": ""
			},
			{
				"signature": "public int hashUnsafeWords(Object base, long offset, int lengthInBytes)",
				"documentation": ""
			},
			{
				"signature": "public static int hashUnsafeWords(Object base, long offset, int lengthInBytes, int seed)",
				"documentation": ""
			},
			{
				"signature": "public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes, int seed)",
				"documentation": ""
			},
			{
				"signature": "public static int hashUnsafeBytes2(Object base, long offset, int lengthInBytes, int seed)",
				"documentation": ""
			},
			{
				"signature": "private static int hashBytesByInt(Object base, long offset, int lengthInBytes, int seed)",
				"documentation": ""
			},
			{
				"signature": "public int hashLong(long input)",
				"documentation": ""
			},
			{
				"signature": "public static int hashLong(long input, int seed)",
				"documentation": ""
			},
			{
				"signature": "private static int mixK1(int k1)",
				"documentation": ""
			},
			{
				"signature": "private static int mixH1(int h1, int k1)",
				"documentation": ""
			},
			{
				"signature": "private static int fmix(int h1, int length)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.sketch.Platform",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int getInt(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putInt(Object object, long offset, int value)",
				"documentation": ""
			},
			{
				"signature": "public static boolean getBoolean(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putBoolean(Object object, long offset, boolean value)",
				"documentation": ""
			},
			{
				"signature": "public static byte getByte(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putByte(Object object, long offset, byte value)",
				"documentation": ""
			},
			{
				"signature": "public static short getShort(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putShort(Object object, long offset, short value)",
				"documentation": ""
			},
			{
				"signature": "public static long getLong(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putLong(Object object, long offset, long value)",
				"documentation": ""
			},
			{
				"signature": "public static float getFloat(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putFloat(Object object, long offset, float value)",
				"documentation": ""
			},
			{
				"signature": "public static double getDouble(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putDouble(Object object, long offset, double value)",
				"documentation": ""
			},
			{
				"signature": "public static Object getObjectVolatile(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putObjectVolatile(Object object, long offset, Object value)",
				"documentation": ""
			},
			{
				"signature": "public static long allocateMemory(long size)",
				"documentation": ""
			},
			{
				"signature": "public static void freeMemory(long address)",
				"documentation": ""
			},
			{
				"signature": "public static void copyMemory(\n    Object src, long srcOffset, Object dst, long dstOffset, long length)",
				"documentation": ""
			},
			{
				"signature": "public static void throwException(Throwable t)",
				"documentation": "/**\n   * Raises an exception bypassing compiler checks for checked exceptions.\n   */"
			},
			{
				"signature": "static",
				"documentation": "/**\n   * Limits the number of bytes to copy per {@link Unsafe#copyMemory(long, long, long)} to\n   * allow safepoint polling during a large copy.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.sketch.Utils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static byte[] getBytesFromUTF8String(String str)",
				"documentation": ""
			},
			{
				"signature": "public static long integralToLong(Object i)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A new component of Spark which may have unstable API's.\n *\n * NOTE: If there exists a Scaladoc comment that immediately precedes this annotation, the first\n * line of the comment must be \":: AlphaComponent ::\" with no trailing blank line. This is because\n * of the known issue that Scaladoc displays only either the annotation or the comment, whichever\n * comes first.\n */",
		"name": "org.apache.spark.annotation.AlphaComponent",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.mllib.JavaPackage"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A lower-level, unstable API intended for developers.\n *\n * Developer API's might change or be removed in minor versions of Spark.\n *\n * NOTE: If there exists a Scaladoc comment that immediately precedes this annotation, the first\n * line of the comment must be \":: DeveloperApi ::\" with no trailing blank line. This is because\n * of the known issue that Scaladoc displays only either the annotation or the comment, whichever\n * comes first.\n */",
		"name": "org.apache.spark.annotation.DeveloperApi",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.SparkFirehoseListener",
			"org.apache.spark.api.plugin.DriverPlugin",
			"org.apache.spark.api.plugin.ExecutorPlugin",
			"org.apache.spark.api.plugin.PluginContext",
			"org.apache.spark.api.plugin.SparkPlugin",
			"org.apache.spark.api.resource.ResourceDiscoveryPlugin",
			"org.apache.spark.sql.catalyst.expressions.ExpressionDescription",
			"org.apache.spark.sql.types.SQLUserDefinedType",
			"org.apache.spark.sql.vectorized.ArrowColumnVector",
			"org.apache.spark.sql.vectorized.ColumnarBatch",
			"org.apache.spark.sql.vectorized.ColumnarBatchRow",
			"org.apache.spark.streaming.StreamingContextState"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * APIs that are meant to evolve towards becoming stable APIs, but are not stable APIs yet.\n * Evolving interfaces can change from one feature release to another release (i.e. 2.1 to 2.2).\n */",
		"name": "org.apache.spark.annotation.Evolving",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.ErrorHandler",
			"org.apache.spark.network.shuffle.MergedShuffleFileManager",
			"org.apache.spark.SparkThrowable",
			"org.apache.spark.sql.connector.catalog.CatalogExtension",
			"org.apache.spark.sql.connector.catalog.CatalogPlugin",
			"org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension",
			"org.apache.spark.sql.connector.catalog.FunctionCatalog",
			"org.apache.spark.sql.connector.catalog.Identifier",
			"org.apache.spark.sql.connector.catalog.IdentifierImpl",
			"org.apache.spark.sql.connector.catalog.MetadataColumn",
			"org.apache.spark.sql.connector.catalog.NamespaceChange",
			"org.apache.spark.sql.connector.catalog.SessionConfigSupport",
			"org.apache.spark.sql.connector.catalog.StagedTable",
			"org.apache.spark.sql.connector.catalog.StagingTableCatalog",
			"org.apache.spark.sql.connector.catalog.SupportsCatalogOptions",
			"org.apache.spark.sql.connector.catalog.SupportsDelete",
			"org.apache.spark.sql.connector.catalog.SupportsMetadataColumns",
			"org.apache.spark.sql.connector.catalog.SupportsNamespaces",
			"org.apache.spark.sql.connector.catalog.SupportsRead",
			"org.apache.spark.sql.connector.catalog.SupportsWrite",
			"org.apache.spark.sql.connector.catalog.Table",
			"org.apache.spark.sql.connector.catalog.TableCapability",
			"org.apache.spark.sql.connector.catalog.TableCatalog",
			"org.apache.spark.sql.connector.catalog.TableChange",
			"org.apache.spark.sql.connector.catalog.TableProvider",
			"org.apache.spark.sql.connector.catalog.TruncatableTable",
			"org.apache.spark.sql.connector.catalog.functions.AggregateFunction",
			"org.apache.spark.sql.connector.catalog.functions.BoundFunction",
			"org.apache.spark.sql.connector.catalog.functions.Function",
			"org.apache.spark.sql.connector.catalog.functions.ScalarFunction",
			"org.apache.spark.sql.connector.catalog.functions.UnboundFunction",
			"org.apache.spark.sql.connector.catalog.index.SupportsIndex",
			"org.apache.spark.sql.connector.catalog.index.TableIndex",
			"org.apache.spark.sql.connector.expressions.Cast",
			"org.apache.spark.sql.connector.expressions.Expression",
			"org.apache.spark.sql.connector.expressions.Expressions",
			"org.apache.spark.sql.connector.expressions.GeneralScalarExpression",
			"org.apache.spark.sql.connector.expressions.Literal",
			"org.apache.spark.sql.connector.expressions.NamedReference",
			"org.apache.spark.sql.connector.expressions.Transform",
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc",
			"org.apache.spark.sql.connector.expressions.aggregate.Aggregation",
			"org.apache.spark.sql.connector.expressions.aggregate.Avg",
			"org.apache.spark.sql.connector.expressions.aggregate.Count",
			"org.apache.spark.sql.connector.expressions.aggregate.CountStar",
			"org.apache.spark.sql.connector.expressions.aggregate.GeneralAggregateFunc",
			"org.apache.spark.sql.connector.expressions.aggregate.Max",
			"org.apache.spark.sql.connector.expressions.aggregate.Min",
			"org.apache.spark.sql.connector.expressions.aggregate.Sum",
			"org.apache.spark.sql.connector.expressions.filter.AlwaysFalse",
			"org.apache.spark.sql.connector.expressions.filter.AlwaysTrue",
			"org.apache.spark.sql.connector.expressions.filter.And",
			"org.apache.spark.sql.connector.expressions.filter.Not",
			"org.apache.spark.sql.connector.expressions.filter.Or",
			"org.apache.spark.sql.connector.expressions.filter.Predicate",
			"org.apache.spark.sql.connector.metric.CustomAvgMetric",
			"org.apache.spark.sql.connector.metric.CustomMetric",
			"org.apache.spark.sql.connector.metric.CustomSumMetric",
			"org.apache.spark.sql.connector.metric.CustomTaskMetric",
			"org.apache.spark.sql.connector.read.Batch",
			"org.apache.spark.sql.connector.read.InputPartition",
			"org.apache.spark.sql.connector.read.PartitionReader",
			"org.apache.spark.sql.connector.read.PartitionReaderFactory",
			"org.apache.spark.sql.connector.read.Scan",
			"org.apache.spark.sql.connector.read.ScanBuilder",
			"org.apache.spark.sql.connector.read.Statistics",
			"org.apache.spark.sql.connector.read.SupportsPushDownAggregates",
			"org.apache.spark.sql.connector.read.SupportsPushDownFilters",
			"org.apache.spark.sql.connector.read.SupportsPushDownLimit",
			"org.apache.spark.sql.connector.read.SupportsPushDownRequiredColumns",
			"org.apache.spark.sql.connector.read.SupportsPushDownTableSample",
			"org.apache.spark.sql.connector.read.SupportsPushDownTopN",
			"org.apache.spark.sql.connector.read.SupportsPushDownV2Filters",
			"org.apache.spark.sql.connector.read.SupportsReportPartitioning",
			"org.apache.spark.sql.connector.read.SupportsReportStatistics",
			"org.apache.spark.sql.connector.read.partitioning.KeyGroupedPartitioning",
			"org.apache.spark.sql.connector.read.partitioning.Partitioning",
			"org.apache.spark.sql.connector.read.partitioning.UnknownPartitioning",
			"org.apache.spark.sql.connector.read.streaming.CompositeReadLimit",
			"org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader",
			"org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory",
			"org.apache.spark.sql.connector.read.streaming.ContinuousStream",
			"org.apache.spark.sql.connector.read.streaming.MicroBatchStream",
			"org.apache.spark.sql.connector.read.streaming.Offset",
			"org.apache.spark.sql.connector.read.streaming.PartitionOffset",
			"org.apache.spark.sql.connector.read.streaming.ReadAllAvailable",
			"org.apache.spark.sql.connector.read.streaming.ReadLimit",
			"org.apache.spark.sql.connector.read.streaming.ReadMaxFiles",
			"org.apache.spark.sql.connector.read.streaming.ReadMaxRows",
			"org.apache.spark.sql.connector.read.streaming.ReadMinRows",
			"org.apache.spark.sql.connector.read.streaming.ReportsSinkMetrics",
			"org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics",
			"org.apache.spark.sql.connector.read.streaming.SparkDataStream",
			"org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl",
			"org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow",
			"org.apache.spark.sql.connector.write.BatchWrite",
			"org.apache.spark.sql.connector.write.DataWriter",
			"org.apache.spark.sql.connector.write.DataWriterFactory",
			"org.apache.spark.sql.connector.write.LogicalWriteInfo",
			"org.apache.spark.sql.connector.write.PhysicalWriteInfo",
			"org.apache.spark.sql.connector.write.SupportsDynamicOverwrite",
			"org.apache.spark.sql.connector.write.SupportsOverwrite",
			"org.apache.spark.sql.connector.write.SupportsTruncate",
			"org.apache.spark.sql.connector.write.Write",
			"org.apache.spark.sql.connector.write.WriteBuilder",
			"org.apache.spark.sql.connector.write.WriterCommitMessage",
			"org.apache.spark.sql.connector.write.streaming.StreamingDataWriterFactory",
			"org.apache.spark.sql.connector.write.streaming.StreamingWrite",
			"org.apache.spark.sql.streaming.GroupStateTimeout",
			"org.apache.spark.sql.streaming.OutputMode",
			"org.apache.spark.sql.vectorized.ColumnVector",
			"org.apache.spark.sql.vectorized.ColumnarArray",
			"org.apache.spark.sql.vectorized.ColumnarRow",
			"org.apache.spark.api.java.function.FlatMapGroupsWithStateFunction",
			"org.apache.spark.api.java.function.MapGroupsWithStateFunction",
			"org.apache.spark.sql.streaming.Trigger"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An experimental user-facing API.\n *\n * Experimental API's might change or be removed in minor versions of Spark, or be adopted as\n * first-class Spark API's.\n *\n * NOTE: If there exists a Scaladoc comment that immediately precedes this annotation, the first\n * line of the comment must be \":: Experimental ::\" with no trailing blank line. This is because\n * of the known issue that Scaladoc displays only either the annotation or the comment, whichever\n * comes first.\n */",
		"name": "org.apache.spark.annotation.Experimental",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.SupportsAtomicPartitionManagement",
			"org.apache.spark.sql.connector.catalog.SupportsPartitionManagement",
			"org.apache.spark.sql.connector.catalog.SupportsRowLevelOperations",
			"org.apache.spark.sql.connector.distributions.ClusteredDistribution",
			"org.apache.spark.sql.connector.distributions.Distribution",
			"org.apache.spark.sql.connector.distributions.Distributions",
			"org.apache.spark.sql.connector.distributions.OrderedDistribution",
			"org.apache.spark.sql.connector.distributions.UnspecifiedDistribution",
			"org.apache.spark.sql.connector.expressions.NullOrdering",
			"org.apache.spark.sql.connector.expressions.SortDirection",
			"org.apache.spark.sql.connector.expressions.SortOrder",
			"org.apache.spark.sql.connector.read.LocalScan",
			"org.apache.spark.sql.connector.read.SupportsRuntimeFiltering",
			"org.apache.spark.sql.connector.write.RequiresDistributionAndOrdering",
			"org.apache.spark.sql.connector.write.RowLevelOperation",
			"org.apache.spark.sql.connector.write.RowLevelOperationBuilder",
			"org.apache.spark.sql.connector.write.RowLevelOperationInfo",
			"org.apache.spark.sql.streaming.GroupStateTimeout",
			"org.apache.spark.sql.util.CaseInsensitiveStringMap",
			"org.apache.spark.api.java.function.FlatMapGroupsWithStateFunction",
			"org.apache.spark.api.java.function.MapGroupsWithStateFunction"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A class that is considered private to the internals of Spark -- there is a high-likelihood\n * they will be changed in future versions of Spark.\n *\n * This should be used only when the standard Scala / Java means of protecting classes are\n * insufficient.  In particular, Java has no equivalent of private[spark], so we use this annotation\n * in its place.\n *\n * NOTE: If there exists a Scaladoc comment that immediately precedes this annotation, the first\n * line of the comment must be \":: Private ::\" with no trailing blank line. This is because\n * of the known issue that Scaladoc displays only either the annotation or the comment, whichever\n * comes first.\n */",
		"name": "org.apache.spark.annotation.Private",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.kvstore.InMemoryStore",
			"org.apache.spark.util.kvstore.KVIndex",
			"org.apache.spark.util.kvstore.KVStore",
			"org.apache.spark.util.kvstore.KVStoreIterator",
			"org.apache.spark.util.kvstore.KVStoreSerializer",
			"org.apache.spark.util.kvstore.KVStoreView",
			"org.apache.spark.util.kvstore.KVTypeInfo",
			"org.apache.spark.util.kvstore.LevelDB",
			"org.apache.spark.util.kvstore.RocksDB",
			"org.apache.spark.util.kvstore.UnsupportedStoreVersionException",
			"org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper",
			"org.apache.spark.memory.MemoryMode",
			"org.apache.spark.memory.SparkOutOfMemoryError",
			"org.apache.spark.serializer.DummySerializerInstance",
			"org.apache.spark.shuffle.api.ShuffleDataIO",
			"org.apache.spark.shuffle.api.ShuffleDriverComponents",
			"org.apache.spark.shuffle.api.ShuffleExecutorComponents",
			"org.apache.spark.shuffle.api.ShuffleMapOutputWriter",
			"org.apache.spark.shuffle.api.ShufflePartitionWriter",
			"org.apache.spark.shuffle.api.SingleSpillShuffleMapOutputWriter",
			"org.apache.spark.shuffle.api.WritableByteChannelWrapper",
			"org.apache.spark.shuffle.api.metadata.MapOutputCommitMessage",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter",
			"org.apache.spark.storage.TimeTrackingOutputStream",
			"org.apache.spark.util.EnumUtil",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Stable APIs that retain source and binary compatibility within a major release.\n * These interfaces can change from one major release to another major release\n * (e.g. from 1.0 to 2.0).\n */",
		"name": "org.apache.spark.annotation.Stable",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.sql.api.java.UDF23Test"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.RowFactory",
			"org.apache.spark.sql.types.DataTypes",
			"org.apache.spark.sql.SaveMode",
			"org.apache.spark.sql.api.java.UDF0",
			"org.apache.spark.sql.api.java.UDF1",
			"org.apache.spark.sql.api.java.UDF10",
			"org.apache.spark.sql.api.java.UDF11",
			"org.apache.spark.sql.api.java.UDF12",
			"org.apache.spark.sql.api.java.UDF13",
			"org.apache.spark.sql.api.java.UDF14",
			"org.apache.spark.sql.api.java.UDF15",
			"org.apache.spark.sql.api.java.UDF16",
			"org.apache.spark.sql.api.java.UDF17",
			"org.apache.spark.sql.api.java.UDF18",
			"org.apache.spark.sql.api.java.UDF19",
			"org.apache.spark.sql.api.java.UDF2",
			"org.apache.spark.sql.api.java.UDF20",
			"org.apache.spark.sql.api.java.UDF21",
			"org.apache.spark.sql.api.java.UDF22",
			"org.apache.spark.sql.api.java.UDF3",
			"org.apache.spark.sql.api.java.UDF4",
			"org.apache.spark.sql.api.java.UDF5",
			"org.apache.spark.sql.api.java.UDF6",
			"org.apache.spark.sql.api.java.UDF7",
			"org.apache.spark.sql.api.java.UDF8",
			"org.apache.spark.sql.api.java.UDF9"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Unstable APIs, with no guarantee on stability.\n * Classes that are unannotated are considered Unstable.\n */",
		"name": "org.apache.spark.annotation.Unstable",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.unsafe.types.CalendarInterval",
			"org.apache.spark.sql.connector.ExternalCommandRunner",
			"org.apache.spark.sql.connector.read.V1Scan",
			"org.apache.spark.sql.connector.write.V1Write",
			"org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.tags.ChromeUITest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.tags.DockerTest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.tags.ExtendedHiveTest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.tags.ExtendedLevelDBTest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.tags.ExtendedSQLTest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.tags.ExtendedYarnTest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.tags.SlowHiveTest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Simulates Hive's hashing function from Hive v1.2.1\n * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode()\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.HiveHasher",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public String toString()",
				"documentation": "/**\n * Simulates Hive's hashing function from Hive v1.2.1\n * org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils#hashcode()\n */"
			},
			{
				"signature": "public static int hashInt(int input)",
				"documentation": ""
			},
			{
				"signature": "public static int hashLong(long input)",
				"documentation": ""
			},
			{
				"signature": "public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.catalyst.util.DateTimeConstants",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.KVIterator",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.Platform",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": ""
			},
			{
				"signature": "public static boolean unaligned()",
				"documentation": "/**\n   * @return true when running JVM is having sun's Unsafe package available in it and underlying\n   *         system having unaligned-access capability.\n   */"
			},
			{
				"signature": "public static int getInt(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putInt(Object object, long offset, int value)",
				"documentation": ""
			},
			{
				"signature": "public static boolean getBoolean(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putBoolean(Object object, long offset, boolean value)",
				"documentation": ""
			},
			{
				"signature": "public static byte getByte(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putByte(Object object, long offset, byte value)",
				"documentation": ""
			},
			{
				"signature": "public static short getShort(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putShort(Object object, long offset, short value)",
				"documentation": ""
			},
			{
				"signature": "public static long getLong(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putLong(Object object, long offset, long value)",
				"documentation": ""
			},
			{
				"signature": "public static float getFloat(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putFloat(Object object, long offset, float value)",
				"documentation": ""
			},
			{
				"signature": "public static double getDouble(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putDouble(Object object, long offset, double value)",
				"documentation": ""
			},
			{
				"signature": "public static Object getObjectVolatile(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putObjectVolatile(Object object, long offset, Object value)",
				"documentation": ""
			},
			{
				"signature": "public static long allocateMemory(long size)",
				"documentation": ""
			},
			{
				"signature": "public static void freeMemory(long address)",
				"documentation": ""
			},
			{
				"signature": "public static long reallocateMemory(long address, long oldSize, long newSize)",
				"documentation": ""
			},
			{
				"signature": "public static ByteBuffer allocateDirectBuffer(int size)",
				"documentation": "/**\n   * Allocate a DirectByteBuffer, potentially bypassing the JVM's MaxDirectMemorySize limit.\n   */"
			},
			{
				"signature": "public static void setMemory(Object object, long offset, long size, byte value)",
				"documentation": ""
			},
			{
				"signature": "public static void setMemory(long address, byte value, long size)",
				"documentation": ""
			},
			{
				"signature": "public static void copyMemory(\n    Object src, long srcOffset, Object dst, long dstOffset, long length)",
				"documentation": ""
			},
			{
				"signature": "public static void throwException(Throwable t)",
				"documentation": "/**\n   * Raises an exception bypassing compiler checks for checked exceptions.\n   */"
			},
			{
				"signature": "static",
				"documentation": "/**\n   * Limits the number of bytes to copy per {@link Unsafe#copyMemory(long, long, long)} to\n   * allow safepoint polling during a large copy.\n   */"
			},
			{
				"signature": "static",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.catalyst.expressions.HiveHasher",
			"org.apache.spark.unsafe.bitset.BitSetMethods",
			"org.apache.spark.unsafe.hash.Murmur3_x86_32",
			"org.apache.spark.unsafe.memory.UnsafeMemoryAllocator",
			"org.apache.spark.unsafe.types.ByteArray",
			"org.apache.spark.unsafe.types.UTF8String",
			"org.apache.spark.unsafe.array.ByteArraySuite",
			"org.apache.spark.serializer.DummySerializerInstance",
			"org.apache.spark.shuffle.sort.ShuffleExternalSorter",
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter",
			"org.apache.spark.shuffle.sort.ShuffleSortDataFormat",
			"org.apache.spark.unsafe.map.BytesToBytesMap",
			"org.apache.spark.unsafe.map.MapIterator",
			"org.apache.spark.unsafe.map.Location",
			"org.apache.spark.util.collection.unsafe.sort.RadixSort",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSortDataFormat",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter",
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite",
			"org.apache.spark.unsafe.map.AbstractBytesToBytesMapSuite",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite",
			"org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch",
			"org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeMapData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow",
			"org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch",
			"org.apache.spark.sql.catalyst.expressions.XXH64",
			"org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter",
			"org.apache.spark.sql.execution.RecordBinaryComparator",
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter",
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter",
			"org.apache.spark.sql.execution.KVSorterIterator",
			"org.apache.spark.sql.execution.vectorized.OffHeapColumnVector",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector",
			"test.org.apache.spark.sql.execution.sort.RecordBinaryComparatorSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A helper class to write {@link UTF8String}s to an internal buffer and build the concatenated\n * {@link UTF8String} at the end.\n */",
		"name": "org.apache.spark.unsafe.UTF8StringBuilder",
		"extends": "",
		"Methods": [
			{
				"signature": "public UTF8StringBuilder()",
				"documentation": "/**\n * A helper class to write {@link UTF8String}s to an internal buffer and build the concatenated\n * {@link UTF8String} at the end.\n */"
			},
			{
				"signature": "public UTF8StringBuilder(int initialSize)",
				"documentation": ""
			},
			{
				"signature": "private void grow(int neededSize)",
				"documentation": ""
			},
			{
				"signature": "private int totalSize()",
				"documentation": ""
			},
			{
				"signature": "public void append(UTF8String value)",
				"documentation": ""
			},
			{
				"signature": "public void append(String value)",
				"documentation": ""
			},
			{
				"signature": "public void appendBytes(Object base, long offset, int length)",
				"documentation": ""
			},
			{
				"signature": "public UTF8String build()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Class to make changes to record length offsets uniform through out\n * various areas of Apache Spark core and unsafe.  The SPARC platform\n * requires this because using a 4 byte Int for record lengths causes\n * the entire record of 8 byte Items to become misaligned by 4 bytes.\n * Using a 8 byte long for record length keeps things 8 byte aligned.\n */",
		"name": "org.apache.spark.unsafe.UnsafeAlignedOffset",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void setUaoSize(int size)",
				"documentation": "/**\n * Class to make changes to record length offsets uniform through out\n * various areas of Apache Spark core and unsafe.  The SPARC platform\n * requires this because using a 4 byte Int for record lengths causes\n * the entire record of 8 byte Items to become misaligned by 4 bytes.\n * Using a 8 byte long for record length keeps things 8 byte aligned.\n */"
			},
			{
				"signature": "public static int getUaoSize()",
				"documentation": ""
			},
			{
				"signature": "public static int getSize(Object object, long offset)",
				"documentation": ""
			},
			{
				"signature": "public static void putSize(Object object, long offset, int value)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.ShuffleExternalSorter",
			"org.apache.spark.unsafe.map.BytesToBytesMap",
			"org.apache.spark.unsafe.map.MapIterator",
			"org.apache.spark.unsafe.map.Location",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.SortComparator",
			"org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch",
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter",
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter.KVComparator",
			"org.apache.spark.sql.execution.KVSorterIterator",
			"test.org.apache.spark.sql.execution.sort.RecordBinaryComparatorSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.array.ByteArrayMethods",
		"extends": "",
		"Methods": [
			{
				"signature": "private ByteArrayMethods()",
				"documentation": ""
			},
			{
				"signature": "public static long nextPowerOf2(long num)",
				"documentation": "/** Returns the next number greater or equal num that is power of 2. */"
			},
			{
				"signature": "public static int roundNumberOfBytesToNearestWord(int numBytes)",
				"documentation": ""
			},
			{
				"signature": "public static long roundNumberOfBytesToNearestWord(long numBytes)",
				"documentation": ""
			},
			{
				"signature": "public static boolean arrayEquals(\n      Object leftBase, long leftOffset, Object rightBase, long rightOffset, final long length)",
				"documentation": "/**\n   * Optimized byte array equality check for byte arrays.\n   * @return true if the arrays are equal, false otherwise\n   */"
			},
			{
				"signature": "public static boolean contains(byte[] arr, byte[] sub)",
				"documentation": ""
			},
			{
				"signature": "public static boolean startsWith(byte[] array, byte[] target)",
				"documentation": ""
			},
			{
				"signature": "public static boolean endsWith(byte[] array, byte[] target)",
				"documentation": ""
			},
			{
				"signature": "public static boolean matchAt(byte[] arr, byte[] sub, int pos)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.unsafe.types.UTF8String",
			"org.apache.spark.unsafe.map.BytesToBytesMap",
			"org.apache.spark.unsafe.map.AbstractBytesToBytesMapSuite",
			"org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An array of long values. Compared with native JVM arrays, this:\n * \u003cul\u003e\n *   \u003cli\u003esupports using both on-heap and off-heap memory\u003c/li\u003e\n *   \u003cli\u003ehas no bound checking, and thus can crash the JVM process when assert is turned off\u003c/li\u003e\n * \u003c/ul\u003e\n */",
		"name": "org.apache.spark.unsafe.array.LongArray",
		"extends": "",
		"Methods": [
			{
				"signature": "public LongArray(MemoryBlock memory)",
				"documentation": "/**\n * An array of long values. Compared with native JVM arrays, this:\n * \u003cul\u003e\n *   \u003cli\u003esupports using both on-heap and off-heap memory\u003c/li\u003e\n *   \u003cli\u003ehas no bound checking, and thus can crash the JVM process when assert is turned off\u003c/li\u003e\n * \u003c/ul\u003e\n */"
			},
			{
				"signature": "public MemoryBlock memoryBlock()",
				"documentation": ""
			},
			{
				"signature": "public Object getBaseObject()",
				"documentation": ""
			},
			{
				"signature": "public long getBaseOffset()",
				"documentation": ""
			},
			{
				"signature": "public long size()",
				"documentation": "/**\n   * Returns the number of elements this array can hold.\n   */"
			},
			{
				"signature": "public void zeroOut()",
				"documentation": "/**\n   * Fill this all with 0L.\n   */"
			},
			{
				"signature": "public void set(int index, long value)",
				"documentation": "/**\n   * Sets the value at position {@code index}.\n   */"
			},
			{
				"signature": "public long get(int index)",
				"documentation": "/**\n   * Returns the value at position {@code index}.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Methods for working with fixed-size uncompressed bitsets.\n *\n * We assume that the bitset data is word-aligned (that is, a multiple of 8 bytes in length).\n *\n * Each bit occupies exactly one bit of storage.\n */",
		"name": "org.apache.spark.unsafe.bitset.BitSetMethods",
		"extends": "",
		"Methods": [
			{
				"signature": "private BitSetMethods()",
				"documentation": "/**\n * Methods for working with fixed-size uncompressed bitsets.\n *\n * We assume that the bitset data is word-aligned (that is, a multiple of 8 bytes in length).\n *\n * Each bit occupies exactly one bit of storage.\n */"
			},
			{
				"signature": "public static void set(Object baseObject, long baseOffset, int index)",
				"documentation": "/**\n   * Sets the bit at the specified index to {@code true}.\n   */"
			},
			{
				"signature": "public static void unset(Object baseObject, long baseOffset, int index)",
				"documentation": "/**\n   * Sets the bit at the specified index to {@code false}.\n   */"
			},
			{
				"signature": "public static boolean isSet(Object baseObject, long baseOffset, int index)",
				"documentation": "/**\n   * Returns {@code true} if the bit is set at the specified index.\n   */"
			},
			{
				"signature": "public static boolean anySet(Object baseObject, long baseOffset, long bitSetWidthInWords)",
				"documentation": "/**\n   * Returns {@code true} if any bit is set.\n   */"
			},
			{
				"signature": "public static int nextSetBit(\n      Object baseObject,\n      long baseOffset,\n      int fromIndex,\n      int bitsetSizeInWords)",
				"documentation": "/**\n   * Returns the index of the first bit that is set to true that occurs on or after the\n   * specified starting index. If no such bit exists then {@code -1} is returned.\n   * \u003cp\u003e\n   * To iterate over the true bits in a BitSet, use the following loop:\n   * \u003cpre\u003e\n   * \u003ccode\u003e\n   *  for (long i = bs.nextSetBit(0, sizeInWords); i \u0026gt;= 0;\n   *    i = bs.nextSetBit(i + 1, sizeInWords)) {\n   *    // operate on index i here\n   *  }\n   * \u003c/code\u003e\n   * \u003c/pre\u003e\n   *\n   * @param fromIndex the index to start checking from (inclusive)\n   * @param bitsetSizeInWords the size of the bitset, measured in 8-byte words\n   * @return the index of the next set bit, or -1 if there is no such bit\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [
			"org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow",
			"org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * 32-bit Murmur3 hasher.  This is based on Guava's Murmur3_32HashFunction.\n */",
		"name": "org.apache.spark.unsafe.hash.Murmur3_x86_32",
		"extends": "",
		"Methods": [
			{
				"signature": "public Murmur3_x86_32(int seed)",
				"documentation": "/**\n * 32-bit Murmur3 hasher.  This is based on Guava's Murmur3_32HashFunction.\n */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "public int hashInt(int input)",
				"documentation": ""
			},
			{
				"signature": "public static int hashInt(int input, int seed)",
				"documentation": ""
			},
			{
				"signature": "public int hashUnsafeWords(Object base, long offset, int lengthInBytes)",
				"documentation": ""
			},
			{
				"signature": "public static int hashUnsafeWords(Object base, long offset, int lengthInBytes, int seed)",
				"documentation": ""
			},
			{
				"signature": "public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes, int seed)",
				"documentation": ""
			},
			{
				"signature": "public static int hashUnsafeBytes2(Object base, long offset, int lengthInBytes, int seed)",
				"documentation": ""
			},
			{
				"signature": "private static int hashBytesByInt(Object base, long offset, int lengthInBytes, int seed)",
				"documentation": ""
			},
			{
				"signature": "public int hashLong(long input)",
				"documentation": ""
			},
			{
				"signature": "public static int hashLong(long input, int seed)",
				"documentation": ""
			},
			{
				"signature": "private static int mixK1(int k1)",
				"documentation": ""
			},
			{
				"signature": "private static int mixH1(int h1, int k1)",
				"documentation": ""
			},
			{
				"signature": "private static int fmix(int h1, int length)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [
			"org.apache.spark.unsafe.types.UTF8String",
			"org.apache.spark.unsafe.map.BytesToBytesMap",
			"org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A simple {@link MemoryAllocator} that can allocate up to 16GB using a JVM long primitive array.\n */",
		"name": "org.apache.spark.unsafe.memory.HeapMemoryAllocator",
		"extends": "",
		"Methods": [
			{
				"signature": "private boolean shouldPool(long size)",
				"documentation": "/**\n   * Returns true if allocations of the given size should go through the pooling mechanism and\n   * false otherwise.\n   */"
			},
			{
				"signature": "@Override\n  public MemoryBlock allocate(long size) throws OutOfMemoryError",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void free(MemoryBlock memory)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.unsafe.memory.MemoryAllocator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.unsafe.PlatformUtilSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.memory.MemoryAllocator",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.unsafe.memory.HeapMemoryAllocator",
			"org.apache.spark.unsafe.memory.UnsafeMemoryAllocator"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A consecutive block of memory, starting at a {@link MemoryLocation} with a fixed size.\n */",
		"name": "org.apache.spark.unsafe.memory.MemoryBlock",
		"extends": "org.apache.spark.unsafe.memory.MemoryLocation",
		"Methods": [
			{
				"signature": "public MemoryBlock(@Nullable Object obj, long offset, long length)",
				"documentation": "/**\n   * Optional page number; used when this MemoryBlock represents a page allocated by a\n   * TaskMemoryManager. This field is public so that it can be modified by the TaskMemoryManager,\n   * which lives in a different package.\n   */"
			},
			{
				"signature": "public long size()",
				"documentation": "/**\n   * Returns the size of the memory block.\n   */"
			},
			{
				"signature": "public static MemoryBlock fromLongArray(final long[] array)",
				"documentation": "/**\n   * Creates a memory block pointing to the memory used by the long array.\n   */"
			},
			{
				"signature": "public void fill(byte value)",
				"documentation": "/**\n   * Fills the memory block with the specified byte value.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A memory location. Tracked either by a memory address (with off-heap allocation),\n * or by an offset from a JVM object (on-heap allocation).\n */",
		"name": "org.apache.spark.unsafe.memory.MemoryLocation",
		"extends": "",
		"Methods": [
			{
				"signature": "public MemoryLocation(@Nullable Object obj, long offset)",
				"documentation": "/**\n * A memory location. Tracked either by a memory address (with off-heap allocation),\n * or by an offset from a JVM object (on-heap allocation).\n */"
			},
			{
				"signature": "public MemoryLocation()",
				"documentation": ""
			},
			{
				"signature": "public void setObjAndOffset(Object newObj, long newOffset)",
				"documentation": ""
			},
			{
				"signature": "public final Object getBaseObject()",
				"documentation": ""
			},
			{
				"signature": "public final long getBaseOffset()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.unsafe.memory.MemoryBlock"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A simple {@link MemoryAllocator} that uses {@code Unsafe} to allocate off-heap memory.\n */",
		"name": "org.apache.spark.unsafe.memory.UnsafeMemoryAllocator",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public MemoryBlock allocate(long size) throws OutOfMemoryError",
				"documentation": "/**\n * A simple {@link MemoryAllocator} that uses {@code Unsafe} to allocate off-heap memory.\n */"
			},
			{
				"signature": "@Override\n  public void free(MemoryBlock memory)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.unsafe.memory.MemoryAllocator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.types.ByteArray",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void writeToMemory(byte[] src, Object target, long targetOffset)",
				"documentation": "/**\n   * Writes the content of a byte array into a memory address, identified by an object and an\n   * offset. The target memory address must already been allocated, and have enough space to\n   * hold all the bytes in this string.\n   */"
			},
			{
				"signature": "public static long getPrefix(byte[] bytes)",
				"documentation": "/**\n   * Returns a 64-bit integer that can be used as the prefix used in sorting.\n   */"
			},
			{
				"signature": "static long getPrefix(Object base, long offset, int numBytes)",
				"documentation": ""
			},
			{
				"signature": "public static int compareBinary(byte[] leftBase, byte[] rightBase)",
				"documentation": ""
			},
			{
				"signature": "static int compareBinary(\n      Object leftBase,\n      long leftOffset,\n      int leftNumBytes,\n      Object rightBase,\n      long rightOffset,\n      int rightNumBytes)",
				"documentation": ""
			},
			{
				"signature": "public static byte[] subStringSQL(byte[] bytes, int pos, int len)",
				"documentation": ""
			},
			{
				"signature": "public static byte[] concat(byte[]... inputs)",
				"documentation": ""
			},
			{
				"signature": "private static byte[] padWithEmptyPattern(byte[] bytes, int len)",
				"documentation": ""
			},
			{
				"signature": "private static void fillWithPattern(byte[] result, int firstPos, int beyondPos, byte[] pad)",
				"documentation": ""
			},
			{
				"signature": "public static byte[] lpad(byte[] bytes, int len, byte[] pad)",
				"documentation": ""
			},
			{
				"signature": "public static byte[] rpad(byte[] bytes, int len, byte[] pad)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.BinaryPrefixComparator"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The class representing calendar intervals. The calendar interval is stored internally in\n * three components:\n * \u003cul\u003e\n *   \u003cli\u003ean integer value representing the number of `months` in this interval,\u003c/li\u003e\n *   \u003cli\u003ean integer value representing the number of `days` in this interval,\u003c/li\u003e\n *   \u003cli\u003ea long value representing the number of `microseconds` in this interval.\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * The `months` and `days` are not units of time with a constant length (unlike hours, seconds), so\n * they are two separated fields from microseconds. One month may be equal to 28, 29, 30 or 31 days\n * and one day may be equal to 23, 24 or 25 hours (daylight saving).\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.unsafe.types.CalendarInterval",
		"extends": "",
		"Methods": [
			{
				"signature": "public CalendarInterval(int months, int days, long microseconds)",
				"documentation": "/**\n * The class representing calendar intervals. The calendar interval is stored internally in\n * three components:\n * \u003cul\u003e\n *   \u003cli\u003ean integer value representing the number of `months` in this interval,\u003c/li\u003e\n *   \u003cli\u003ean integer value representing the number of `days` in this interval,\u003c/li\u003e\n *   \u003cli\u003ea long value representing the number of `microseconds` in this interval.\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * The `months` and `days` are not units of time with a constant length (unlike hours, seconds), so\n * they are two separated fields from microseconds. One month may be equal to 28, 29, 30 or 31 days\n * and one day may be equal to 23, 24 or 25 hours (daylight saving).\n *\n * @since 3.0.0\n */"
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "private void appendUnit(StringBuilder sb, long value, String unit)",
				"documentation": ""
			},
			{
				"signature": "public Period extractAsPeriod()",
				"documentation": "/**\n   * Extracts the date part of the interval.\n   * @return an instance of {@code java.time.Period} based on the months and days fields\n   *         of the given interval, not null.\n   */"
			},
			{
				"signature": "public Duration extractAsDuration()",
				"documentation": "/**\n   * Extracts the time part of the interval.\n   * @return an instance of {@code java.time.Duration} based on the microseconds field\n   *         of the given interval, not null.\n   * @throws ArithmeticException if a numeric overflow occurs\n   */"
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Unstable"
		],
		"usedBy": [
			"org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow",
			"org.apache.spark.sql.vectorized.ColumnVector"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A UTF-8 String for internal Spark use.\n * \u003cp\u003e\n * A String encoded in UTF-8 as an Array[Byte], which can be used for comparison,\n * search, see http://en.wikipedia.org/wiki/UTF-8 for details.\n * \u003cp\u003e\n * Note: This is not designed for general use cases, should not be used outside SQL.\n */",
		"name": "org.apache.spark.unsafe.types.UTF8String",
		"extends": "",
		"Methods": [
			{
				"signature": "public Object getBaseObject()",
				"documentation": "/**\n * A UTF-8 String for internal Spark use.\n * \u003cp\u003e\n * A String encoded in UTF-8 as an Array[Byte], which can be used for comparison,\n * search, see http://en.wikipedia.org/wiki/UTF-8 for details.\n * \u003cp\u003e\n * Note: This is not designed for general use cases, should not be used outside SQL.\n */"
			},
			{
				"signature": "public long getBaseOffset()",
				"documentation": ""
			},
			{
				"signature": "private static byte[] bytesOfCodePointInUTF8 =",
				"documentation": "/**\n   * A char in UTF-8 encoding can take 1-4 bytes depending on the first byte which\n   * indicates the size of the char. See Unicode standard in page 126, Table 3-6:\n   * http://www.unicode.org/versions/Unicode10.0.0/UnicodeStandard-10.0.pdf\n   *\n   * Binary    Hex          Comments\n   * 0xxxxxxx  0x00..0x7F   Only byte of a 1-byte character encoding\n   * 10xxxxxx  0x80..0xBF   Continuation bytes (1-3 continuation bytes)\n   * 110xxxxx  0xC0..0xDF   First byte of a 2-byte character encoding\n   * 1110xxxx  0xE0..0xEF   First byte of a 3-byte character encoding\n   * 11110xxx  0xF0..0xF7   First byte of a 4-byte character encoding\n   *\n   * As a consequence of the well-formedness conditions specified in\n   * Table 3-7 (page 126), the following byte values are disallowed in UTF-8:\n   *   C0–C1, F5–FF.\n   */"
			},
			{
				"signature": "public static UTF8String fromBytes(byte[] bytes)",
				"documentation": "/**\n   * Creates an UTF8String from byte array, which should be encoded in UTF-8.\n   *\n   * Note: `bytes` will be hold by returned UTF8String.\n   */"
			},
			{
				"signature": "public static UTF8String fromBytes(byte[] bytes, int offset, int numBytes)",
				"documentation": "/**\n   * Creates an UTF8String from byte array, which should be encoded in UTF-8.\n   *\n   * Note: `bytes` will be hold by returned UTF8String.\n   */"
			},
			{
				"signature": "public static UTF8String fromAddress(Object base, long offset, int numBytes)",
				"documentation": "/**\n   * Creates an UTF8String from given address (base and offset) and length.\n   */"
			},
			{
				"signature": "public static UTF8String fromString(String str)",
				"documentation": "/**\n   * Creates an UTF8String from String.\n   */"
			},
			{
				"signature": "public static UTF8String blankString(int length)",
				"documentation": "/**\n   * Creates an UTF8String that contains `length` spaces.\n   */"
			},
			{
				"signature": "protected UTF8String(Object base, long offset, int numBytes)",
				"documentation": ""
			},
			{
				"signature": "public UTF8String()",
				"documentation": ""
			},
			{
				"signature": "public void writeToMemory(Object target, long targetOffset)",
				"documentation": "/**\n   * Writes the content of this string into a memory address, identified by an object and an offset.\n   * The target memory address must already been allocated, and have enough space to hold all the\n   * bytes in this string.\n   */"
			},
			{
				"signature": "public void writeTo(ByteBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "@Nonnull\n  public ByteBuffer getByteBuffer()",
				"documentation": "/**\n   * Returns a {@link ByteBuffer} wrapping the base object if it is a byte array\n   * or a copy of the data if the base object is not a byte array.\n   *\n   * Unlike getBytes this will not create a copy the array if this is a slice.\n   */"
			},
			{
				"signature": "public void writeTo(OutputStream out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static int numBytesForFirstByte(final byte b)",
				"documentation": "/**\n   * Returns the number of bytes for a code point with the first byte as `b`\n   * @param b The first byte of a code point\n   */"
			},
			{
				"signature": "public int numBytes()",
				"documentation": "/**\n   * Returns the number of bytes\n   */"
			},
			{
				"signature": "public int numChars()",
				"documentation": "/**\n   * Returns the number of code points in it.\n   */"
			},
			{
				"signature": "public long getPrefix()",
				"documentation": "/**\n   * Returns a 64-bit integer that can be used as the prefix used in sorting.\n   */"
			},
			{
				"signature": "public byte[] getBytes()",
				"documentation": "/**\n   * Returns the underline bytes, will be a copy of it if it's part of another array.\n   */"
			},
			{
				"signature": "public UTF8String substring(final int start, final int until)",
				"documentation": "/**\n   * Returns a substring of this.\n   * @param start the position of first code point\n   * @param until the position after last code point, exclusive.\n   */"
			},
			{
				"signature": "public UTF8String substringSQL(int pos, int length)",
				"documentation": ""
			},
			{
				"signature": "public boolean contains(final UTF8String substring)",
				"documentation": "/**\n   * Returns whether this contains `substring` or not.\n   */"
			},
			{
				"signature": "private byte getByte(int i)",
				"documentation": "/**\n   * Returns the byte at position `i`.\n   */"
			},
			{
				"signature": "public boolean matchAt(final UTF8String s, int pos)",
				"documentation": ""
			},
			{
				"signature": "public boolean startsWith(final UTF8String prefix)",
				"documentation": ""
			},
			{
				"signature": "public boolean endsWith(final UTF8String suffix)",
				"documentation": ""
			},
			{
				"signature": "public UTF8String toUpperCase()",
				"documentation": "/**\n   * Returns the upper case of this string\n   */"
			},
			{
				"signature": "private UTF8String toUpperCaseSlow()",
				"documentation": ""
			},
			{
				"signature": "public UTF8String toLowerCase()",
				"documentation": "/**\n   * Returns the lower case of this string\n   */"
			},
			{
				"signature": "private UTF8String toLowerCaseSlow()",
				"documentation": ""
			},
			{
				"signature": "public UTF8String toTitleCase()",
				"documentation": "/**\n   * Returns the title case of this string, that could be used as title.\n   */"
			},
			{
				"signature": "private UTF8String toTitleCaseSlow()",
				"documentation": ""
			},
			{
				"signature": "public int findInSet(UTF8String match)",
				"documentation": ""
			},
			{
				"signature": "private UTF8String copyUTF8String(int start, int end)",
				"documentation": "/**\n   * Copy the bytes from the current UTF8String, and make a new UTF8String.\n   * @param start the start position of the current UTF8String in bytes.\n   * @param end the end position of the current UTF8String in bytes.\n   * @return a new UTF8String in the position of [start, end] of current UTF8String bytes.\n   */"
			},
			{
				"signature": "public UTF8String trim()",
				"documentation": "/**\n   * Trims space characters (ASCII 32) from both ends of this string.\n   *\n   * @return this string with no spaces at the start or end\n   */"
			},
			{
				"signature": "public UTF8String trimAll()",
				"documentation": "/**\n   * Trims whitespace ASCII characters from both ends of this string.\n   *\n   * Note that, this method is different from {@link UTF8String#trim()} which removes\n   * only spaces(= ASCII 32) from both ends.\n   *\n   * @return A UTF8String whose value is this UTF8String, with any leading and trailing white\n   * space removed, or this UTF8String if it has no leading or trailing whitespace.\n   *\n   */"
			},
			{
				"signature": "public UTF8String trim(UTF8String trimString)",
				"documentation": "/**\n   * Trims instances of the given trim string from both ends of this string.\n   *\n   * @param trimString the trim character string\n   * @return this string with no occurrences of the trim string at the start or end, or `null`\n   *  if `trimString` is `null`\n   */"
			},
			{
				"signature": "public UTF8String trimLeft()",
				"documentation": "/**\n   * Trims space characters (ASCII 32) from the start of this string.\n   *\n   * @return this string with no spaces at the start\n   */"
			},
			{
				"signature": "public UTF8String trimLeft(UTF8String trimString)",
				"documentation": "/**\n   * Trims instances of the given trim string from the start of this string.\n   *\n   * @param trimString the trim character string\n   * @return this string with no occurrences of the trim string at the start, or `null`\n   *  if `trimString` is `null`\n   */"
			},
			{
				"signature": "public UTF8String trimRight()",
				"documentation": "/**\n   * Trims space characters (ASCII 32) from the end of this string.\n   *\n   * @return this string with no spaces at the end\n   */"
			},
			{
				"signature": "public UTF8String trimTrailingSpaces(int numSpaces)",
				"documentation": "/**\n   * Trims at most `numSpaces` space characters (ASCII 32) from the end of this string.\n   */"
			},
			{
				"signature": "public UTF8String trimRight(UTF8String trimString)",
				"documentation": "/**\n   * Trims instances of the given trim string from the end of this string.\n   *\n   * @param trimString the trim character string\n   * @return this string with no occurrences of the trim string at the end, or `null`\n   *  if `trimString` is `null`\n   */"
			},
			{
				"signature": "public UTF8String reverse()",
				"documentation": ""
			},
			{
				"signature": "public UTF8String repeat(int times)",
				"documentation": ""
			},
			{
				"signature": "public int indexOf(UTF8String v, int start)",
				"documentation": "/**\n   * Returns the position of the first occurrence of substr in\n   * current string from the specified position (0-based index).\n   *\n   * @param v the string to be searched\n   * @param start the start position of the current string for searching\n   * @return the position of the first occurrence of substr, if not found, -1 returned.\n   */"
			},
			{
				"signature": "do",
				"documentation": ""
			},
			{
				"signature": "private int find(UTF8String str, int start)",
				"documentation": "/**\n   * Find the `str` from left to right.\n   */"
			},
			{
				"signature": "private int rfind(UTF8String str, int start)",
				"documentation": "/**\n   * Find the `str` from right to left.\n   */"
			},
			{
				"signature": "public UTF8String subStringIndex(UTF8String delim, int count)",
				"documentation": "/**\n   * Returns the substring from string str before count occurrences of the delimiter delim.\n   * If count is positive, everything the left of the final delimiter (counting from left) is\n   * returned. If count is negative, every to the right of the final delimiter (counting from the\n   * right) is returned. subStringIndex performs a case-sensitive match when searching for delim.\n   */"
			},
			{
				"signature": "public UTF8String rpad(int len, UTF8String pad)",
				"documentation": "/**\n   * Returns str, right-padded with pad to a length of len\n   * For example:\n   *   ('hi', 5, '??') =\u0026gt; 'hi???'\n   *   ('hi', 1, '??') =\u0026gt; 'h'\n   */"
			},
			{
				"signature": "public UTF8String lpad(int len, UTF8String pad)",
				"documentation": "/**\n   * Returns str, left-padded with pad to a length of len.\n   * For example:\n   *   ('hi', 5, '??') =\u0026gt; '???hi'\n   *   ('hi', 1, '??') =\u0026gt; 'h'\n   */"
			},
			{
				"signature": "public static UTF8String concat(UTF8String... inputs)",
				"documentation": "/**\n   * Concatenates input strings together into a single string. Returns null if any input is null.\n   */"
			},
			{
				"signature": "public static UTF8String concatWs(UTF8String separator, UTF8String... inputs)",
				"documentation": "/**\n   * Concatenates input strings together into a single string using the separator.\n   * A null input is skipped. For example, concat(\",\", \"a\", null, \"c\") would yield \"a,c\".\n   */"
			},
			{
				"signature": "public UTF8String[] split(UTF8String pattern, int limit)",
				"documentation": ""
			},
			{
				"signature": "public UTF8String[] splitSQL(UTF8String delimiter, int limit)",
				"documentation": ""
			},
			{
				"signature": "private UTF8String[] split(String delimiter, int limit)",
				"documentation": ""
			},
			{
				"signature": "public UTF8String replace(UTF8String search, UTF8String replace)",
				"documentation": ""
			},
			{
				"signature": "public UTF8String translate(Map\u003cString, String\u003e dict)",
				"documentation": ""
			},
			{
				"signature": "public boolean toLong(LongWrapper toLongResult)",
				"documentation": "/**\n   * Parses this UTF8String(trimmed if needed) to long.\n   *\n   * Note that, in this method we accumulate the result in negative format, and convert it to\n   * positive format at the end, if this string is not started with '-'. This is because min value\n   * is bigger than max value in digits, e.g. Long.MAX_VALUE is '9223372036854775807' and\n   * Long.MIN_VALUE is '-9223372036854775808'.\n   *\n   * This code is mostly copied from LazyLong.parseLong in Hive.\n   *\n   * @param toLongResult If a valid `long` was parsed from this UTF8String, then its value would\n   *                     be set in `toLongResult`\n   * @return true if the parsing was successful else false\n   */"
			},
			{
				"signature": "private boolean toLong(LongWrapper toLongResult, boolean allowDecimal)",
				"documentation": ""
			},
			{
				"signature": "public boolean toInt(IntWrapper intWrapper)",
				"documentation": "/**\n   * Parses this UTF8String(trimmed if needed) to int.\n   *\n   * Note that, in this method we accumulate the result in negative format, and convert it to\n   * positive format at the end, if this string is not started with '-'. This is because min value\n   * is bigger than max value in digits, e.g. Integer.MAX_VALUE is '2147483647' and\n   * Integer.MIN_VALUE is '-2147483648'.\n   *\n   * This code is mostly copied from LazyInt.parseInt in Hive.\n   *\n   * Note that, this method is almost same as `toLong`, but we leave it duplicated for performance\n   * reasons, like Hive does.\n   *\n   * @param intWrapper If a valid `int` was parsed from this UTF8String, then its value would\n   *                    be set in `intWrapper`\n   * @return true if the parsing was successful else false\n   */"
			},
			{
				"signature": "private boolean toInt(IntWrapper intWrapper, boolean allowDecimal)",
				"documentation": ""
			},
			{
				"signature": "public boolean toShort(IntWrapper intWrapper)",
				"documentation": ""
			},
			{
				"signature": "public boolean toByte(IntWrapper intWrapper)",
				"documentation": ""
			},
			{
				"signature": "public long toLongExact()",
				"documentation": "/**\n   * Parses UTF8String(trimmed if needed) to long. This method is used when ANSI is enabled.\n   *\n   * @return If string contains valid numeric value then it returns the long value otherwise a\n   * NumberFormatException  is thrown.\n   */"
			},
			{
				"signature": "public int toIntExact()",
				"documentation": "/**\n   * Parses UTF8String(trimmed if needed) to int. This method is used when ANSI is enabled.\n   *\n   * @return If string contains valid numeric value then it returns the int value otherwise a\n   * NumberFormatException  is thrown.\n   */"
			},
			{
				"signature": "public short toShortExact()",
				"documentation": ""
			},
			{
				"signature": "public byte toByteExact()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String clone()",
				"documentation": ""
			},
			{
				"signature": "public UTF8String copy()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int compareTo(@Nonnull final UTF8String other)",
				"documentation": ""
			},
			{
				"signature": "public int compare(final UTF8String other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(final Object other)",
				"documentation": ""
			},
			{
				"signature": "public int levenshteinDistance(UTF8String other)",
				"documentation": "/**\n   * Levenshtein distance is a metric for measuring the distance of two strings. The distance is\n   * defined by the minimum number of single-character edits (i.e. insertions, deletions or\n   * substitutions) that are required to change one of the strings into the other.\n   */"
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "private static final byte[] US_ENGLISH_MAPPING =",
				"documentation": "/**\n   * Soundex mapping table\n   */"
			},
			{
				"signature": "public UTF8String soundex()",
				"documentation": "/**\n   * Encodes a string into a Soundex value. Soundex is an encoding used to relate similar names,\n   * but can also be used as a general purpose scheme to find word with similar phonemes.\n   * https://en.wikipedia.org/wiki/Soundex\n   */"
			},
			{
				"signature": "byte[] sx =",
				"documentation": ""
			},
			{
				"signature": "public void writeExternal(ObjectOutput out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(Kryo kryo, Output out)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void read(Kryo kryo, Input in)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Comparable",
			"Externalizable",
			"com.esotericsoftware.kryo.KryoSerializable",
			"Cloneable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UTF8StringBuilder",
			"org.apache.spark.unsafe.array.ByteArrayMethods",
			"org.apache.spark.unsafe.hash.Murmur3_x86_32",
			"org.apache.spark.unsafe.types.UTF8String.LongWrapper",
			"org.apache.spark.unsafe.types.UTF8String.IntWrapper"
		],
		"usedBy": [
			"org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow",
			"org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils",
			"org.apache.spark.sql.vectorized.ArrowColumnVector",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.StringAccessor",
			"org.apache.spark.sql.catalyst.expressions.HiveHasherSuite",
			"org.apache.spark.sql.execution.datasources.orc.OrcAtomicColumnVector",
			"org.apache.spark.sql.execution.vectorized.OffHeapColumnVector",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector",
			"org.apache.spark.sql.execution.vectorized.WritableColumnVector",
			"org.apache.hive.service.cli.ColumnValue"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.unsafe.types.UTF8String.LongWrapper",
			"org.apache.spark.unsafe.types.UTF8String.IntWrapper"
		]
	},
	{
		"documentation": "/**\n   * Wrapper over `long` to allow result of parsing long from string to be accessed via reference.\n   * This is done solely for better performance and is not expected to be used by end users.\n   */",
		"name": "org.apache.spark.unsafe.types.UTF8String.LongWrapper",
		"extends": "",
		"Methods": [],
		"interfaces": [
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Wrapper over `int` to allow result of parsing integer from string to be accessed via reference.\n   * This is done solely for better performance and is not expected to be used by end users.\n   *\n   * {@link LongWrapper} could have been used here but using `int` directly save the extra cost of\n   * conversion from `long` to `int`\n   */",
		"name": "org.apache.spark.unsafe.types.UTF8String.IntWrapper",
		"extends": "",
		"Methods": [],
		"interfaces": [
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.PlatformUtilSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void overlappingCopyMemory()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void onHeapMemoryAllocatorPoolingReUsesLongArrays()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void freeingOnHeapMemoryBlockResetsBaseObjectAndOffset()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void freeingOffHeapMemoryBlockResetsOffset()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void onHeapMemoryAllocatorThrowsAssertionErrorOnDoubleFree()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void offHeapMemoryAllocatorThrowsAssertionErrorOnDoubleFree()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void memoryDebugFillEnabledInTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void heapMemoryReuse()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.memory.HeapMemoryAllocator"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.array.ByteArraySuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private long getPrefixByByte(byte[] bytes)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGetPrefix()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCompareBinary()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.array.LongArraySuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void basicTest()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test file based on Guava's Murmur3Hash32Test.\n */",
		"name": "org.apache.spark.unsafe.hash.Murmur3_x86_32Suite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testKnownIntegerInputs()",
				"documentation": "/**\n * Test file based on Guava's Murmur3Hash32Test.\n */"
			},
			{
				"signature": "@Test\n  public void testKnownLongInputs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKnownBytesInputs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTestBytes()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTestPaddedStrings()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.types.CalendarIntervalSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void equalsTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void toStringTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void periodAndDurationTest()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.types.UTF8StringSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private static void checkBasic(String str, int len)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void basicTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void emptyStringTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void prefix()",
				"documentation": ""
			},
			{
				"signature": "byte[] buf1 =",
				"documentation": ""
			},
			{
				"signature": "byte[] buf2 =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void compareTo()",
				"documentation": ""
			},
			{
				"signature": "protected static void testUpperandLower(String upper, String lower)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void upperAndLower()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void titleCase()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void concatTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void concatWsTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void contains()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void startsWith()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void endsWith()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void substring()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void trims()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void indexOf()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void substring_index()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void reverse()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void repeat()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void pad()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void substringSQL()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void split()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void replace()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void levenshteinDistance()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void translate()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void createBlankString()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void findInSet()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void soundex()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeToOutputStreamUnderflow() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeToOutputStreamSlice() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeToOutputStreamOverflow() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeToOutputStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeToOutputStreamIntArray() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testToShort() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testToByte() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testToInt() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testToLong() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void trimBothWithTrimString()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void trimLeftWithTrimString()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void trimRightWithTrimString()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void skipWrongFirstByte()",
				"documentation": ""
			},
			{
				"signature": "int[] wrongFirstBytes =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.avro.SparkAvroKeyOutputFormat",
		"extends": "org.apache.avro.mapreduce.AvroKeyOutputFormat",
		"Methods": [
			{
				"signature": "SparkAvroKeyOutputFormat(Map\u003cString, String\u003e metadata)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.avro.SparkAvroKeyOutputFormat.SparkRecordWriterFactory"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.sql.avro.SparkAvroKeyOutputFormat.SparkRecordWriterFactory"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.avro.SparkAvroKeyOutputFormat.SparkRecordWriterFactory",
		"extends": "RecordWriterFactory",
		"Methods": [
			{
				"signature": "SparkRecordWriterFactory(Map\u003cString, String\u003e metadata)",
				"documentation": ""
			},
			{
				"signature": "protected RecordWriter\u003cAvroKey\u003cGenericRecord\u003e, NullWritable\u003e create(\n        Schema writerSchema,\n        GenericData dataModel,\n        CodecFactory compressionCodec,\n        OutputStream outputStream,\n        int syncInterval) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.avro.SparkAvroKeyOutputFormat"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.avro.SparkAvroKeyRecordWriter",
		"extends": "org.apache.hadoop.mapreduce.RecordWriter",
		"Methods": [
			{
				"signature": "SparkAvroKeyRecordWriter(\n      Schema writerSchema,\n      GenericData dataModel,\n      CodecFactory compressionCodec,\n      OutputStream outputStream,\n      int syncInterval,\n      Map\u003cString, String\u003e metadata) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void write(AvroKey\u003cT\u003e record, NullWritable ignore) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void close(TaskAttemptContext context) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public long sync() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.mapreduce.Syncable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.avro.JavaAvroFunctionsSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testToAvroFromAvro()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kafka010.JavaConsumerStrategySuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testConsumerStrategyConstructors()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKafkaStream() throws InterruptedException",
				"documentation": ""
			},
			{
				"signature": "private  String[] createTopicAndSendData(String topic)",
				"documentation": ""
			},
			{
				"signature": "String[] data =",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKafkaRDD() throws InterruptedException",
				"documentation": ""
			},
			{
				"signature": "OffsetRange[] offsetRanges =",
				"documentation": ""
			},
			{
				"signature": "private  String[] createTopicAndSendData(String topic)",
				"documentation": ""
			},
			{
				"signature": "String[] data =",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kafka010.JavaLocationStrategySuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testLocationStrategyConstructors()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Consumes messages from a Amazon Kinesis streams and does wordcount.\n *\n * This example spins up 1 Kinesis Receiver per shard for the given stream.\n * It then starts pulling from the last checkpointed sequence number of the given stream.\n *\n * Usage: JavaKinesisWordCountASL [app-name] [stream-name] [endpoint-url] [region-name]\n *   [app-name] is the name of the consumer app, used to track the read data in DynamoDB\n *   [stream-name] name of the Kinesis stream (i.e. mySparkStream)\n *   [endpoint-url] endpoint of the Kinesis service\n *     (e.g. https://kinesis.us-east-1.amazonaws.com)\n *\n *\n * Example:\n *      # export AWS keys if necessary\n *      $ export AWS_ACCESS_KEY_ID=[your-access-key]\n *      $ export AWS_SECRET_ACCESS_KEY=\u003cyour-secret-key\u003e\n *\n *      # run the example\n *      $ SPARK_HOME/bin/run-example   streaming.JavaKinesisWordCountASL myAppName  mySparkStream \\\n *             https://kinesis.us-east-1.amazonaws.com\n *\n * There is a companion helper class called KinesisWordProducerASL which puts dummy data\n * onto the Kinesis stream.\n *\n * This code uses the DefaultAWSCredentialsProviderChain to find credentials\n * in the following order:\n *    Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n *    Java System Properties - aws.accessKeyId and aws.secretKey\n *    Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n *    Instance profile credentials - delivered through the Amazon EC2 metadata service\n * For more information, see\n * https://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html\n *\n * See https://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more details on\n * the Kinesis Spark Streaming integration.\n */",
		"name": "org.apache.spark.examples.streaming.JavaKinesisWordCountASL",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Consumes messages from a Amazon Kinesis streams and does wordcount.\n *\n * This example spins up 1 Kinesis Receiver per shard for the given stream.\n * It then starts pulling from the last checkpointed sequence number of the given stream.\n *\n * Usage: JavaKinesisWordCountASL [app-name] [stream-name] [endpoint-url] [region-name]\n *   [app-name] is the name of the consumer app, used to track the read data in DynamoDB\n *   [stream-name] name of the Kinesis stream (i.e. mySparkStream)\n *   [endpoint-url] endpoint of the Kinesis service\n *     (e.g. https://kinesis.us-east-1.amazonaws.com)\n *\n *\n * Example:\n *      # export AWS keys if necessary\n *      $ export AWS_ACCESS_KEY_ID=[your-access-key]\n *      $ export AWS_SECRET_ACCESS_KEY=\u003cyour-secret-key\u003e\n *\n *      # run the example\n *      $ SPARK_HOME/bin/run-example   streaming.JavaKinesisWordCountASL myAppName  mySparkStream \\\n *             https://kinesis.us-east-1.amazonaws.com\n *\n * There is a companion helper class called KinesisWordProducerASL which puts dummy data\n * onto the Kinesis stream.\n *\n * This code uses the DefaultAWSCredentialsProviderChain to find credentials\n * in the following order:\n *    Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n *    Java System Properties - aws.accessKeyId and aws.secretKey\n *    Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n *    Instance profile credentials - delivered through the Amazon EC2 metadata service\n * For more information, see\n * https://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html\n *\n * See https://spark.apache.org/docs/latest/streaming-kinesis-integration.html for more details on\n * the Kinesis Spark Streaming integration.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.api.java.function.FlatMapFunction",
			"org.apache.spark.api.java.function.Function2",
			"org.apache.spark.api.java.function.PairFunction"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A java wrapper for exposing [[InitialPositionInStream]]\n * to the corresponding Kinesis readers.\n */",
		"name": "org.apache.spark.streaming.kinesis.KinesisInitialPosition",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest",
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.TrimHorizon",
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.AtTimestamp"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kinesis.KinesisInitialPositions",
		"extends": "",
		"Methods": [
			{
				"signature": "public static KinesisInitialPosition fromKinesisInitialPosition(\n            InitialPositionInStream initialPositionInStream) throws UnsupportedOperationException",
				"documentation": "/**\n     * Returns instance of [[KinesisInitialPosition]] based on the passed\n     * [[InitialPositionInStream]]. This method is used in KinesisUtils for translating the\n     * InitialPositionInStream to InitialPosition. This function would be removed when we deprecate\n     * the KinesisUtils.\n     *\n     * @return [[InitialPosition]]\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest",
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.TrimHorizon",
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.AtTimestamp"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest",
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.TrimHorizon",
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.AtTimestamp"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest",
		"extends": "",
		"Methods": [
			{
				"signature": "public Latest()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public InitialPositionInStream getPosition()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPosition",
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kinesis.KinesisInitialPositions.TrimHorizon",
		"extends": "",
		"Methods": [
			{
				"signature": "public TrimHorizon()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public InitialPositionInStream getPosition()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPosition",
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions",
			"org.apache.spark.streaming.kinesis.JavaKinesisInputDStreamBuilderSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kinesis.KinesisInitialPositions.AtTimestamp",
		"extends": "",
		"Methods": [
			{
				"signature": "public AtTimestamp(Date timestamp)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public InitialPositionInStream getPosition()",
				"documentation": ""
			},
			{
				"signature": "public Date getTimestamp()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPosition",
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.kinesis.JavaKinesisInputDStreamBuilderSuite",
		"extends": "org.apache.spark.streaming.LocalJavaStreamingContext",
		"Methods": [
			{
				"signature": "@Test\n  public void testJavaKinesisDStreamBuilder()",
				"documentation": "/**\n   * Basic test to ensure that the KinesisDStream.Builder interface is accessible from Java.\n   */"
			},
			{
				"signature": "@Test\n  public void testJavaKinesisDStreamBuilderOldApi()",
				"documentation": "/**\n   * Test to ensure that the old API for InitialPositionInStream\n   * is supported in KinesisDStream.Builder.\n   * This test would be removed when we deprecate the KinesisUtils.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.streaming.kinesis.KinesisInitialPositions.TrimHorizon"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A reporter which announces metric values to a Ganglia cluster.\n *\n * @see \u003ca href=\"http://ganglia.sourceforge.net/\"\u003eGanglia Monitoring System\u003c/a\u003e\n */",
		"name": "com.codahale.metrics.ganglia.GangliaReporter",
		"extends": "ScheduledReporter",
		"Methods": [
			{
				"signature": "public static Builder forRegistry(MetricRegistry registry)",
				"documentation": "/**\n     * Returns a new {@link Builder} for {@link GangliaReporter}.\n     *\n     * @param registry the registry to report\n     * @return a {@link Builder} instance for a {@link GangliaReporter}\n     */"
			},
			{
				"signature": "private GangliaReporter(MetricRegistry registry,\n                            GMetric gmetric,\n                            GMetric[] gmetrics,\n                            String prefix,\n                            int tMax,\n                            int dMax,\n                            TimeUnit rateUnit,\n                            TimeUnit durationUnit,\n                            MetricFilter filter,\n                            ScheduledExecutorService executor,\n                            boolean shutdownExecutorOnStop,\n                            Set\u003cMetricAttribute\u003e disabledMetricAttributes)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void report(SortedMap\u003cString, Gauge\u003e gauges,\n                       SortedMap\u003cString, Counter\u003e counters,\n                       SortedMap\u003cString, Histogram\u003e histograms,\n                       SortedMap\u003cString, Meter\u003e meters,\n                       SortedMap\u003cString, Timer\u003e timers)",
				"documentation": ""
			},
			{
				"signature": "private void reportTimer(String name, Timer timer)",
				"documentation": ""
			},
			{
				"signature": "private void reportMeter(String name, Meter meter)",
				"documentation": ""
			},
			{
				"signature": "private void reportMetered(String name, Metered meter, String group, String eventName) throws GangliaException",
				"documentation": ""
			},
			{
				"signature": "private void reportHistogram(String name, Histogram histogram)",
				"documentation": ""
			},
			{
				"signature": "private void reportCounter(String name, Counter counter)",
				"documentation": ""
			},
			{
				"signature": "private void reportGauge(String name, Gauge gauge)",
				"documentation": ""
			},
			{
				"signature": "private void announceIfEnabled(MetricAttribute metricAttribute, String metricName, String group, double value, String units)\n            throws GangliaException",
				"documentation": ""
			},
			{
				"signature": "private void announceIfEnabled(MetricAttribute metricAttribute, String metricName, String group, long value, String units)\n            throws GangliaException",
				"documentation": ""
			},
			{
				"signature": "private void announce(String name, String group, String value, GMetricType type, String units)\n            throws GangliaException",
				"documentation": ""
			},
			{
				"signature": "private GMetricType detectType(Object o)",
				"documentation": ""
			},
			{
				"signature": "private String group(String name)",
				"documentation": ""
			},
			{
				"signature": "private String prefix(String name, String n)",
				"documentation": ""
			},
			{
				"signature": "private String escapeSlashes(String name)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"com.codahale.metrics.ganglia.GangliaReporter.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"com.codahale.metrics.ganglia.GangliaReporter.Builder"
		]
	},
	{
		"documentation": "/**\n     * A builder for {@link GangliaReporter} instances. Defaults to using a {@code tmax} of {@code 60},\n     * a {@code dmax} of {@code 0}, converting rates to events/second, converting durations to\n     * milliseconds, and not filtering metrics.\n     */",
		"name": "com.codahale.metrics.ganglia.GangliaReporter.Builder",
		"extends": "",
		"Methods": [
			{
				"signature": "private Builder(MetricRegistry registry)",
				"documentation": "/**\n     * A builder for {@link GangliaReporter} instances. Defaults to using a {@code tmax} of {@code 60},\n     * a {@code dmax} of {@code 0}, converting rates to events/second, converting durations to\n     * milliseconds, and not filtering metrics.\n     */"
			},
			{
				"signature": "public Builder shutdownExecutorOnStop(boolean shutdownExecutorOnStop)",
				"documentation": "/**\n         * Specifies whether or not, the executor (used for reporting) will be stopped with same time with reporter.\n         * Default value is true.\n         * Setting this parameter to false, has the sense in combining with providing external managed executor via {@link #scheduleOn(ScheduledExecutorService)}.\n         *\n         * @param shutdownExecutorOnStop if true, then executor will be stopped in same time with this reporter\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public Builder scheduleOn(ScheduledExecutorService executor)",
				"documentation": "/**\n         * Specifies the executor to use while scheduling reporting of metrics.\n         * Default value is null.\n         * Null value leads to executor will be auto created on start.\n         *\n         * @param executor the executor to use while scheduling reporting of metrics.\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public Builder withTMax(int tMax)",
				"documentation": "/**\n         * Use the given {@code tmax} value when announcing metrics.\n         *\n         * @param tMax the desired gmond {@code tmax} value\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public Builder prefixedWith(String prefix)",
				"documentation": "/**\n         * Prefix all metric names with the given string.\n         *\n         * @param prefix the prefix for all metric names\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public Builder withDMax(int dMax)",
				"documentation": "/**\n         * Use the given {@code dmax} value when announcing metrics.\n         *\n         * @param dMax the desired gmond {@code dmax} value\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public Builder convertRatesTo(TimeUnit rateUnit)",
				"documentation": "/**\n         * Convert rates to the given time unit.\n         *\n         * @param rateUnit a unit of time\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public Builder convertDurationsTo(TimeUnit durationUnit)",
				"documentation": "/**\n         * Convert durations to the given time unit.\n         *\n         * @param durationUnit a unit of time\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public Builder filter(MetricFilter filter)",
				"documentation": "/**\n         * Only report metrics which match the given filter.\n         *\n         * @param filter a {@link MetricFilter}\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public Builder disabledMetricAttributes(Set\u003cMetricAttribute\u003e disabledMetricAttributes)",
				"documentation": "/**\n         * Don't report the passed metric attributes for all metrics (e.g. \"p999\", \"stddev\" or \"m15\").\n         * See {@link MetricAttribute}.\n         *\n         * @param disabledMetricAttributes a {@link MetricFilter}\n         * @return {@code this}\n         */"
			},
			{
				"signature": "public GangliaReporter build(GMetric gmetric)",
				"documentation": "/**\n         * Builds a {@link GangliaReporter} with the given properties, announcing metrics to the\n         * given {@link GMetric} client.\n         *\n         * @param gmetric the client to use for announcing metrics\n         * @return a {@link GangliaReporter}\n         */"
			},
			{
				"signature": "public GangliaReporter build(GMetric... gmetrics)",
				"documentation": "/**\n         * Builds a {@link GangliaReporter} with the given properties, announcing metrics to the\n         * given {@link GMetric} client.\n         *\n         * @param gmetrics the clients to use for announcing metrics\n         * @return a {@link GangliaReporter}\n         */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"com.codahale.metrics.ganglia.GangliaReporter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.JobExecutionStatus",
		"extends": "",
		"Methods": [
			{
				"signature": "public static JobExecutionStatus fromString(String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.EnumUtil"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Exposes information about Spark Executors.\n *\n * This interface is not designed to be implemented outside of Spark.  We may add additional methods\n * which may break binary compatibility with outside implementations.\n */",
		"name": "org.apache.spark.SparkExecutorInfo",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Class that allows users to receive all SparkListener events.\n * Users should override the onEvent method.\n *\n * This is a concrete Java class in order to ensure that we don't forget to update it when adding\n * new methods to SparkListener: forgetting to add a method will result in a compilation error (if\n * this was a concrete Scala class, default implementations of new event handlers would be inherited\n * from the SparkListener trait).\n *\n * Please note until Spark 3.1.0 this was missing the DevelopApi annotation, this needs to be\n * taken into account if changing this API before a major release.\n */",
		"name": "org.apache.spark.SparkFirehoseListener",
		"extends": "",
		"Methods": [
			{
				"signature": "public void onEvent(SparkListenerEvent event)",
				"documentation": "/**\n * Class that allows users to receive all SparkListener events.\n * Users should override the onEvent method.\n *\n * This is a concrete Java class in order to ensure that we don't forget to update it when adding\n * new methods to SparkListener: forgetting to add a method will result in a compilation error (if\n * this was a concrete Scala class, default implementations of new event handlers would be inherited\n * from the SparkListener trait).\n *\n * Please note until Spark 3.1.0 this was missing the DevelopApi annotation, this needs to be\n * taken into account if changing this API before a major release.\n */"
			},
			{
				"signature": "@Override\n  public final void onStageCompleted(SparkListenerStageCompleted stageCompleted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onStageSubmitted(SparkListenerStageSubmitted stageSubmitted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onTaskStart(SparkListenerTaskStart taskStart)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onTaskGettingResult(SparkListenerTaskGettingResult taskGettingResult)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onTaskEnd(SparkListenerTaskEnd taskEnd)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onJobStart(SparkListenerJobStart jobStart)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onJobEnd(SparkListenerJobEnd jobEnd)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onEnvironmentUpdate(SparkListenerEnvironmentUpdate environmentUpdate)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onBlockManagerAdded(SparkListenerBlockManagerAdded blockManagerAdded)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onBlockManagerRemoved(SparkListenerBlockManagerRemoved blockManagerRemoved)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onUnpersistRDD(SparkListenerUnpersistRDD unpersistRDD)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onApplicationStart(SparkListenerApplicationStart applicationStart)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onApplicationEnd(SparkListenerApplicationEnd applicationEnd)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onExecutorMetricsUpdate(\n      SparkListenerExecutorMetricsUpdate executorMetricsUpdate)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onStageExecutorMetrics(\n      SparkListenerStageExecutorMetrics executorMetrics)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onExecutorAdded(SparkListenerExecutorAdded executorAdded)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onExecutorRemoved(SparkListenerExecutorRemoved executorRemoved)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onExecutorBlacklisted(SparkListenerExecutorBlacklisted executorBlacklisted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onExecutorExcluded(SparkListenerExecutorExcluded executorExcluded)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onExecutorBlacklistedForStage(\n      SparkListenerExecutorBlacklistedForStage executorBlacklistedForStage)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onExecutorExcludedForStage(\n      SparkListenerExecutorExcludedForStage executorExcludedForStage)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onNodeBlacklistedForStage(\n      SparkListenerNodeBlacklistedForStage nodeBlacklistedForStage)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onNodeExcludedForStage(\n      SparkListenerNodeExcludedForStage nodeExcludedForStage)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onExecutorUnblacklisted(\n      SparkListenerExecutorUnblacklisted executorUnblacklisted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onExecutorUnexcluded(\n      SparkListenerExecutorUnexcluded executorUnexcluded)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onNodeBlacklisted(SparkListenerNodeBlacklisted nodeBlacklisted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onNodeExcluded(SparkListenerNodeExcluded nodeExcluded)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onNodeUnblacklisted(SparkListenerNodeUnblacklisted nodeUnblacklisted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void onNodeUnexcluded(SparkListenerNodeUnexcluded nodeUnexcluded)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onBlockUpdated(SparkListenerBlockUpdated blockUpdated)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onSpeculativeTaskSubmitted(SparkListenerSpeculativeTaskSubmitted speculativeTask)",
				"documentation": ""
			},
			{
				"signature": "public void onUnschedulableTaskSetAdded(\n      SparkListenerUnschedulableTaskSetAdded unschedulableTaskSetAdded)",
				"documentation": ""
			},
			{
				"signature": "public void onUnschedulableTaskSetRemoved(\n      SparkListenerUnschedulableTaskSetRemoved unschedulableTaskSetRemoved)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onResourceProfileAdded(SparkListenerResourceProfileAdded event)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onOtherEvent(SparkListenerEvent event)",
				"documentation": ""
			}
		],
		"interfaces": [
			"SparkListenerInterface"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Exposes information about Spark Jobs.\n *\n * This interface is not designed to be implemented outside of Spark.  We may add additional methods\n * which may break binary compatibility with outside implementations.\n */",
		"name": "org.apache.spark.SparkJobInfo",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Exposes information about Spark Stages.\n *\n * This interface is not designed to be implemented outside of Spark.  We may add additional methods\n * which may break binary compatibility with outside implementations.\n */",
		"name": "org.apache.spark.SparkStageInfo",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface mixed into Throwables thrown from Spark.\n *\n * - For backwards compatibility, existing Throwable types can be thrown with an arbitrary error\n *   message with a null error class. See [[SparkException]].\n * - To promote standardization, Throwables should be thrown with an error class and message\n *   parameters to construct an error message with SparkThrowableHelper.getMessage(). New Throwable\n *   types should not accept arbitrary error messages. See [[SparkArithmeticException]].\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.SparkThrowable",
		"extends": "",
		"Methods": [
			{
				"signature": "default String getSqlState()",
				"documentation": "/**\n * Interface mixed into Throwables thrown from Spark.\n *\n * - For backwards compatibility, existing Throwable types can be thrown with an arbitrary error\n *   message with a null error class. See [[SparkException]].\n * - To promote standardization, Throwables should be thrown with an error class and message\n *   parameters to construct an error message with SparkThrowableHelper.getMessage(). New Throwable\n *   types should not accept arbitrary error messages. See [[SparkArithmeticException]].\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "default boolean isInternalError()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.api.java.JavaFutureAction",
		"extends": "java.util.concurrent.Future",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * \u003cp\u003eLike {@code java.util.Optional} in Java 8, {@code scala.Option} in Scala, and\n * {@code com.google.common.base.Optional} in Google Guava, this class represents a\n * value of a given type that may or may not exist. It is used in methods that wish\n * to optionally return a value, in preference to returning {@code null}.\u003c/p\u003e\n *\n * \u003cp\u003eIn fact, the class here is a reimplementation of the essential API of both\n * {@code java.util.Optional} and {@code com.google.common.base.Optional}. From\n * {@code java.util.Optional}, it implements:\u003c/p\u003e\n *\n * \u003cul\u003e\n *   \u003cli\u003e{@link #empty()}\u003c/li\u003e\n *   \u003cli\u003e{@link #of(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #ofNullable(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #get()}\u003c/li\u003e\n *   \u003cli\u003e{@link #orElse(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #isPresent()}\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * \u003cp\u003eFrom {@code com.google.common.base.Optional} it implements:\u003c/p\u003e\n *\n * \u003cul\u003e\n *   \u003cli\u003e{@link #absent()}\u003c/li\u003e\n *   \u003cli\u003e{@link #of(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #fromNullable(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #get()}\u003c/li\u003e\n *   \u003cli\u003e{@link #or(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #orNull()}\u003c/li\u003e\n *   \u003cli\u003e{@link #isPresent()}\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * \u003cp\u003e{@code java.util.Optional} itself was not used because at the time, the\n * project did not require Java 8. Using {@code com.google.common.base.Optional}\n * has in the past caused serious library version conflicts with Guava that can't\n * be resolved by shading. Hence this work-alike clone.\u003c/p\u003e\n *\n * @param \u003cT\u003e type of value held inside\n */",
		"name": "org.apache.spark.api.java.Optional",
		"extends": "",
		"Methods": [
			{
				"signature": "private Optional()",
				"documentation": "/**\n * \u003cp\u003eLike {@code java.util.Optional} in Java 8, {@code scala.Option} in Scala, and\n * {@code com.google.common.base.Optional} in Google Guava, this class represents a\n * value of a given type that may or may not exist. It is used in methods that wish\n * to optionally return a value, in preference to returning {@code null}.\u003c/p\u003e\n *\n * \u003cp\u003eIn fact, the class here is a reimplementation of the essential API of both\n * {@code java.util.Optional} and {@code com.google.common.base.Optional}. From\n * {@code java.util.Optional}, it implements:\u003c/p\u003e\n *\n * \u003cul\u003e\n *   \u003cli\u003e{@link #empty()}\u003c/li\u003e\n *   \u003cli\u003e{@link #of(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #ofNullable(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #get()}\u003c/li\u003e\n *   \u003cli\u003e{@link #orElse(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #isPresent()}\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * \u003cp\u003eFrom {@code com.google.common.base.Optional} it implements:\u003c/p\u003e\n *\n * \u003cul\u003e\n *   \u003cli\u003e{@link #absent()}\u003c/li\u003e\n *   \u003cli\u003e{@link #of(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #fromNullable(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #get()}\u003c/li\u003e\n *   \u003cli\u003e{@link #or(Object)}\u003c/li\u003e\n *   \u003cli\u003e{@link #orNull()}\u003c/li\u003e\n *   \u003cli\u003e{@link #isPresent()}\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * \u003cp\u003e{@code java.util.Optional} itself was not used because at the time, the\n * project did not require Java 8. Using {@code com.google.common.base.Optional}\n * has in the past caused serious library version conflicts with Guava that can't\n * be resolved by shading. Hence this work-alike clone.\u003c/p\u003e\n *\n * @param \u003cT\u003e type of value held inside\n */"
			},
			{
				"signature": "private Optional(T value)",
				"documentation": ""
			},
			{
				"signature": "public static \u003cT\u003e Optional\u003cT\u003e empty()",
				"documentation": "/**\n   * @return an empty {@code Optional}\n   */"
			},
			{
				"signature": "public static \u003cT\u003e Optional\u003cT\u003e of(T value)",
				"documentation": "/**\n   * @param value non-null value to wrap\n   * @return {@code Optional} wrapping this value\n   * @throws NullPointerException if value is null\n   */"
			},
			{
				"signature": "public static \u003cT\u003e Optional\u003cT\u003e ofNullable(T value)",
				"documentation": "/**\n   * @param value value to wrap, which may be null\n   * @return {@code Optional} wrapping this value, which may be empty\n   */"
			},
			{
				"signature": "public T get()",
				"documentation": "/**\n   * @return the value wrapped by this {@code Optional}\n   * @throws NullPointerException if this is empty (contains no value)\n   */"
			},
			{
				"signature": "public T orElse(T other)",
				"documentation": "/**\n   * @param other value to return if this is empty\n   * @return this {@code Optional}'s value if present, or else the given value\n   */"
			},
			{
				"signature": "public boolean isPresent()",
				"documentation": "/**\n   * @return true iff this {@code Optional} contains a value (non-empty)\n   */"
			},
			{
				"signature": "public static \u003cT\u003e Optional\u003cT\u003e absent()",
				"documentation": "/**\n   * @return an empty {@code Optional}\n   */"
			},
			{
				"signature": "public static \u003cT\u003e Optional\u003cT\u003e fromNullable(T value)",
				"documentation": "/**\n   * @param value value to wrap, which may be null\n   * @return {@code Optional} wrapping this value, which may be empty\n   */"
			},
			{
				"signature": "public T or(T other)",
				"documentation": "/**\n   * @param other value to return if this is empty\n   * @return this {@code Optional}'s value if present, or else the given value\n   */"
			},
			{
				"signature": "public T orNull()",
				"documentation": "/**\n   * @return this {@code Optional}'s value if present, or else null\n   */"
			},
			{
				"signature": "@Override\n  public boolean equals(Object obj)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite",
			"org.apache.spark.streaming.JavaMapWithStateSuite",
			"test.org.apache.spark.streaming.Java8APISuite",
			"test.org.apache.spark.streaming.JavaAPISuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Expose some commonly useful storage level constants.\n */",
		"name": "org.apache.spark.api.java.StorageLevels",
		"extends": "",
		"Methods": [
			{
				"signature": "public static StorageLevel create(\n    boolean useDisk,\n    boolean useMemory,\n    boolean useOffHeap,\n    boolean deserialized,\n    int replication)",
				"documentation": "/**\n   * Create a new StorageLevel object.\n   * @param useDisk saved to disk, if true\n   * @param useMemory saved to on-heap memory, if true\n   * @param useOffHeap saved to off-heap memory, if true\n   * @param deserialized saved as deserialized objects, if true\n   * @param replication replication factor\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A function that returns zero or more output records from each grouping key and its values from 2\n * Datasets.\n */",
		"name": "org.apache.spark.api.java.function.CoGroupFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A function that returns zero or more records of type Double from each input record.\n */",
		"name": "org.apache.spark.api.java.function.DoubleFlatMapFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n *  A function that returns Doubles, and can be used to construct DoubleRDDs.\n */",
		"name": "org.apache.spark.api.java.function.DoubleFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base interface for a function used in Dataset's filter function.\n *\n * If the function returns true, the element is included in the returned Dataset.\n */",
		"name": "org.apache.spark.api.java.function.FilterFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A function that returns zero or more output records from each input record.\n */",
		"name": "org.apache.spark.api.java.function.FlatMapFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.streaming.JavaKinesisWordCountASL"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A function that takes two inputs and returns zero or more output records.\n */",
		"name": "org.apache.spark.api.java.function.FlatMapFunction2",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A function that returns zero or more output records from each grouping key and its values.\n */",
		"name": "org.apache.spark.api.java.function.FlatMapGroupsFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base interface for a function used in Dataset's foreach function.\n *\n * Spark will invoke the call function on each element in the input Dataset.\n */",
		"name": "org.apache.spark.api.java.function.ForeachFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base interface for a function used in Dataset's foreachPartition function.\n */",
		"name": "org.apache.spark.api.java.function.ForeachPartitionFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base interface for functions whose return types do not create special RDDs. PairFunction and\n * DoubleFunction are handled separately, to allow PairRDDs and DoubleRDDs to be constructed\n * when mapping RDDs of other types.\n */",
		"name": "org.apache.spark.api.java.function.Function",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A zero-argument function that returns an R.\n */",
		"name": "org.apache.spark.api.java.function.Function0",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A two-argument function that takes arguments of type T1 and T2 and returns an R.\n */",
		"name": "org.apache.spark.api.java.function.Function2",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.streaming.JavaKinesisWordCountASL"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A three-argument function that takes arguments of type T1, T2 and T3 and returns an R.\n */",
		"name": "org.apache.spark.api.java.function.Function3",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A four-argument function that takes arguments of type T1, T2, T3 and T4 and returns an R.\n */",
		"name": "org.apache.spark.api.java.function.Function4",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base interface for a map function used in Dataset's map function.\n */",
		"name": "org.apache.spark.api.java.function.MapFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base interface for a map function used in GroupedDataset's mapGroup function.\n */",
		"name": "org.apache.spark.api.java.function.MapGroupsFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base interface for function used in Dataset's mapPartitions.\n */",
		"name": "org.apache.spark.api.java.function.MapPartitionsFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A function that returns zero or more key-value pair records from each input record. The\n * key-value pairs are represented as scala.Tuple2 objects.\n */",
		"name": "org.apache.spark.api.java.function.PairFlatMapFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A function that returns key-value pairs (Tuple2\u0026lt;K, V\u0026gt;), and can be used to\n * construct PairRDDs.\n */",
		"name": "org.apache.spark.api.java.function.PairFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.streaming.JavaKinesisWordCountASL"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base interface for function used in Dataset's reduce.\n */",
		"name": "org.apache.spark.api.java.function.ReduceFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A function with no return value.\n */",
		"name": "org.apache.spark.api.java.function.VoidFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A two-argument function that takes arguments of type T1 and T2 with no return value.\n */",
		"name": "org.apache.spark.api.java.function.VoidFunction2",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n * Driver component of a {@link SparkPlugin}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.api.plugin.DriverPlugin",
		"extends": "",
		"Methods": [
			{
				"signature": "default Map\u003cString, String\u003e init(SparkContext sc, PluginContext pluginContext)",
				"documentation": "/**\n   * Initialize the plugin.\n   * \u003cp\u003e\n   * This method is called early in the initialization of the Spark driver. Explicitly, it is\n   * called before the Spark driver's task scheduler is initialized. This means that a lot\n   * of other Spark subsystems may yet not have been initialized. This call also blocks driver\n   * initialization.\n   * \u003cp\u003e\n   * It's recommended that plugins be careful about what operations are performed in this call,\n   * preferably performing expensive operations in a separate thread, or postponing them until\n   * the application has fully started.\n   *\n   * @param sc The SparkContext loading the plugin.\n   * @param pluginContext Additional plugin-specific about the Spark application where the plugin\n   *                      is running.\n   * @return A map that will be provided to the {@link ExecutorPlugin#init(PluginContext,Map)}\n   *         method.\n   */"
			},
			{
				"signature": "default void registerMetrics(String appId, PluginContext pluginContext)",
				"documentation": "/**\n   * Register metrics published by the plugin with Spark's metrics system.\n   * \u003cp\u003e\n   * This method is called later in the initialization of the Spark application, after most\n   * subsystems are up and the application ID is known. If there are metrics registered in\n   * the registry ({@link PluginContext#metricRegistry()}), then a metrics source with the\n   * plugin name will be created.\n   * \u003cp\u003e\n   * Note that even though the metric registry is still accessible after this method is called,\n   * registering new metrics after this method is called may result in the metrics not being\n   * available.\n   *\n   * @param appId The application ID from the cluster manager.\n   * @param pluginContext Additional plugin-specific about the Spark application where the plugin\n   *                      is running.\n   */"
			},
			{
				"signature": "default Object receive(Object message) throws Exception",
				"documentation": "/**\n   * RPC message handler.\n   * \u003cp\u003e\n   * Plugins can use Spark's RPC system to send messages from executors to the driver (but not\n   * the other way around, currently). Messages sent by the executor component of the plugin will\n   * be delivered to this method, and the returned value will be sent back to the executor as\n   * the reply, if the executor has requested one.\n   * \u003cp\u003e\n   * Any exception thrown will be sent back to the executor as an error, in case it is expecting\n   * a reply. In case a reply is not expected, a log message will be written to the driver log.\n   * \u003cp\u003e\n   * The implementation of this handler should be thread-safe.\n   * \u003cp\u003e\n   * Note all plugins share RPC dispatch threads, and this method is called synchronously. So\n   * performing expensive operations in this handler may affect the operation of other active\n   * plugins. Internal Spark endpoints are not directly affected, though, since they use different\n   * threads.\n   * \u003cp\u003e\n   * Spark guarantees that the driver component will be ready to receive messages through this\n   * handler when executors are started.\n   *\n   * @param message The incoming message.\n   * @return Value to be returned to the caller. Ignored if the caller does not expect a reply.\n   */"
			},
			{
				"signature": "default void shutdown()",
				"documentation": "/**\n   * Informs the plugin that the Spark application is shutting down.\n   * \u003cp\u003e\n   * This method is called during the driver shutdown phase. It is recommended that plugins\n   * not use any Spark functions (e.g. send RPC messages) during this call.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n * Executor component of a {@link SparkPlugin}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.api.plugin.ExecutorPlugin",
		"extends": "",
		"Methods": [
			{
				"signature": "default void init(PluginContext ctx, Map\u003cString, String\u003e extraConf)",
				"documentation": "/**\n   * Initialize the executor plugin.\n   * \u003cp\u003e\n   * When a Spark plugin provides an executor plugin, this method will be called during the\n   * initialization of the executor process. It will block executor initialization until it\n   * returns.\n   * \u003cp\u003e\n   * Executor plugins that publish metrics should register all metrics with the context's\n   * registry ({@link PluginContext#metricRegistry()}) when this method is called. Metrics\n   * registered afterwards are not guaranteed to show up.\n   *\n   * @param ctx Context information for the executor where the plugin is running.\n   * @param extraConf Extra configuration provided by the driver component during its\n   *                  initialization.\n   */"
			},
			{
				"signature": "default void shutdown()",
				"documentation": "/**\n   * Clean up and terminate this plugin.\n   * \u003cp\u003e\n   * This method is called during the executor shutdown phase, and blocks executor shutdown.\n   */"
			},
			{
				"signature": "default void onTaskStart()",
				"documentation": "/**\n   * Perform any action before the task is run.\n   * \u003cp\u003e\n   * This method is invoked from the same thread the task will be executed.\n   * Task-specific information can be accessed via {@link org.apache.spark.TaskContext#get}.\n   * \u003cp\u003e\n   * Plugin authors should avoid expensive operations here, as this method will be called\n   * on every task, and doing something expensive can significantly slow down a job.\n   * It is not recommended for a user to call a remote service, for example.\n   * \u003cp\u003e\n   * Exceptions thrown from this method do not propagate - they're caught,\n   * logged, and suppressed. Therefore exceptions when executing this method won't\n   * make the job fail.\n   *\n   * @since 3.1.0\n   */"
			},
			{
				"signature": "default void onTaskSucceeded()",
				"documentation": "/**\n   * Perform an action after tasks completes without exceptions.\n   * \u003cp\u003e\n   * As {@link #onTaskStart() onTaskStart} exceptions are suppressed, this method\n   * will still be invoked even if the corresponding {@link #onTaskStart} call for this\n   * task failed.\n   * \u003cp\u003e\n   * Same warnings of {@link #onTaskStart() onTaskStart} apply here.\n   *\n   * @since 3.1.0\n   */"
			},
			{
				"signature": "default void onTaskFailed(TaskFailedReason failureReason)",
				"documentation": "/**\n   * Perform an action after tasks completes with exceptions.\n   * \u003cp\u003e\n   * Same warnings of {@link #onTaskStart() onTaskStart} apply here.\n   *\n   * @param failureReason the exception thrown from the failed task.\n   *\n   * @since 3.1.0\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n * Context information and operations for plugins loaded by Spark.\n * \u003cp\u003e\n * An instance of this class is provided to plugins in their initialization method. It is safe\n * for plugins to keep a reference to the instance for later use (for example, to send messages\n * to the plugin's driver component).\n * \u003cp\u003e\n * Context instances are plugin-specific, so metrics and messages are tied each plugin. It is\n * not possible for a plugin to directly interact with other plugins.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.api.plugin.PluginContext",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n * A plugin that can be dynamically loaded into a Spark application.\n * \u003cp\u003e\n * Plugins can be loaded by adding the plugin's class name to the appropriate Spark configuration.\n * Check the Spark monitoring guide for details.\n * \u003cp\u003e\n * Plugins have two optional components: a driver-side component, of which a single instance is\n * created per application, inside the Spark driver. And an executor-side component, of which one\n * instance is created in each executor that is started by Spark. Details of each component can be\n * found in the documentation for {@link DriverPlugin} and {@link ExecutorPlugin}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.api.plugin.SparkPlugin",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n * A plugin that can be dynamically loaded into a Spark application to control how custom\n * resources are discovered. Plugins can be chained to allow different plugins to handle\n * different resource types.\n * \u003cp\u003e\n * Plugins must implement the function discoveryResource.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.api.resource.ResourceDiscoveryPlugin",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * {@link InputStream} implementation which uses direct buffer\n * to read a file to avoid extra copy of data between Java and\n * native memory which happens when using {@link java.io.BufferedInputStream}.\n * Unfortunately, this is not something already available in JDK,\n * {@code sun.nio.ch.ChannelInputStream} supports reading a file using nio,\n * but does not support buffering.\n */",
		"name": "org.apache.spark.io.NioBufferedFileInputStream",
		"extends": "java.io.InputStream",
		"Methods": [
			{
				"signature": "public NioBufferedFileInputStream(File file, int bufferSizeInBytes) throws IOException",
				"documentation": "/**\n * {@link InputStream} implementation which uses direct buffer\n * to read a file to avoid extra copy of data between Java and\n * native memory which happens when using {@link java.io.BufferedInputStream}.\n * Unfortunately, this is not something already available in JDK,\n * {@code sun.nio.ch.ChannelInputStream} supports reading a file using nio,\n * but does not support buffering.\n */"
			},
			{
				"signature": "public NioBufferedFileInputStream(File file) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private boolean refill() throws IOException",
				"documentation": "/**\n   * Checks whether data is left to be read from the input stream.\n   * @return true if data is left, false otherwise\n   */"
			},
			{
				"signature": "private long skipFromFileChannel(long n) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Override\n  protected void finalize() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * {@link InputStream} implementation which asynchronously reads ahead from the underlying input\n * stream when specified amount of data has been read from the current buffer. It does it by\n * maintaining two buffers - active buffer and read ahead buffer. Active buffer contains data\n * which should be returned when a read() call is issued. The read ahead buffer is used to\n * asynchronously read from the underlying input stream and once the current active buffer is\n * exhausted, we flip the two buffers so that we can start reading from the read ahead buffer\n * without being blocked in disk I/O.\n */",
		"name": "org.apache.spark.io.ReadAheadInputStream",
		"extends": "java.io.InputStream",
		"Methods": [
			{
				"signature": "public ReadAheadInputStream(\n      InputStream inputStream, int bufferSizeInBytes)",
				"documentation": "/**\n   * Creates a \u003ccode\u003eReadAheadInputStream\u003c/code\u003e with the specified buffer size and read-ahead\n   * threshold\n   *\n   * @param inputStream The underlying input stream.\n   * @param bufferSizeInBytes The buffer size.\n   */"
			},
			{
				"signature": "private boolean isEndOfStream()",
				"documentation": ""
			},
			{
				"signature": "private void checkReadException() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void readAsync() throws IOException",
				"documentation": "/** Read data from underlyingInputStream to readAheadBuffer asynchronously. */"
			},
			{
				"signature": "private void closeUnderlyingInputStreamIfNecessary()",
				"documentation": ""
			},
			{
				"signature": "private void signalAsyncReadComplete()",
				"documentation": ""
			},
			{
				"signature": "private void waitForAsyncReadComplete() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int read() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int read(byte[] b, int offset, int len) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void swapBuffers()",
				"documentation": "/**\n   * flip the active and read ahead buffer\n   */"
			},
			{
				"signature": "@Override\n  public int available() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long skip(long n) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private long skipInternal(long n) throws IOException",
				"documentation": "/**\n   * Internal skip function which should be called only from skip() api. The assumption is that\n   * the stateChangeLock is already acquired in the caller before calling this function.\n   */"
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A memory consumer of {@link TaskMemoryManager} that supports spilling.\n *\n * Note: this only supports allocation / spilling of Tungsten memory.\n */",
		"name": "org.apache.spark.memory.MemoryConsumer",
		"extends": "",
		"Methods": [
			{
				"signature": "protected MemoryConsumer(TaskMemoryManager taskMemoryManager, long pageSize, MemoryMode mode)",
				"documentation": "/**\n * A memory consumer of {@link TaskMemoryManager} that supports spilling.\n *\n * Note: this only supports allocation / spilling of Tungsten memory.\n */"
			},
			{
				"signature": "protected MemoryConsumer(TaskMemoryManager taskMemoryManager, MemoryMode mode)",
				"documentation": ""
			},
			{
				"signature": "public MemoryMode getMode()",
				"documentation": "/**\n   * Returns the memory mode, {@link MemoryMode#ON_HEAP} or {@link MemoryMode#OFF_HEAP}.\n   */"
			},
			{
				"signature": "public long getUsed()",
				"documentation": "/**\n   * Returns the size of used memory in bytes.\n   */"
			},
			{
				"signature": "public void spill() throws IOException",
				"documentation": "/**\n   * Force spill during building.\n   */"
			},
			{
				"signature": "public LongArray allocateArray(long size)",
				"documentation": "/**\n   * Allocates a LongArray of `size`. Note that this method may throw `SparkOutOfMemoryError`\n   * if Spark doesn't have enough memory for this allocation, or throw `TooLargePageException`\n   * if this `LongArray` is too large to fit in a single page. The caller side should take care of\n   * these two exceptions, or make sure the `size` is small enough that won't trigger exceptions.\n   *\n   * @throws SparkOutOfMemoryError\n   * @throws TooLargePageException\n   */"
			},
			{
				"signature": "public void freeArray(LongArray array)",
				"documentation": "/**\n   * Frees a LongArray.\n   */"
			},
			{
				"signature": "protected MemoryBlock allocatePage(long required)",
				"documentation": "/**\n   * Allocate a memory block with at least `required` bytes.\n   *\n   * @throws SparkOutOfMemoryError\n   */"
			},
			{
				"signature": "protected void freePage(MemoryBlock page)",
				"documentation": "/**\n   * Free a memory block.\n   */"
			},
			{
				"signature": "public long acquireMemory(long size)",
				"documentation": "/**\n   * Allocates memory of `size`.\n   */"
			},
			{
				"signature": "public void freeMemory(long size)",
				"documentation": "/**\n   * Release N bytes of memory.\n   */"
			},
			{
				"signature": "private void throwOom(final MemoryBlock page, final long required)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.memory.TestMemoryConsumer"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.memory.MemoryMode",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This exception is thrown when a task can not acquire memory from the Memory manager.\n * Instead of throwing {@link OutOfMemoryError}, which kills the executor,\n * we should use throw this exception, which just kills the current task.\n */",
		"name": "org.apache.spark.memory.SparkOutOfMemoryError",
		"extends": "OutOfMemoryError",
		"Methods": [
			{
				"signature": "public SparkOutOfMemoryError(String s)",
				"documentation": "/**\n * This exception is thrown when a task can not acquire memory from the Memory manager.\n * Instead of throwing {@link OutOfMemoryError}, which kills the executor,\n * we should use throw this exception, which just kills the current task.\n */"
			},
			{
				"signature": "public SparkOutOfMemoryError(OutOfMemoryError e)",
				"documentation": ""
			},
			{
				"signature": "public SparkOutOfMemoryError(String errorClass, String[] messageParameters)",
				"documentation": ""
			},
			{
				"signature": "public String getErrorClass()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.SparkThrowable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Manages the memory allocated by an individual task.\n * \u003cp\u003e\n * Most of the complexity in this class deals with encoding of off-heap addresses into 64-bit longs.\n * In off-heap mode, memory can be directly addressed with 64-bit longs. In on-heap mode, memory is\n * addressed by the combination of a base Object reference and a 64-bit offset within that object.\n * This is a problem when we want to store pointers to data structures inside of other structures,\n * such as record pointers inside hashmaps or sorting buffers. Even if we decided to use 128 bits\n * to address memory, we can't just store the address of the base object since it's not guaranteed\n * to remain stable as the heap gets reorganized due to GC.\n * \u003cp\u003e\n * Instead, we use the following approach to encode record pointers in 64-bit longs: for off-heap\n * mode, just store the raw address, and for on-heap mode use the upper 13 bits of the address to\n * store a \"page number\" and the lower 51 bits to store an offset within this page. These page\n * numbers are used to index into a \"page table\" array inside of the MemoryManager in order to\n * retrieve the base object.\n * \u003cp\u003e\n * This allows us to address 8192 pages. In on-heap mode, the maximum page size is limited by the\n * maximum size of a long[] array, allowing us to address 8192 * (2^31 - 1) * 8 bytes, which is\n * approximately 140 terabytes of memory.\n */",
		"name": "org.apache.spark.memory.TaskMemoryManager",
		"extends": "",
		"Methods": [
			{
				"signature": "public TaskMemoryManager(MemoryManager memoryManager, long taskAttemptId)",
				"documentation": "/**\n   * Construct a new TaskMemoryManager.\n   */"
			},
			{
				"signature": "public long acquireExecutionMemory(long required, MemoryConsumer requestingConsumer)",
				"documentation": "/**\n   * Acquire N bytes of memory for a consumer. If there is no enough memory, it will call\n   * spill() of consumers to release more memory.\n   *\n   * @return number of bytes successfully granted (\u003c= N).\n   */"
			},
			{
				"signature": "private long trySpillAndAcquire(\n      MemoryConsumer requestingConsumer,\n      long requested,\n      List\u003cMemoryConsumer\u003e cList,\n      int idx)",
				"documentation": "/**\n   * Try to acquire as much memory as possible from `cList[idx]`, up to `requested` bytes by\n   * spilling and then acquiring the freed memory. If no more memory can be spilled from\n   * `cList[idx]`, remove it from the list.\n   *\n   * @return number of bytes acquired (\u003c= requested)\n   * @throws RuntimeException if task is interrupted\n   * @throws SparkOutOfMemoryError if an IOException occurs during spilling\n   */"
			},
			{
				"signature": "public void releaseExecutionMemory(long size, MemoryConsumer consumer)",
				"documentation": "/**\n   * Release N bytes of execution memory for a MemoryConsumer.\n   */"
			},
			{
				"signature": "public void showMemoryUsage()",
				"documentation": "/**\n   * Dump the memory usage of all consumers.\n   */"
			},
			{
				"signature": "public long pageSizeBytes()",
				"documentation": "/**\n   * Return the page size in bytes.\n   */"
			},
			{
				"signature": "public MemoryBlock allocatePage(long size, MemoryConsumer consumer)",
				"documentation": "/**\n   * Allocate a block of memory that will be tracked in the MemoryManager's page table; this is\n   * intended for allocating large blocks of Tungsten memory that will be shared between operators.\n   *\n   * Returns `null` if there was not enough memory to allocate the page. May return a page that\n   * contains fewer bytes than requested, so callers should verify the size of returned pages.\n   *\n   * @throws TooLargePageException\n   */"
			},
			{
				"signature": "public void freePage(MemoryBlock page, MemoryConsumer consumer)",
				"documentation": "/**\n   * Free a block of memory allocated via {@link TaskMemoryManager#allocatePage}.\n   */"
			},
			{
				"signature": "public long encodePageNumberAndOffset(MemoryBlock page, long offsetInPage)",
				"documentation": "/**\n   * Given a memory page and offset within that page, encode this address into a 64-bit long.\n   * This address will remain valid as long as the corresponding page has not been freed.\n   *\n   * @param page a data page allocated by {@link TaskMemoryManager#allocatePage}/\n   * @param offsetInPage an offset in this page which incorporates the base offset. In other words,\n   *                     this should be the value that you would pass as the base offset into an\n   *                     UNSAFE call (e.g. page.baseOffset() + something).\n   * @return an encoded page address.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  public static long encodePageNumberAndOffset(int pageNumber, long offsetInPage)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public static int decodePageNumber(long pagePlusOffsetAddress)",
				"documentation": ""
			},
			{
				"signature": "private static long decodeOffset(long pagePlusOffsetAddress)",
				"documentation": ""
			},
			{
				"signature": "public Object getPage(long pagePlusOffsetAddress)",
				"documentation": "/**\n   * Get the page associated with an address encoded by\n   * {@link TaskMemoryManager#encodePageNumberAndOffset(MemoryBlock, long)}\n   */"
			},
			{
				"signature": "public long getOffsetInPage(long pagePlusOffsetAddress)",
				"documentation": "/**\n   * Get the offset associated with an address encoded by\n   * {@link TaskMemoryManager#encodePageNumberAndOffset(MemoryBlock, long)}\n   */"
			},
			{
				"signature": "public long cleanUpAllAllocatedMemory()",
				"documentation": "/**\n   * Clean up all allocated memory and pages. Returns the number of bytes freed. A non-zero return\n   * value can be used to detect memory leaks.\n   */"
			},
			{
				"signature": "public long getMemoryConsumptionForThisTask()",
				"documentation": "/**\n   * Returns the memory consumption, in bytes, for the current task.\n   */"
			},
			{
				"signature": "public MemoryMode getTungstenMemoryMode()",
				"documentation": "/**\n   * Returns Tungsten memory mode\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite",
			"test.org.apache.spark.sql.execution.sort.RecordBinaryComparatorSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.memory.TooLargePageException",
		"extends": "RuntimeException",
		"Methods": [
			{
				"signature": "TooLargePageException(long size)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n * Our shuffle write path doesn't actually use this serializer (since we end up calling the\n * `write() OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n * around this, we pass a dummy no-op serializer.\n */",
		"name": "org.apache.spark.serializer.DummySerializerInstance",
		"extends": "SerializerInstance",
		"Methods": [
			{
				"signature": "private DummySerializerInstance()",
				"documentation": "/**\n * Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.\n * Our shuffle write path doesn't actually use this serializer (since we end up calling the\n * `write() OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work\n * around this, we pass a dummy no-op serializer.\n */"
			},
			{
				"signature": "@Override\n  public SerializationStream serializeStream(final OutputStream s)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void flush()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public \u003cT\u003e SerializationStream writeObject(T t, ClassTag\u003cT\u003e ev1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e ByteBuffer serialize(T t, ClassTag\u003cT\u003e ev1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public DeserializationStream deserializeStream(InputStream s)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e T deserialize(ByteBuffer bytes, ClassLoader loader, ClassTag\u003cT\u003e ev1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e T deserialize(ByteBuffer bytes, ClassTag\u003cT\u003e ev1)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private",
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: Private ::\n * An interface for plugging in modules for storing and reading temporary shuffle data.\n * \u003cp\u003e\n * This is the root of a plugin system for storing shuffle bytes to arbitrary storage\n * backends in the sort-based shuffle algorithm implemented by the\n * {@link org.apache.spark.shuffle.sort.SortShuffleManager}. If another shuffle algorithm is\n * needed instead of sort-based shuffle, one should implement\n * {@link org.apache.spark.shuffle.ShuffleManager} instead.\n * \u003cp\u003e\n * A single instance of this module is loaded per process in the Spark application.\n * The default implementation reads and writes shuffle data from the local disks of\n * the executor, and is the implementation of shuffle file storage that has remained\n * consistent throughout most of Spark's history.\n * \u003cp\u003e\n * Alternative implementations of shuffle data storage can be loaded via setting\n * \u003ccode\u003espark.shuffle.sort.io.plugin.class\u003c/code\u003e.\n * @since 3.0.0\n */",
		"name": "org.apache.spark.shuffle.api.ShuffleDataIO",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: Private ::\n * An interface for building shuffle support modules for the Driver.\n */",
		"name": "org.apache.spark.shuffle.api.ShuffleDriverComponents",
		"extends": "",
		"Methods": [
			{
				"signature": "default void registerShuffle(int shuffleId)",
				"documentation": "/**\n   * Called once per shuffle id when the shuffle id is first generated for a shuffle stage.\n   *\n   * @param shuffleId The unique identifier for the shuffle stage.\n   */"
			},
			{
				"signature": "default void removeShuffle(int shuffleId, boolean blocking)",
				"documentation": "/**\n   * Removes shuffle data associated with the given shuffle.\n   *\n   * @param shuffleId The unique identifier for the shuffle stage.\n   * @param blocking Whether this call should block on the deletion of the data.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: Private ::\n * An interface for building shuffle support for Executors.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.shuffle.api.ShuffleExecutorComponents",
		"extends": "",
		"Methods": [
			{
				"signature": "default Optional\u003cSingleSpillShuffleMapOutputWriter\u003e createSingleFileMapOutputWriter(\n      int shuffleId,\n      long mapId) throws IOException",
				"documentation": "/**\n   * An optional extension for creating a map output writer that can optimize the transfer of a\n   * single partition file, as the entire result of a map task, to the backing store.\n   * \u003cp\u003e\n   * Most implementations should return the default {@link Optional#empty()} to indicate that\n   * they do not support this optimization. This primarily is for backwards-compatibility in\n   * preserving an optimization in the local disk shuffle storage implementation.\n   *\n   * @param shuffleId Unique identifier for the shuffle the map task is a part of\n   * @param mapId An ID of the map task. The ID is unique within this Spark application.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: Private ::\n * A top-level writer that returns child writers for persisting the output of a map task,\n * and then commits all of the writes as one atomic operation.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.shuffle.api.ShuffleMapOutputWriter",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: Private ::\n * An interface for opening streams to persist partition bytes to a backing data store.\n * \u003cp\u003e\n * This writer stores bytes for one (mapper, reducer) pair, corresponding to one shuffle\n * block.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.shuffle.api.ShufflePartitionWriter",
		"extends": "",
		"Methods": [
			{
				"signature": "default Optional\u003cWritableByteChannelWrapper\u003e openChannelWrapper() throws IOException",
				"documentation": "/**\n   * Opens and returns a {@link WritableByteChannelWrapper} for transferring bytes from\n   * input byte channels to the underlying shuffle data store.\n   * \u003cp\u003e\n   * This method will only be called once on this partition writer in the map task, to write the\n   * bytes to the partition. The channel will only be used to write the bytes for this\n   * partition. The map task closes this channel upon writing all the bytes for this\n   * block, or if the write fails for any reason.\n   * \u003cp\u003e\n   * Implementations that intend on combining the bytes for all the partitions written by this\n   * map task should reuse the same channel instance across all the partition writers provided\n   * by the parent {@link ShuffleMapOutputWriter}. If one does so, ensure that\n   * {@link WritableByteChannelWrapper#close()} does not close the resource, since the channel\n   * will be reused across partition writes. The underlying resources should be cleaned up in\n   * {@link ShuffleMapOutputWriter#commitAllPartitions(long[])} and\n   * {@link ShuffleMapOutputWriter#abort(Throwable)}.\n   * \u003cp\u003e\n   * This method is primarily for advanced optimizations where bytes can be copied from the input\n   * spill files to the output channel without copying data into memory. If such optimizations are\n   * not supported, the implementation should return {@link Optional#empty()}. By default, the\n   * implementation returns {@link Optional#empty()}.\n   * \u003cp\u003e\n   * Note that the returned {@link WritableByteChannelWrapper} itself is closed, but not the\n   * underlying channel that is returned by {@link WritableByteChannelWrapper#channel()}. Ensure\n   * that the underlying channel is cleaned up in {@link WritableByteChannelWrapper#close()},\n   * {@link ShuffleMapOutputWriter#commitAllPartitions(long[])}, or\n   * {@link ShuffleMapOutputWriter#abort(Throwable)}.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Optional extension for partition writing that is optimized for transferring a single\n * file to the backing store.\n */",
		"name": "org.apache.spark.shuffle.api.SingleSpillShuffleMapOutputWriter",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: Private ::\n * A thin wrapper around a {@link WritableByteChannel}.\n * \u003cp\u003e\n * This is primarily provided for the local disk shuffle implementation to provide a\n * {@link java.nio.channels.FileChannel} that keeps the channel open across partition writes.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.shuffle.api.WritableByteChannelWrapper",
		"extends": "java.io.Closeable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: Private ::\n * Represents the result of writing map outputs for a shuffle map task.\n * \u003cp\u003e\n * Partition lengths represents the length of each block written in the map task. This can\n * be used for downstream readers to allocate resources, such as in-memory buffers.\n * \u003cp\u003e\n * Map output writers can choose to attach arbitrary metadata tags to register with a\n * shuffle output tracker (a module that is currently yet to be built in a future\n * iteration of the shuffle storage APIs).\n */",
		"name": "org.apache.spark.shuffle.api.metadata.MapOutputCommitMessage",
		"extends": "",
		"Methods": [
			{
				"signature": "private MapOutputCommitMessage(\n      long[] partitionLengths, Optional\u003cMapOutputMetadata\u003e mapOutputMetadata)",
				"documentation": "/**\n * :: Private ::\n * Represents the result of writing map outputs for a shuffle map task.\n * \u003cp\u003e\n * Partition lengths represents the length of each block written in the map task. This can\n * be used for downstream readers to allocate resources, such as in-memory buffers.\n * \u003cp\u003e\n * Map output writers can choose to attach arbitrary metadata tags to register with a\n * shuffle output tracker (a module that is currently yet to be built in a future\n * iteration of the shuffle storage APIs).\n */"
			},
			{
				"signature": "public static MapOutputCommitMessage of(long[] partitionLengths)",
				"documentation": ""
			},
			{
				"signature": "public static MapOutputCommitMessage of(\n      long[] partitionLengths, MapOutputMetadata mapOutputMetadata)",
				"documentation": ""
			},
			{
				"signature": "public long[] getPartitionLengths()",
				"documentation": ""
			},
			{
				"signature": "public Optional\u003cMapOutputMetadata\u003e getMapOutputMetadata()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: Private ::\n * An opaque metadata tag for registering the result of committing the output of a\n * shuffle map task.\n * \u003cp\u003e\n * All implementations must be serializable since this is sent from the executors to\n * the driver.\n */",
		"name": "org.apache.spark.shuffle.api.metadata.MapOutputMetadata",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.checksum.ShuffleChecksumSupport",
		"extends": "",
		"Methods": [
			{
				"signature": "default Checksum[] createPartitionChecksums(int numPartitions, SparkConf conf)",
				"documentation": ""
			},
			{
				"signature": "default long[] getChecksumValues(Checksum[] partitionChecksums)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class implements sort-based shuffle's hash-style shuffle fallback path. This write path\n * writes incoming records to separate files, one file per reduce partition, then concatenates these\n * per-partition files to form a single output file, regions of which are served to reducers.\n * Records are not buffered in memory. It writes output in a format\n * that can be served / consumed via {@link org.apache.spark.shuffle.IndexShuffleBlockResolver}.\n * \u003cp\u003e\n * This write path is inefficient for shuffles with large numbers of reduce partitions because it\n * simultaneously opens separate serializers and file streams for all partitions. As a result,\n * {@link SortShuffleManager} only selects this write path when\n * \u003cul\u003e\n *    \u003cli\u003eno map-side combine is specified, and\u003c/li\u003e\n *    \u003cli\u003ethe number of partitions is less than or equal to\n *      \u003ccode\u003espark.shuffle.sort.bypassMergeThreshold\u003c/code\u003e.\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * This code used to be part of {@link org.apache.spark.util.collection.ExternalSorter} but was\n * refactored into its own class in order to reduce code complexity; see SPARK-7855 for details.\n * \u003cp\u003e\n * There have been proposals to completely remove this code path; see SPARK-6026 for details.\n */",
		"name": "org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter",
		"extends": "org.apache.spark.shuffle.ShuffleWriter",
		"Methods": [
			{
				"signature": "BypassMergeSortShuffleWriter(\n      BlockManager blockManager,\n      BypassMergeSortShuffleHandle\u003cK, V\u003e handle,\n      long mapId,\n      SparkConf conf,\n      ShuffleWriteMetricsReporter writeMetrics,\n      ShuffleExecutorComponents shuffleExecutorComponents) throws SparkException",
				"documentation": "/**\n   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n   * and then call stop() with success = false if they get an exception, we want to make sure\n   * we don't try deleting files, etc twice.\n   */"
			},
			{
				"signature": "@Override\n  public void write(Iterator\u003cProduct2\u003cK, V\u003e\u003e records) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long[] getPartitionLengths()",
				"documentation": ""
			},
			{
				"signature": "private long[] writePartitionedData(ShuffleMapOutputWriter mapOutputWriter) throws IOException",
				"documentation": "/**\n   * Concatenate all of the per-partition files into a single combined file.\n   *\n   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker).\n   */"
			},
			{
				"signature": "private void writePartitionedDataWithChannel(\n      File file,\n      WritableByteChannelWrapper outputChannel) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void writePartitionedDataWithStream(File file, ShufflePartitionWriter writer)\n      throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Option\u003cMapStatus\u003e stop(boolean success)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.checksum.ShuffleChecksumSupport"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Wrapper around an 8-byte word that holds a 24-bit partition number and 40-bit record pointer.\n * \u003cp\u003e\n * Within the long, the data is laid out as follows:\n * \u003cpre\u003e\n *   [24 bit partition number][13 bit memory page number][27 bit offset in page]\n * \u003c/pre\u003e\n * This implies that the maximum addressable page size is 2^27 bits = 128 megabytes, assuming that\n * our offsets in pages are not 8-byte-word-aligned. Since we have 2^13 pages (based off the\n * 13-bit page numbers assigned by {@link org.apache.spark.memory.TaskMemoryManager}), this\n * implies that we can address 2^13 * 128 megabytes = 1 terabyte of RAM per task.\n * \u003cp\u003e\n * Assuming word-alignment would allow for a 1 gigabyte maximum page size, but we leave this\n * optimization to future work as it will require more careful design to ensure that addresses are\n * properly aligned (e.g. by padding records).\n */",
		"name": "org.apache.spark.shuffle.sort.PackedRecordPointer",
		"extends": "",
		"Methods": [
			{
				"signature": "public static long packPointer(long recordPointer, int partitionId)",
				"documentation": "/**\n   * Pack a record address and partition id into a single word.\n   *\n   * @param recordPointer a record pointer encoded by TaskMemoryManager.\n   * @param partitionId a shuffle partition id (maximum value of 2^24).\n   * @return a packed pointer that can be decoded using the {@link PackedRecordPointer} class.\n   */"
			},
			{
				"signature": "public void set(long packedRecordPointer)",
				"documentation": ""
			},
			{
				"signature": "public int getPartitionId()",
				"documentation": ""
			},
			{
				"signature": "public long getRecordPointer()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An external sorter that is specialized for sort-based shuffle.\n * \u003cp\u003e\n * Incoming records are appended to data pages. When all records have been inserted (or when the\n * current thread's shuffle memory limit is reached), the in-memory records are sorted according to\n * their partition ids (using a {@link ShuffleInMemorySorter}). The sorted records are then\n * written to a single output file (or multiple files, if we've spilled). The format of the output\n * files is the same as the format of the final output file written by\n * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are\n * written as a single serialized, compressed stream that can be read with a new decompression and\n * deserialization stream.\n * \u003cp\u003e\n * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its\n * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a\n * specialized merge procedure that avoids extra serialization/deserialization.\n */",
		"name": "org.apache.spark.shuffle.sort.ShuffleExternalSorter",
		"extends": "org.apache.spark.memory.MemoryConsumer",
		"Methods": [
			{
				"signature": "ShuffleExternalSorter(\n      TaskMemoryManager memoryManager,\n      BlockManager blockManager,\n      TaskContext taskContext,\n      int initialSize,\n      int numPartitions,\n      SparkConf conf,\n      ShuffleWriteMetricsReporter writeMetrics) throws SparkException",
				"documentation": "/** Peak memory used by this sorter so far, in bytes. **/"
			},
			{
				"signature": "public long[] getChecksums()",
				"documentation": ""
			},
			{
				"signature": "private void writeSortedFile(boolean isLastFile)",
				"documentation": "/**\n   * Sorts the in-memory records and writes the sorted records to an on-disk file.\n   * This method does not free the sort data structures.\n   *\n   * @param isLastFile if true, this indicates that we're writing the final output file and that the\n   *                   bytes written should be counted towards shuffle spill metrics rather than\n   *                   shuffle write metrics.\n   */"
			},
			{
				"signature": "@Override\n  public long spill(long size, MemoryConsumer trigger) throws IOException",
				"documentation": "/**\n   * Sort and spill the current records in response to memory pressure.\n   */"
			},
			{
				"signature": "private long getMemoryUsage()",
				"documentation": ""
			},
			{
				"signature": "private void updatePeakMemoryUsed()",
				"documentation": ""
			},
			{
				"signature": "long getPeakMemoryUsedBytes()",
				"documentation": "/**\n   * Return the peak memory used so far, in bytes.\n   */"
			},
			{
				"signature": "private long freeMemory()",
				"documentation": ""
			},
			{
				"signature": "public void cleanupResources()",
				"documentation": "/**\n   * Force all memory and spill files to be deleted; called by shuffle error-handling code.\n   */"
			},
			{
				"signature": "private void growPointerArrayIfNecessary() throws IOException",
				"documentation": "/**\n   * Checks whether there is enough space to insert an additional record in to the sort pointer\n   * array and grows the array if additional space is required. If the required space cannot be\n   * obtained, then the in-memory data will be spilled to disk.\n   */"
			},
			{
				"signature": "private void acquireNewPageIfNecessary(int required)",
				"documentation": "/**\n   * Allocates more memory in order to insert an additional record. This will request additional\n   * memory from the memory manager and spill if the requested memory can not be obtained.\n   *\n   * @param required the required space in the data page, in bytes, including space for storing\n   *                      the record size. This must be less than or equal to the page size (records\n   *                      that exceed the page size are handled via a different code path which uses\n   *                      special overflow pages).\n   */"
			},
			{
				"signature": "public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException",
				"documentation": "/**\n   * Write a record to the shuffle sorter.\n   */"
			},
			{
				"signature": "public SpillInfo[] closeAndGetSpills() throws IOException",
				"documentation": "/**\n   * Close the sorter, causing any buffered data to be sorted and written out to disk.\n   *\n   * @return metadata for the spill files written by this sorter. If no records were ever inserted\n   *         into this sorter, then this will return an empty array.\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.checksum.ShuffleChecksumSupport"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UnsafeAlignedOffset"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.ShuffleInMemorySorter",
		"extends": "",
		"Methods": [
			{
				"signature": "ShuffleInMemorySorter(MemoryConsumer consumer, int initialSize, boolean useRadixSort)",
				"documentation": "/**\n   * How many records could be inserted, because part of the array should be left for sorting.\n   */"
			},
			{
				"signature": "private int getUsableCapacity()",
				"documentation": ""
			},
			{
				"signature": "public void free()",
				"documentation": ""
			},
			{
				"signature": "public int numRecords()",
				"documentation": ""
			},
			{
				"signature": "public void reset()",
				"documentation": ""
			},
			{
				"signature": "public void expandPointerArray(LongArray newArray)",
				"documentation": ""
			},
			{
				"signature": "public boolean hasSpaceForAnotherRecord()",
				"documentation": ""
			},
			{
				"signature": "public long getMemoryUsage()",
				"documentation": ""
			},
			{
				"signature": "public void insertRecord(long recordPointer, int partitionId)",
				"documentation": "/**\n   * Inserts a record to be sorted.\n   *\n   * @param recordPointer a pointer to the record, encoded by the task memory manager. Due to\n   *                      certain pointer compression techniques used by the sorter, the sort can\n   *                      only operate on pointers that point to locations in the first\n   *                      {@link PackedRecordPointer#MAXIMUM_PAGE_SIZE_BYTES} bytes of a data page.\n   * @param partitionId the partition id, which must be less than or equal to\n   *                    {@link PackedRecordPointer#MAXIMUM_PARTITION_ID}.\n   */"
			},
			{
				"signature": "public ShuffleSorterIterator getSortedIterator()",
				"documentation": "/**\n   * Return an iterator over record pointers in sorted order.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.array.LongArray",
			"org.apache.spark.unsafe.memory.MemoryBlock",
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter.SortComparator",
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter.ShuffleSorterIterator",
			"org.apache.spark.util.collection.unsafe.sort.RadixSort"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter.SortComparator",
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter.ShuffleSorterIterator"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.ShuffleInMemorySorter.SortComparator",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public int compare(PackedRecordPointer left, PackedRecordPointer right)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Comparator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * An iterator-like class that's used instead of Java's Iterator in order to facilitate inlining.\n   */",
		"name": "org.apache.spark.shuffle.sort.ShuffleInMemorySorter.ShuffleSorterIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "ShuffleSorterIterator(int numRecords, LongArray pointerArray, int startingPosition)",
				"documentation": "/**\n   * An iterator-like class that's used instead of Java's Iterator in order to facilitate inlining.\n   */"
			},
			{
				"signature": "public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "public void loadNext()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.ShuffleSortDataFormat",
		"extends": "org.apache.spark.util.collection.SortDataFormat",
		"Methods": [
			{
				"signature": "ShuffleSortDataFormat(LongArray buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public PackedRecordPointer getKey(LongArray data, int pos)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public PackedRecordPointer newKey()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public PackedRecordPointer getKey(LongArray data, int pos, PackedRecordPointer reuse)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void swap(LongArray data, int pos0, int pos1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void copyElement(LongArray src, int srcPos, LongArray dst, int dstPos)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void copyRange(LongArray src, int srcPos, LongArray dst, int dstPos, int length)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public LongArray allocate(int length)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Metadata for a block of data written by {@link ShuffleExternalSorter}.\n */",
		"name": "org.apache.spark.shuffle.sort.SpillInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "SpillInfo(int numPartitions, File file, TempShuffleBlockId blockId)",
				"documentation": "/**\n * Metadata for a block of data written by {@link ShuffleExternalSorter}.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.UnsafeShuffleWriter",
		"extends": "org.apache.spark.shuffle.ShuffleWriter",
		"Methods": [
			{
				"signature": "public UnsafeShuffleWriter(\n      BlockManager blockManager,\n      TaskMemoryManager memoryManager,\n      SerializedShuffleHandle\u003cK, V\u003e handle,\n      long mapId,\n      TaskContext taskContext,\n      SparkConf sparkConf,\n      ShuffleWriteMetricsReporter writeMetrics,\n      ShuffleExecutorComponents shuffleExecutorComponents) throws SparkException",
				"documentation": "/**\n   * Are we in the process of stopping? Because map tasks can call stop() with success = true\n   * and then call stop() with success = false if they get an exception, we want to make sure\n   * we don't try deleting files, etc twice.\n   */"
			},
			{
				"signature": "private void updatePeakMemoryUsed()",
				"documentation": ""
			},
			{
				"signature": "public long getPeakMemoryUsedBytes()",
				"documentation": "/**\n   * Return the peak memory used so far, in bytes.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  public void write(Iterator\u003cProduct2\u003cK, V\u003e\u003e records) throws IOException",
				"documentation": "/**\n   * This convenience method should only be called in test code.\n   */"
			},
			{
				"signature": "@Override\n  public void write(scala.collection.Iterator\u003cProduct2\u003cK, V\u003e\u003e records) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void open() throws SparkException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  void closeAndWriteOutput() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  void insertRecordIntoSorter(Product2\u003cK, V\u003e record) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  void forceSorterToSpill() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private long[] mergeSpills(SpillInfo[] spills) throws IOException",
				"documentation": "/**\n   * Merge zero or more spill files together, choosing the fastest merging strategy based on the\n   * number of spills and the IO compression codec.\n   *\n   * @return the partition lengths in the merged file.\n   */"
			},
			{
				"signature": "private long[] mergeSpillsUsingStandardWriter(SpillInfo[] spills) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void mergeSpillsWithFileStream(\n      SpillInfo[] spills,\n      ShuffleMapOutputWriter mapWriter,\n      @Nullable CompressionCodec compressionCodec) throws IOException",
				"documentation": "/**\n   * Merges spill files using Java FileStreams. This code path is typically slower than\n   * the NIO-based merge, {@link UnsafeShuffleWriter#mergeSpillsWithTransferTo(SpillInfo[],\n   * ShuffleMapOutputWriter)}, and it's mostly used in cases where the IO compression codec\n   * does not support concatenation of compressed data, when encryption is enabled, or when\n   * users have explicitly disabled use of {@code transferTo} in order to work around kernel bugs.\n   * This code path might also be faster in cases where individual partition size in a spill\n   * is small and UnsafeShuffleWriter#mergeSpillsWithTransferTo method performs many small\n   * disk ios which is inefficient. In those case, Using large buffers for input and output\n   * files helps reducing the number of disk ios, making the file merging faster.\n   *\n   * @param spills the spills to merge.\n   * @param mapWriter the map output writer to use for output.\n   * @param compressionCodec the IO compression codec, or null if shuffle compression is disabled.\n   * @return the partition lengths in the merged file.\n   */"
			},
			{
				"signature": "private void mergeSpillsWithTransferTo(\n      SpillInfo[] spills,\n      ShuffleMapOutputWriter mapWriter) throws IOException",
				"documentation": "/**\n   * Merges spill files by using NIO's transferTo to concatenate spill partitions' bytes.\n   * This is only safe when the IO compression codec and serializer support concatenation of\n   * serialized streams.\n   *\n   * @param spills the spills to merge.\n   * @param mapWriter the map output writer to use for output.\n   * @return the partition lengths in the merged file.\n   */"
			},
			{
				"signature": "@Override\n  public Option\u003cMapStatus\u003e stop(boolean success)",
				"documentation": ""
			},
			{
				"signature": "private static OutputStream openStreamUnchecked(ShufflePartitionWriter writer)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long[] getPartitionLengths()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.LimitedInputStream",
			"org.apache.spark.annotation.Private",
			"org.apache.spark.io.NioBufferedFileInputStream",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter.MyByteArrayOutputStream",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter.StreamFallbackChannelWrapper",
			"org.apache.spark.storage.TimeTrackingOutputStream"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter.MyByteArrayOutputStream",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter.StreamFallbackChannelWrapper"
		]
	},
	{
		"documentation": "/** Subclass of ByteArrayOutputStream that exposes `buf` directly. */",
		"name": "org.apache.spark.shuffle.sort.UnsafeShuffleWriter.MyByteArrayOutputStream",
		"extends": "ByteArrayOutputStream",
		"Methods": [
			{
				"signature": "MyByteArrayOutputStream(int size)",
				"documentation": "/** Subclass of ByteArrayOutputStream that exposes `buf` directly. */"
			},
			{
				"signature": "public byte[] getBuf()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.UnsafeShuffleWriter.StreamFallbackChannelWrapper",
		"extends": "",
		"Methods": [
			{
				"signature": "StreamFallbackChannelWrapper(OutputStream fallbackStream)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public WritableByteChannel channel()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.api.WritableByteChannelWrapper"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Implementation of the {@link ShuffleDataIO} plugin system that replicates the local shuffle\n * storage and index file functionality that has historically been used from Spark 2.4 and earlier.\n */",
		"name": "org.apache.spark.shuffle.sort.io.LocalDiskShuffleDataIO",
		"extends": "",
		"Methods": [
			{
				"signature": "public LocalDiskShuffleDataIO(SparkConf sparkConf)",
				"documentation": "/**\n * Implementation of the {@link ShuffleDataIO} plugin system that replicates the local shuffle\n * storage and index file functionality that has historically been used from Spark 2.4 and earlier.\n */"
			},
			{
				"signature": "@Override\n  public ShuffleExecutorComponents executor()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ShuffleDriverComponents driver()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.api.ShuffleDataIO"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.io.LocalDiskShuffleDriverComponents",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Map\u003cString, String\u003e initializeApplication()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cleanupApplication()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void removeShuffle(int shuffleId, boolean blocking)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.api.ShuffleDriverComponents"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents",
		"extends": "",
		"Methods": [
			{
				"signature": "public LocalDiskShuffleExecutorComponents(SparkConf sparkConf)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public LocalDiskShuffleExecutorComponents(\n      SparkConf sparkConf,\n      BlockManager blockManager,\n      IndexShuffleBlockResolver blockResolver)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initializeExecutor(String appId, String execId, Map\u003cString, String\u003e extraConfigs)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ShuffleMapOutputWriter createMapOutputWriter(\n      int shuffleId,\n      long mapTaskId,\n      int numPartitions)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Optional\u003cSingleSpillShuffleMapOutputWriter\u003e createSingleFileMapOutputWriter(\n      int shuffleId,\n      long mapId)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.api.ShuffleExecutorComponents"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Implementation of {@link ShuffleMapOutputWriter} that replicates the functionality of shuffle\n * persisting shuffle data to local disk alongside index files, identical to Spark's historic\n * canonical shuffle storage mechanism.\n */",
		"name": "org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter",
		"extends": "",
		"Methods": [
			{
				"signature": "public LocalDiskShuffleMapOutputWriter(\n      int shuffleId,\n      long mapId,\n      int numPartitions,\n      IndexShuffleBlockResolver blockResolver,\n      SparkConf sparkConf)",
				"documentation": "/**\n * Implementation of {@link ShuffleMapOutputWriter} that replicates the functionality of shuffle\n * persisting shuffle data to local disk alongside index files, identical to Spark's historic\n * canonical shuffle storage mechanism.\n */"
			},
			{
				"signature": "@Override\n  public ShufflePartitionWriter getPartitionWriter(int reducePartitionId) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MapOutputCommitMessage commitAllPartitions(long[] checksums) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void abort(Throwable error) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void cleanUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void initStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void initChannel() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.api.ShuffleMapOutputWriter"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.shuffle.api.metadata.MapOutputCommitMessage",
			"org.apache.spark.shuffle.sort.io.LocalDiskShufflePartitionWriter",
			"org.apache.spark.shuffle.sort.io.PartitionWriterStream",
			"org.apache.spark.shuffle.sort.io.PartitionWriterChannel"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.shuffle.sort.io.LocalDiskShufflePartitionWriter",
			"org.apache.spark.shuffle.sort.io.PartitionWriterStream",
			"org.apache.spark.shuffle.sort.io.PartitionWriterChannel"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.io.LocalDiskShufflePartitionWriter",
		"extends": "",
		"Methods": [
			{
				"signature": "private LocalDiskShufflePartitionWriter(int partitionId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public OutputStream openStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Optional\u003cWritableByteChannelWrapper\u003e openChannelWrapper() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long getNumBytesWritten()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.api.ShufflePartitionWriter"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.io.PartitionWriterStream",
		"extends": "java.io.OutputStream",
		"Methods": [
			{
				"signature": "PartitionWriterStream(int partitionId)",
				"documentation": ""
			},
			{
				"signature": "public long getCount()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void write(int b) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void write(byte[] buf, int pos, int length) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close()",
				"documentation": ""
			},
			{
				"signature": "private void verifyNotClosed()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.io.PartitionWriterChannel",
		"extends": "",
		"Methods": [
			{
				"signature": "PartitionWriterChannel(int partitionId)",
				"documentation": ""
			},
			{
				"signature": "public long getCount() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public WritableByteChannel channel()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.api.WritableByteChannelWrapper"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.io.LocalDiskSingleSpillMapOutputWriter",
		"extends": "",
		"Methods": [
			{
				"signature": "public LocalDiskSingleSpillMapOutputWriter(\n      int shuffleId,\n      long mapId,\n      IndexShuffleBlockResolver blockResolver)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void transferMapSpillFile(\n      File mapSpillFile,\n      long[] partitionLengths,\n      long[] checksums) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.api.SingleSpillShuffleMapOutputWriter"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.status.api.v1.ApplicationStatus",
		"extends": "",
		"Methods": [
			{
				"signature": "public static ApplicationStatus fromString(String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.EnumUtil"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.status.api.v1.StageStatus",
		"extends": "",
		"Methods": [
			{
				"signature": "public static StageStatus fromString(String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.EnumUtil"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.status.api.v1.TaskSorting",
		"extends": "",
		"Methods": [
			{
				"signature": "TaskSorting(String... names)",
				"documentation": ""
			},
			{
				"signature": "public static TaskSorting fromString(String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.EnumUtil"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.status.api.v1.TaskStatus",
		"extends": "",
		"Methods": [
			{
				"signature": "public static TaskStatus fromString(String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.EnumUtil"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Intercepts write calls and tracks total time spent writing in order to update shuffle write\n * metrics. Not thread safe.\n */",
		"name": "org.apache.spark.storage.TimeTrackingOutputStream",
		"extends": "java.io.OutputStream",
		"Methods": [
			{
				"signature": "public TimeTrackingOutputStream(\n      ShuffleWriteMetricsReporter writeMetrics, OutputStream outputStream)",
				"documentation": "/**\n * Intercepts write calls and tracks total time spent writing in order to update shuffle write\n * metrics. Not thread safe.\n */"
			},
			{
				"signature": "@Override\n  public void write(int b) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(byte[] b) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(byte[] b, int off, int len) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void flush() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An append-only hash map where keys and values are contiguous regions of bytes.\n *\n * This is backed by a power-of-2-sized hash table, using quadratic probing with triangular numbers,\n * which is guaranteed to exhaust the space.\n *\n * The map can support up to 2^29 keys. If the key cardinality is higher than this, you should\n * probably be using sorting instead of hashing for better cache locality.\n *\n * The key and values under the hood are stored together, in the following format:\n *   First uaoSize bytes: len(k) (key length in bytes) + len(v) (value length in bytes) + uaoSize\n *   Next uaoSize bytes: len(k)\n *   Next len(k) bytes: key data\n *   Next len(v) bytes: value data\n *   Last 8 bytes: pointer to next pair\n *\n * It means first uaoSize bytes store the entire record (key + value + uaoSize) length. This format\n * is compatible with {@link org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter},\n * so we can pass records from this map directly into the sorter to sort records in place.\n */",
		"name": "org.apache.spark.unsafe.map.BytesToBytesMap",
		"extends": "org.apache.spark.memory.MemoryConsumer",
		"Methods": [
			{
				"signature": "public BytesToBytesMap(\n      TaskMemoryManager taskMemoryManager,\n      BlockManager blockManager,\n      SerializerManager serializerManager,\n      int initialCapacity,\n      double loadFactor,\n      long pageSizeBytes)",
				"documentation": "/**\n   * Return value of {@link BytesToBytesMap#lookup(Object, long, int)}.\n   */"
			},
			{
				"signature": "public BytesToBytesMap(\n      TaskMemoryManager taskMemoryManager,\n      int initialCapacity,\n      long pageSizeBytes)",
				"documentation": ""
			},
			{
				"signature": "public int numKeys()",
				"documentation": "/**\n   * Returns the number of keys defined in the map.\n   */"
			},
			{
				"signature": "public int numValues()",
				"documentation": "/**\n   * Returns the number of values defined in the map. A key could have multiple values.\n   */"
			},
			{
				"signature": "public MapIterator iterator()",
				"documentation": "/**\n   * Returns an iterator for iterating over the entries of this map.\n   *\n   * For efficiency, all calls to `next()` will return the same {@link Location} object.\n   *\n   * The returned iterator is thread-safe. However if the map is modified while iterating over it,\n   * the behavior of the returned iterator is undefined.\n   */"
			},
			{
				"signature": "public MapIterator destructiveIterator()",
				"documentation": "/**\n   * Returns a destructive iterator for iterating over the entries of this map. It frees each page\n   * as it moves onto next one. Notice: it is illegal to call any method on the map after\n   * `destructiveIterator()` has been called.\n   *\n   * For efficiency, all calls to `next()` will return the same {@link Location} object.\n   *\n   * The returned iterator is thread-safe. However if the map is modified while iterating over it,\n   * the behavior of the returned iterator is undefined.\n   */"
			},
			{
				"signature": "public MapIteratorWithKeyIndex iteratorWithKeyIndex()",
				"documentation": "/**\n   * Returns an iterator for iterating over the entries of this map,\n   * by first iterating over the key index inside hash map's `longArray`.\n   *\n   * For efficiency, all calls to `next()` will return the same {@link Location} object.\n   *\n   * The returned iterator is NOT thread-safe. If the map is modified while iterating over it,\n   * the behavior of the returned iterator is undefined.\n   */"
			},
			{
				"signature": "public int maxNumKeysIndex()",
				"documentation": "/**\n   * The maximum number of allowed keys index.\n   *\n   * The value of allowed keys index is in the range of [0, maxNumKeysIndex - 1].\n   */"
			},
			{
				"signature": "public Location lookup(Object keyBase, long keyOffset, int keyLength)",
				"documentation": "/**\n   * Looks up a key, and return a {@link Location} handle that can be used to test existence\n   * and read/write values.\n   *\n   * This function always returns the same {@link Location} instance to avoid object allocation.\n   * This function is not thread-safe.\n   */"
			},
			{
				"signature": "public Location lookup(Object keyBase, long keyOffset, int keyLength, int hash)",
				"documentation": "/**\n   * Looks up a key, and return a {@link Location} handle that can be used to test existence\n   * and read/write values.\n   *\n   * This function always returns the same {@link Location} instance to avoid object allocation.\n   * This function is not thread-safe.\n   */"
			},
			{
				"signature": "public void safeLookup(Object keyBase, long keyOffset, int keyLength, Location loc, int hash)",
				"documentation": "/**\n   * Looks up a key, and saves the result in provided `loc`.\n   *\n   * This is a thread-safe version of `lookup`, could be used by multiple threads.\n   */"
			},
			{
				"signature": "private boolean acquireNewPage(long required)",
				"documentation": "/**\n   * Acquire a new page from the memory manager.\n   * @return whether there is enough space to allocate the new page.\n   */"
			},
			{
				"signature": "@Override\n  public long spill(long size, MemoryConsumer trigger) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void allocate(int capacity)",
				"documentation": "/**\n   * Allocate new data structures for this map. When calling this outside of the constructor,\n   * make sure to keep references to the old data structures so that you can free them.\n   *\n   * @param capacity the new map capacity\n   */"
			},
			{
				"signature": "public void free()",
				"documentation": "/**\n   * Free all allocated memory associated with this map, including the storage for keys and values\n   * as well as the hash map array itself.\n   *\n   * This method is idempotent and can be called multiple times.\n   */"
			},
			{
				"signature": "public TaskMemoryManager getTaskMemoryManager()",
				"documentation": ""
			},
			{
				"signature": "public long getPageSizeBytes()",
				"documentation": ""
			},
			{
				"signature": "public long getTotalMemoryConsumption()",
				"documentation": "/**\n   * Returns the total amount of memory, in bytes, consumed by this map's managed structures.\n   */"
			},
			{
				"signature": "private void updatePeakMemoryUsed()",
				"documentation": ""
			},
			{
				"signature": "public long getPeakMemoryUsedBytes()",
				"documentation": "/**\n   * Return the peak memory used so far, in bytes.\n   */"
			},
			{
				"signature": "public double getAvgHashProbeBucketListIterations()",
				"documentation": "/**\n   * Returns the average number of probes per key lookup.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  public int getNumDataPages()",
				"documentation": ""
			},
			{
				"signature": "public LongArray getArray()",
				"documentation": "/**\n   * Returns the underline long[] of longArray.\n   */"
			},
			{
				"signature": "public void reset()",
				"documentation": "/**\n   * Reset this map to initialized state.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  void growAndRehash()",
				"documentation": "/**\n   * Grows the size of the hash table and re-hash everything.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UnsafeAlignedOffset",
			"org.apache.spark.unsafe.array.ByteArrayMethods",
			"org.apache.spark.unsafe.hash.Murmur3_x86_32",
			"org.apache.spark.unsafe.map.MapIteratorWithKeyIndex"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.unsafe.map.MapIterator",
			"org.apache.spark.unsafe.map.MapIteratorWithKeyIndex",
			"org.apache.spark.unsafe.map.Location"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.map.MapIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "private MapIterator(int numRecords, Location loc, boolean destructive)",
				"documentation": ""
			},
			{
				"signature": "private void advanceToNextPage()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Location next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void remove()",
				"documentation": ""
			},
			{
				"signature": "private void handleFailedDelete()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Iterator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UnsafeAlignedOffset",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Iterator for the entries of this map. This is to first iterate over key indices in\n   * `longArray` then accessing values in `dataPages`. NOTE: this is different from `MapIterator`\n   * in the sense that key index is preserved here\n   * (See `UnsafeHashedRelation` for example of usage).\n   */",
		"name": "org.apache.spark.unsafe.map.MapIteratorWithKeyIndex",
		"extends": "",
		"Methods": [
			{
				"signature": "private MapIteratorWithKeyIndex()",
				"documentation": "/**\n     * The index in `longArray` where the key is stored.\n     */"
			},
			{
				"signature": "@Override\n    public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Location next()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Iterator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.unsafe.map.BytesToBytesMap"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Handle returned by {@link BytesToBytesMap#lookup(Object, long, int)} function.\n   */",
		"name": "org.apache.spark.unsafe.map.Location",
		"extends": "",
		"Methods": [
			{
				"signature": "private void updateAddressesAndSizes(long fullKeyAddress)",
				"documentation": "/**\n     * Memory page containing the record. Only set if created by {@link BytesToBytesMap#iterator()}.\n     */"
			},
			{
				"signature": "private void updateAddressesAndSizes(final Object base, long offset)",
				"documentation": ""
			},
			{
				"signature": "private Location with(int pos, int keyHashcode, boolean isDefined)",
				"documentation": ""
			},
			{
				"signature": "private Location with(MemoryBlock page, long offsetInPage)",
				"documentation": ""
			},
			{
				"signature": "private Location with(Object base, long offset, int length)",
				"documentation": "/**\n     * This is only used for spilling\n     */"
			},
			{
				"signature": "public boolean nextValue()",
				"documentation": "/**\n     * Find the next pair that has the same key as current one.\n     */"
			},
			{
				"signature": "public MemoryBlock getMemoryPage()",
				"documentation": "/**\n     * Returns the memory page that contains the current record.\n     * This is only valid if this is returned by {@link BytesToBytesMap#iterator()}.\n     */"
			},
			{
				"signature": "public boolean isDefined()",
				"documentation": "/**\n     * Returns true if the key is defined at this position, and false otherwise.\n     */"
			},
			{
				"signature": "public int getKeyIndex()",
				"documentation": "/**\n     * Returns index for key.\n     */"
			},
			{
				"signature": "public Object getKeyBase()",
				"documentation": "/**\n     * Returns the base object for key.\n     */"
			},
			{
				"signature": "public long getKeyOffset()",
				"documentation": "/**\n     * Returns the offset for key.\n     */"
			},
			{
				"signature": "public Object getValueBase()",
				"documentation": "/**\n     * Returns the base object for value.\n     */"
			},
			{
				"signature": "public long getValueOffset()",
				"documentation": "/**\n     * Returns the offset for value.\n     */"
			},
			{
				"signature": "public int getKeyLength()",
				"documentation": "/**\n     * Returns the length of the key defined at this position.\n     * Unspecified behavior if the key is not defined.\n     */"
			},
			{
				"signature": "public int getValueLength()",
				"documentation": "/**\n     * Returns the length of the value defined at this position.\n     * Unspecified behavior if the key is not defined.\n     */"
			},
			{
				"signature": "public boolean append(Object kbase, long koff, int klen, Object vbase, long voff, int vlen)",
				"documentation": "/**\n     * Append a new value for the key. This method could be called multiple times for a given key.\n     * The return value indicates whether the put succeeded or whether it failed because additional\n     * memory could not be acquired.\n     * \u003cp\u003e\n     * It is only valid to call this method immediately after calling `lookup()` using the same key.\n     * \u003c/p\u003e\n     * \u003cp\u003e\n     * The key and value must be word-aligned (that is, their sizes must be a multiple of 8).\n     * \u003c/p\u003e\n     * \u003cp\u003e\n     * After calling this method, calls to `get[Key|Value]Address()` and `get[Key|Value]Length`\n     * will return information on the data stored by this `append` call.\n     * \u003c/p\u003e\n     * \u003cp\u003e\n     * As an example usage, here's the proper way to store a new key:\n     * \u003c/p\u003e\n     * \u003cpre\u003e\n     *   Location loc = map.lookup(keyBase, keyOffset, keyLength);\n     *   if (!loc.isDefined()) {\n     *     if (!loc.append(keyBase, keyOffset, keyLength, ...)) {\n     *       // handle failure to grow map (by spilling, for example)\n     *     }\n     *   }\n     * \u003c/pre\u003e\n     * \u003cp\u003e\n     * Unspecified behavior if the key is not defined.\n     * \u003c/p\u003e\n     *\n     * @return true if the put() was successful and false if the put() failed because memory could\n     *         not be acquired.\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UnsafeAlignedOffset"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface that defines how we can grow the size of a hash map when it is over a threshold.\n */",
		"name": "org.apache.spark.unsafe.map.HashMapGrowthStrategy",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.unsafe.map.Doubling"
		],
		"uses": [
			"org.apache.spark.unsafe.map.Doubling"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.unsafe.map.Doubling"
		]
	},
	{
		"documentation": "/**\n   * Double the size of the hash map every time.\n   */",
		"name": "org.apache.spark.unsafe.map.Doubling",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public int nextCapacity(int currentCapacity)",
				"documentation": "/**\n   * Double the size of the hash map every time.\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.unsafe.map.HashMapGrowthStrategy"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.unsafe.map.HashMapGrowthStrategy"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mutable class loader that gives preference to its own URLs over the parent class loader\n * when loading classes and resources.\n */",
		"name": "org.apache.spark.util.ChildFirstURLClassLoader",
		"extends": "org.apache.spark.util.MutableURLClassLoader",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * A mutable class loader that gives preference to its own URLs over the parent class loader\n * when loading classes and resources.\n */"
			},
			{
				"signature": "public ChildFirstURLClassLoader(URL[] urls, ClassLoader parent)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Class\u003c?\u003e loadClass(String name, boolean resolve) throws ClassNotFoundException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Enumeration\u003cURL\u003e getResources(String name) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public URL getResource(String name)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.EnumUtil",
		"extends": "",
		"Methods": [
			{
				"signature": "public static \u003cE extends Enum\u003cE\u003e\u003e E parseIgnoreCase(Class\u003cE\u003e clz, String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [
			"org.apache.spark.JobExecutionStatus",
			"org.apache.spark.status.api.v1.ApplicationStatus",
			"org.apache.spark.status.api.v1.StageStatus",
			"org.apache.spark.status.api.v1.TaskSorting",
			"org.apache.spark.status.api.v1.TaskStatus",
			"org.apache.spark.status.api.v1.streaming.BatchStatus"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * URL class loader that exposes the `addURL` method in URLClassLoader.\n */",
		"name": "org.apache.spark.util.MutableURLClassLoader",
		"extends": "java.net.URLClassLoader",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * URL class loader that exposes the `addURL` method in URLClassLoader.\n */"
			},
			{
				"signature": "public MutableURLClassLoader(URL[] urls, ClassLoader parent)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void addURL(URL url)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.ChildFirstURLClassLoader"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A class loader which makes some protected methods in ClassLoader accessible.\n */",
		"name": "org.apache.spark.util.ParentClassLoader",
		"extends": "ClassLoader",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * A class loader which makes some protected methods in ClassLoader accessible.\n */"
			},
			{
				"signature": "public ParentClassLoader(ClassLoader parent)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Class\u003c?\u003e findClass(String name) throws ClassNotFoundException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Class\u003c?\u003e loadClass(String name, boolean resolve) throws ClassNotFoundException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A port of the Android TimSort class, which utilizes a \"stable, adaptive, iterative mergesort.\"\n * See the method comment on sort() for more details.\n *\n * This has been kept in Java with the original style in order to match very closely with the\n * Android source code, and thus be easy to verify correctness. The class is package private. We put\n * a simple Scala wrapper {@link org.apache.spark.util.collection.Sorter}, which is available to\n * package org.apache.spark.\n *\n * The purpose of the port is to generalize the interface to the sort to accept input data formats\n * besides simple arrays where every element is sorted individually. For instance, the AppendOnlyMap\n * uses this to sort an Array with alternating elements of the form [key, value, key, value].\n * This generalization comes with minimal overhead -- see SortDataFormat for more information.\n *\n * We allow key reuse to prevent creating many key objects -- see SortDataFormat.\n *\n * @see org.apache.spark.util.collection.SortDataFormat\n * @see org.apache.spark.util.collection.Sorter\n */",
		"name": "org.apache.spark.util.collection.TimSort",
		"extends": "",
		"Methods": [
			{
				"signature": "public TimSort(SortDataFormat\u003cK, Buffer\u003e sortDataFormat)",
				"documentation": "/**\n   * This is the minimum sized sequence that will be merged.  Shorter\n   * sequences will be lengthened by calling binarySort.  If the entire\n   * array is less than this length, no merges will be performed.\n   *\n   * This constant should be a power of two.  It was 64 in Tim Peter's C\n   * implementation, but 32 was empirically determined to work better in\n   * this implementation.  In the unlikely event that you set this constant\n   * to be a number that's not a power of two, you'll need to change the\n   * minRunLength computation.\n   *\n   * If you decrease this constant, you must change the stackLen\n   * computation in the TimSort constructor, or you risk an\n   * ArrayOutOfBounds exception.  See listsort.txt for a discussion\n   * of the minimum stack length required as a function of the length\n   * of the array being sorted and the minimum merge sequence length.\n   */"
			},
			{
				"signature": "public void sort(Buffer a, int lo, int hi, Comparator\u003c? super K\u003e c)",
				"documentation": "/**\n   * A stable, adaptive, iterative mergesort that requires far fewer than\n   * n lg(n) comparisons when running on partially sorted arrays, while\n   * offering performance comparable to a traditional mergesort when run\n   * on random arrays.  Like all proper mergesorts, this sort is stable and\n   * runs O(n log n) time (worst case).  In the worst case, this sort requires\n   * temporary storage space for n/2 object references; in the best case,\n   * it requires only a small constant amount of space.\n   *\n   * This implementation was adapted from Tim Peters's list sort for\n   * Python, which is described in detail here:\n   *\n   *   http://svn.python.org/projects/python/trunk/Objects/listsort.txt\n   *\n   * Tim's C code may be found here:\n   *\n   *   http://svn.python.org/projects/python/trunk/Objects/listobject.c\n   *\n   * The underlying techniques are described in this paper (and may have\n   * even earlier origins):\n   *\n   *  \"Optimistic Sorting and Information Theoretic Complexity\"\n   *  Peter McIlroy\n   *  SODA (Fourth Annual ACM-SIAM Symposium on Discrete Algorithms),\n   *  pp 467-474, Austin, Texas, 25-27 January 1993.\n   *\n   * While the API to this class consists solely of static methods, it is\n   * (privately) instantiable; a TimSort instance holds the state of an ongoing\n   * sort, assuming the input array is large enough to warrant the full-blown\n   * TimSort. Small arrays are sorted in place, using a binary insertion sort.\n   *\n   * @author Josh Bloch\n   */"
			},
			{
				"signature": "do",
				"documentation": "/**\n     * March over the array once, left to right, finding natural runs,\n     * extending short natural runs to minRun elements, and merging runs\n     * to maintain stack invariant.\n     */"
			},
			{
				"signature": "@SuppressWarnings(\"fallthrough\")\n  private void binarySort(Buffer a, int lo, int hi, int start, Comparator\u003c? super K\u003e c)",
				"documentation": "/**\n   * Sorts the specified portion of the specified array using a binary\n   * insertion sort.  This is the best method for sorting small numbers\n   * of elements.  It requires O(n log n) compares, but O(n^2) data\n   * movement (worst case).\n   *\n   * If the initial part of the specified range is already sorted,\n   * this method can take advantage of it: the method assumes that the\n   * elements from index {@code lo}, inclusive, to {@code start},\n   * exclusive are already sorted.\n   *\n   * @param a the array in which a range is to be sorted\n   * @param lo the index of the first element in the range to be sorted\n   * @param hi the index after the last element in the range to be sorted\n   * @param start the index of the first element in the range that is\n   *        not already known to be sorted ({@code lo \u003c= start \u003c= hi})\n   * @param c comparator to used for the sort\n   */"
			},
			{
				"signature": "private int countRunAndMakeAscending(Buffer a, int lo, int hi, Comparator\u003c? super K\u003e c)",
				"documentation": "/**\n   * Returns the length of the run beginning at the specified position in\n   * the specified array and reverses the run if it is descending (ensuring\n   * that the run will always be ascending when the method returns).\n   *\n   * A run is the longest ascending sequence with:\n   *\n   *    a[lo] \u003c= a[lo + 1] \u003c= a[lo + 2] \u003c= ...\n   *\n   * or the longest descending sequence with:\n   *\n   *    a[lo] \u003e  a[lo + 1] \u003e  a[lo + 2] \u003e  ...\n   *\n   * For its intended use in a stable mergesort, the strictness of the\n   * definition of \"descending\" is needed so that the call can safely\n   * reverse a descending sequence without violating stability.\n   *\n   * @param a the array in which a run is to be counted and possibly reversed\n   * @param lo index of the first element in the run\n   * @param hi index after the last element that may be contained in the run.\n  It is required that {@code lo \u003c hi}.\n   * @param c the comparator to used for the sort\n   * @return  the length of the run beginning at the specified position in\n   *          the specified array\n   */"
			},
			{
				"signature": "private void reverseRange(Buffer a, int lo, int hi)",
				"documentation": "/**\n   * Reverse the specified range of the specified array.\n   *\n   * @param a the array in which a range is to be reversed\n   * @param lo the index of the first element in the range to be reversed\n   * @param hi the index after the last element in the range to be reversed\n   */"
			},
			{
				"signature": "private int minRunLength(int n)",
				"documentation": "/**\n   * Returns the minimum acceptable run length for an array of the specified\n   * length. Natural runs shorter than this will be extended with\n   * {@link #binarySort}.\n   *\n   * Roughly speaking, the computation is:\n   *\n   *  If n \u003c MIN_MERGE, return n (it's too small to bother with fancy stuff).\n   *  Else if n is an exact power of 2, return MIN_MERGE/2.\n   *  Else return an int k, MIN_MERGE/2 \u003c= k \u003c= MIN_MERGE, such that n/k\n   *   is close to, but strictly less than, an exact power of 2.\n   *\n   * For the rationale, see listsort.txt.\n   *\n   * @param n the length of the array to be sorted\n   * @return the length of the minimum run to be merged\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.collection.SortState"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.util.collection.SortState"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.SortState",
		"extends": "",
		"Methods": [
			{
				"signature": "private SortState(Buffer a, Comparator\u003c? super K\u003e c, int len)",
				"documentation": "/**\n     * Creates a TimSort instance to maintain the state of an ongoing sort.\n     *\n     * @param a the array to be sorted\n     * @param c the comparator to determine the order of the sort\n     */"
			},
			{
				"signature": "private void pushRun(int runBase, int runLen)",
				"documentation": "/**\n     * Pushes the specified run onto the pending-run stack.\n     *\n     * @param runBase index of the first element in the run\n     * @param runLen  the number of elements in the run\n     */"
			},
			{
				"signature": "private void mergeCollapse()",
				"documentation": "/**\n     * Examines the stack of runs waiting to be merged and merges adjacent runs\n     * until the stack invariants are reestablished:\n     *\n     *     1. runLen[i - 3] \u003e runLen[i - 2] + runLen[i - 1]\n     *     2. runLen[i - 2] \u003e runLen[i - 1]\n     *\n     * This method is called each time a new run is pushed onto the stack,\n     * so the invariants are guaranteed to hold for i \u003c stackSize upon\n     * entry to the method.\n     *\n     * Thanks to Stijn de Gouw, Jurriaan Rot, Frank S. de Boer,\n     * Richard Bubel and Reiner Hahnle, this is fixed with respect to\n     * the analysis in \"On the Worst-Case Complexity of TimSort\" by\n     * Nicolas Auger, Vincent Jug, Cyril Nicaud, and Carine Pivoteau.\n     */"
			},
			{
				"signature": "private void mergeForceCollapse()",
				"documentation": "/**\n     * Merges all runs on the stack until only one remains.  This method is\n     * called once, to complete the sort.\n     */"
			},
			{
				"signature": "private void mergeAt(int i)",
				"documentation": "/**\n     * Merges the two runs at stack indices i and i+1.  Run i must be\n     * the penultimate or antepenultimate run on the stack.  In other words,\n     * i must be equal to stackSize-2 or stackSize-3.\n     *\n     * @param i stack index of the first of the two runs to merge\n     */"
			},
			{
				"signature": "private int gallopLeft(K key, Buffer a, int base, int len, int hint, Comparator\u003c? super K\u003e c)",
				"documentation": "/**\n     * Locates the position at which to insert the specified key into the\n     * specified sorted range; if the range contains an element equal to key,\n     * returns the index of the leftmost equal element.\n     *\n     * @param key the key whose insertion point to search for\n     * @param a the array in which to search\n     * @param base the index of the first element in the range\n     * @param len the length of the range; must be \u003e 0\n     * @param hint the index at which to begin the search, 0 \u003c= hint \u003c n.\n     *     The closer hint is to the result, the faster this method will run.\n     * @param c the comparator used to order the range, and to search\n     * @return the int k,  0 \u003c= k \u003c= n such that a[b + k - 1] \u003c key \u003c= a[b + k],\n     *    pretending that a[b - 1] is minus infinity and a[b + n] is infinity.\n     *    In other words, key belongs at index b + k; or in other words,\n     *    the first k elements of a should precede key, and the last n - k\n     *    should follow it.\n     */"
			},
			{
				"signature": "private int gallopRight(K key, Buffer a, int base, int len, int hint, Comparator\u003c? super K\u003e c)",
				"documentation": "/**\n     * Like gallopLeft, except that if the range contains an element equal to\n     * key, gallopRight returns the index after the rightmost equal element.\n     *\n     * @param key the key whose insertion point to search for\n     * @param a the array in which to search\n     * @param base the index of the first element in the range\n     * @param len the length of the range; must be \u003e 0\n     * @param hint the index at which to begin the search, 0 \u003c= hint \u003c n.\n     *     The closer hint is to the result, the faster this method will run.\n     * @param c the comparator used to order the range, and to search\n     * @return the int k,  0 \u003c= k \u003c= n such that a[b + k - 1] \u003c= key \u003c a[b + k]\n     */"
			},
			{
				"signature": "private void mergeLo(int base1, int len1, int base2, int len2)",
				"documentation": "/**\n     * Merges two adjacent runs in place, in a stable fashion.  The first\n     * element of the first run must be greater than the first element of the\n     * second run (a[base1] \u003e a[base2]), and the last element of the first run\n     * (a[base1 + len1-1]) must be greater than all elements of the second run.\n     *\n     * For performance, this method should be called only when len1 \u003c= len2;\n     * its twin, mergeHi should be called if len1 \u003e= len2.  (Either method\n     * may be called if len1 == len2.)\n     *\n     * @param base1 index of first element in first run to be merged\n     * @param len1  length of first run to be merged (must be \u003e 0)\n     * @param base2 index of first element in second run to be merged\n     *        (must be aBase + aLen)\n     * @param len2  length of second run to be merged (must be \u003e 0)\n     */"
			},
			{
				"signature": "do",
				"documentation": ""
			},
			{
				"signature": "do",
				"documentation": ""
			},
			{
				"signature": "private void mergeHi(int base1, int len1, int base2, int len2)",
				"documentation": "/**\n     * Like mergeLo, except that this method should be called only if\n     * len1 \u003e= len2; mergeLo should be called if len1 \u003c= len2.  (Either method\n     * may be called if len1 == len2.)\n     *\n     * @param base1 index of first element in first run to be merged\n     * @param len1  length of first run to be merged (must be \u003e 0)\n     * @param base2 index of first element in second run to be merged\n     *        (must be aBase + aLen)\n     * @param len2  length of second run to be merged (must be \u003e 0)\n     */"
			},
			{
				"signature": "do",
				"documentation": ""
			},
			{
				"signature": "do",
				"documentation": ""
			},
			{
				"signature": "private Buffer ensureCapacity(int minCapacity)",
				"documentation": "/**\n     * Ensures that the external array tmp has at least the specified\n     * number of elements, increasing its size if necessary.  The size\n     * increases exponentially to ensure amortized linear time complexity.\n     *\n     * @param minCapacity the minimum required capacity of the tmp array\n     * @return tmp, whether or not it grew\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.TimSort"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Compares 8-byte key prefixes in prefix sort. Subclasses may implement type-specific\n * comparisons, such as lexicographic comparison for strings.\n */",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparator",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators",
		"extends": "",
		"Methods": [
			{
				"signature": "private PrefixComparators()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Private",
			"org.apache.spark.unsafe.types.ByteArray",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorNullsLast",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorDescNullsFirst",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorDesc",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorNullsLast",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorDescNullsFirst",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorDesc"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.StringPrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.BinaryPrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.DoublePrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorNullsLast",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorDescNullsFirst",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorDesc",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorNullsLast",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorDescNullsFirst",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorDesc"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.StringPrefixComparator",
		"extends": "",
		"Methods": [
			{
				"signature": "public static long computePrefix(UTF8String value)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.BinaryPrefixComparator",
		"extends": "",
		"Methods": [
			{
				"signature": "public static long computePrefix(byte[] bytes)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.types.ByteArray"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.DoublePrefixComparator",
		"extends": "",
		"Methods": [
			{
				"signature": "public static long computePrefix(double value)",
				"documentation": "/**\n     * Converts the double into a value that compares correctly as an unsigned long. For more\n     * details see http://stereopsis.com/radix.html.\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Provides radix sort parameters. Comparators implementing this also are indicating that the\n   * ordering they define is compatible with radix sort.\n   */",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparator",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorNullsLast",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorDescNullsFirst",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorDesc",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparator",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorNullsLast",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorDescNullsFirst",
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorDesc"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparator",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"Methods": [
			{
				"signature": "@Override public boolean sortDescending()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean sortSigned()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean nullsFirst()",
				"documentation": ""
			},
			{
				"signature": "public int compare(long aPrefix, long bPrefix)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorNullsLast",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"Methods": [
			{
				"signature": "@Override public boolean sortDescending()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean sortSigned()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean nullsFirst()",
				"documentation": ""
			},
			{
				"signature": "public int compare(long aPrefix, long bPrefix)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorDescNullsFirst",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"Methods": [
			{
				"signature": "@Override public boolean sortDescending()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean sortSigned()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean nullsFirst()",
				"documentation": ""
			},
			{
				"signature": "public int compare(long bPrefix, long aPrefix)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.UnsignedPrefixComparatorDesc",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"Methods": [
			{
				"signature": "@Override public boolean sortDescending()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean sortSigned()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean nullsFirst()",
				"documentation": ""
			},
			{
				"signature": "public int compare(long bPrefix, long aPrefix)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparator",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"Methods": [
			{
				"signature": "@Override public boolean sortDescending()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean sortSigned()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean nullsFirst()",
				"documentation": ""
			},
			{
				"signature": "public int compare(long a, long b)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorNullsLast",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"Methods": [
			{
				"signature": "@Override public boolean sortDescending()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean sortSigned()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean nullsFirst()",
				"documentation": ""
			},
			{
				"signature": "public int compare(long a, long b)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorDescNullsFirst",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"Methods": [
			{
				"signature": "@Override public boolean sortDescending()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean sortSigned()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean nullsFirst()",
				"documentation": ""
			},
			{
				"signature": "public int compare(long b, long a)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.SignedPrefixComparatorDesc",
		"extends": "org.apache.spark.util.collection.unsafe.sort.PrefixComparators.RadixSortSupport",
		"Methods": [
			{
				"signature": "@Override public boolean sortDescending()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean sortSigned()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean nullsFirst()",
				"documentation": ""
			},
			{
				"signature": "public int compare(long b, long a)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.PrefixComparators"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.RadixSort",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int sort(\n      LongArray array, long numRecords, int startByteIndex, int endByteIndex,\n      boolean desc, boolean signed)",
				"documentation": "/**\n   * Sorts a given array of longs using least-significant-digit radix sort. This routine assumes\n   * you have extra space at the end of the array at least equal to the number of records. The\n   * sort is destructive and may relocate the data positioned within the array.\n   *\n   * @param array array of long elements followed by at least that many empty slots.\n   * @param numRecords number of data records in the array.\n   * @param startByteIndex the first byte (in range [0, 7]) to sort each long by, counting from the\n   *                       least significant byte.\n   * @param endByteIndex the last byte (in range [0, 7]) to sort each long by, counting from the\n   *                     least significant byte. Must be greater than startByteIndex.\n   * @param desc whether this is a descending (binary-order) sort.\n   * @param signed whether this is a signed (two's complement) sort.\n   *\n   * @return The starting index of the sorted data within the given array. We return this instead\n   *         of always copying the data back to position zero for efficiency.\n   */"
			},
			{
				"signature": "private static void sortAtByte(\n      LongArray array, long numRecords, long[] counts, int byteIdx, long inIndex, long outIndex,\n      boolean desc, boolean signed)",
				"documentation": "/**\n   * Performs a partial sort by copying data into destination offsets for each byte value at the\n   * specified byte offset.\n   *\n   * @param array array to partially sort.\n   * @param numRecords number of data records in the array.\n   * @param counts counts for each byte value. This routine destructively modifies this array.\n   * @param byteIdx the byte in a long to sort at, counting from the least significant byte.\n   * @param inIndex the starting index in the array where input data is located.\n   * @param outIndex the starting index where sorted output data should be written.\n   * @param desc whether this is a descending (binary-order) sort.\n   * @param signed whether this is a signed (two's complement) sort (only applies to last byte).\n   */"
			},
			{
				"signature": "private static long[][] getCounts(\n      LongArray array, long numRecords, int startByteIndex, int endByteIndex)",
				"documentation": "/**\n   * Computes a value histogram for each byte in the given array.\n   *\n   * @param array array to count records in.\n   * @param numRecords number of data records in the array.\n   * @param startByteIndex the first byte to compute counts for (the prior are skipped).\n   * @param endByteIndex the last byte to compute counts for.\n   *\n   * @return an array of eight 256-byte count arrays, one for each byte starting from the least\n   *         significant byte. If the byte does not need sorting the array will be null.\n   */"
			},
			{
				"signature": "private static long[] transformCountsToOffsets(\n      long[] counts, long numRecords, long outputOffset, long bytesPerRecord,\n      boolean desc, boolean signed)",
				"documentation": "/**\n   * Transforms counts into the proper unsafe output offsets for the sort type.\n   *\n   * @param counts counts for each byte value. This routine destructively modifies this array.\n   * @param numRecords number of data records in the original data array.\n   * @param outputOffset output offset in bytes from the base array object.\n   * @param bytesPerRecord size of each record (8 for plain sort, 16 for key-prefix sort).\n   * @param desc whether this is a descending (binary-order) sort.\n   * @param signed whether this is a signed (two's complement) sort.\n   *\n   * @return the input counts array.\n   */"
			},
			{
				"signature": "public static int sortKeyPrefixArray(\n      LongArray array,\n      long startIndex,\n      long numRecords,\n      int startByteIndex,\n      int endByteIndex,\n      boolean desc,\n      boolean signed)",
				"documentation": "/**\n   * Specialization of sort() for key-prefix arrays. In this type of array, each record consists\n   * of two longs, only the second of which is sorted on.\n   *\n   * @param startIndex starting index in the array to sort from. This parameter is not supported\n   *    in the plain sort() implementation.\n   */"
			},
			{
				"signature": "private static long[][] getKeyPrefixArrayCounts(\n      LongArray array, long startIndex, long numRecords, int startByteIndex, int endByteIndex)",
				"documentation": "/**\n   * Specialization of getCounts() for key-prefix arrays. We could probably combine this with\n   * getCounts with some added parameters but that seems to hurt in benchmarks.\n   */"
			},
			{
				"signature": "private static void sortKeyPrefixArrayAtByte(\n      LongArray array, long numRecords, long[] counts, int byteIdx, long inIndex, long outIndex,\n      boolean desc, boolean signed)",
				"documentation": "/**\n   * Specialization of sortAtByte() for key-prefix arrays.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Compares records for ordering. In cases where the entire sorting key can fit in the 8-byte\n * prefix, this may simply return 0.\n */",
		"name": "org.apache.spark.util.collection.unsafe.sort.RecordComparator",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.RecordPointerAndKeyPrefix",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * External sorter based on {@link UnsafeInMemorySorter}.\n */",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter",
		"extends": "org.apache.spark.memory.MemoryConsumer",
		"Methods": [
			{
				"signature": "public static UnsafeExternalSorter createWithExistingInMemorySorter(\n      TaskMemoryManager taskMemoryManager,\n      BlockManager blockManager,\n      SerializerManager serializerManager,\n      TaskContext taskContext,\n      Supplier\u003cRecordComparator\u003e recordComparatorSupplier,\n      PrefixComparator prefixComparator,\n      int initialSize,\n      long pageSizeBytes,\n      int numElementsForSpillThreshold,\n      UnsafeInMemorySorter inMemorySorter,\n      long existingMemoryConsumption) throws IOException",
				"documentation": "/**\n   * Memory pages that hold the records being sorted. The pages in this list are freed when\n   * spilling, although in principle we could recycle these pages across spills (on the other hand,\n   * this might not be necessary if we maintained a pool of re-usable pages in the TaskMemoryManager\n   * itself).\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSortDataFormat",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterIterator",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter",
			"org.apache.spark.JavaJdbcRDDSuite",
			"org.apache.spark.api.java.OptionalSuite",
			"org.apache.spark.io.GenericFileInputStreamSuite",
			"org.apache.spark.io.NioBufferedInputStreamSuite",
			"org.apache.spark.io.ReadAheadInputStreamSuite",
			"org.apache.spark.launcher.SparkLauncherSuite",
			"org.apache.spark.memory.TaskMemoryManagerSuite",
			"org.apache.spark.memory.TestMemoryConsumer",
			"org.apache.spark.memory.TestPartialSpillingMemoryConsumer",
			"org.apache.spark.resource.JavaResourceProfileSuite",
			"org.apache.spark.serializer.ContainsProxyClass",
			"org.apache.spark.serializer.TestJavaSerializerImpl",
			"org.apache.spark.shuffle.sort.PackedRecordPointerSuite",
			"org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite",
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite",
			"org.apache.spark.unsafe.map.AbstractBytesToBytesMapSuite",
			"org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite",
			"org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite",
			"org.apache.spark.util.SerializableConfigurationSuite",
			"org.apache.spark.util.collection.TestTimSort",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterRadixSortSuite",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite",
			"test.org.apache.spark.Java8RDDAPISuite",
			"test.org.apache.spark.JavaAPISuite",
			"test.org.apache.spark.JavaSparkContextSuite",
			"test.org.apache.spark.JavaTaskContextCompileCheck",
			"org.apache.spark.examples.JavaHdfsLR",
			"org.apache.spark.examples.JavaLogQuery",
			"org.apache.spark.examples.JavaPageRank",
			"org.apache.spark.examples.JavaSparkPi",
			"org.apache.spark.examples.JavaStatusTrackerDemo",
			"org.apache.spark.examples.JavaTC",
			"org.apache.spark.examples.JavaWordCount",
			"org.apache.spark.examples.ml.JavaAFTSurvivalRegressionExample",
			"org.apache.spark.examples.ml.JavaALSExample",
			"org.apache.spark.examples.ml.JavaBinarizerExample",
			"org.apache.spark.examples.ml.JavaBisectingKMeansExample",
			"org.apache.spark.examples.ml.JavaBucketedRandomProjectionLSHExample",
			"org.apache.spark.examples.ml.JavaBucketizerExample",
			"org.apache.spark.examples.ml.JavaChiSqSelectorExample",
			"org.apache.spark.examples.ml.JavaChiSquareTestExample",
			"org.apache.spark.examples.ml.JavaCorrelationExample",
			"org.apache.spark.examples.ml.JavaCountVectorizerExample",
			"org.apache.spark.examples.ml.JavaDCTExample",
			"org.apache.spark.examples.ml.JavaDecisionTreeClassificationExample",
			"org.apache.spark.examples.ml.JavaDecisionTreeRegressionExample",
			"org.apache.spark.examples.ml.JavaDocument",
			"org.apache.spark.examples.ml.JavaElementwiseProductExample",
			"org.apache.spark.examples.ml.JavaEstimatorTransformerParamExample",
			"org.apache.spark.examples.ml.JavaFMClassifierExample",
			"org.apache.spark.examples.ml.JavaFMRegressorExample",
			"org.apache.spark.examples.ml.JavaFPGrowthExample",
			"org.apache.spark.examples.ml.JavaFeatureHasherExample",
			"org.apache.spark.examples.ml.JavaGaussianMixtureExample",
			"org.apache.spark.examples.ml.JavaGeneralizedLinearRegressionExample",
			"org.apache.spark.examples.ml.JavaGradientBoostedTreeClassifierExample",
			"org.apache.spark.examples.ml.JavaGradientBoostedTreeRegressorExample",
			"org.apache.spark.examples.ml.JavaImputerExample",
			"org.apache.spark.examples.ml.JavaIndexToStringExample",
			"org.apache.spark.examples.ml.JavaInteractionExample",
			"org.apache.spark.examples.ml.JavaIsotonicRegressionExample",
			"org.apache.spark.examples.ml.JavaKMeansExample",
			"org.apache.spark.examples.ml.JavaLDAExample",
			"org.apache.spark.examples.ml.JavaLabeledDocument",
			"org.apache.spark.examples.ml.JavaLinearRegressionWithElasticNetExample",
			"org.apache.spark.examples.ml.JavaLinearSVCExample",
			"org.apache.spark.examples.ml.JavaLogisticRegressionSummaryExample",
			"org.apache.spark.examples.ml.JavaLogisticRegressionWithElasticNetExample",
			"org.apache.spark.examples.ml.JavaMaxAbsScalerExample",
			"org.apache.spark.examples.ml.JavaMinHashLSHExample",
			"org.apache.spark.examples.ml.JavaMinMaxScalerExample",
			"org.apache.spark.examples.ml.JavaModelSelectionViaCrossValidationExample",
			"org.apache.spark.examples.ml.JavaModelSelectionViaTrainValidationSplitExample",
			"org.apache.spark.examples.ml.JavaMulticlassLogisticRegressionWithElasticNetExample",
			"org.apache.spark.examples.ml.JavaMultilayerPerceptronClassifierExample",
			"org.apache.spark.examples.ml.JavaNGramExample",
			"org.apache.spark.examples.ml.JavaNaiveBayesExample",
			"org.apache.spark.examples.ml.JavaNormalizerExample",
			"org.apache.spark.examples.ml.JavaOneHotEncoderExample",
			"org.apache.spark.examples.ml.JavaOneVsRestExample",
			"org.apache.spark.examples.ml.JavaPCAExample",
			"org.apache.spark.examples.ml.JavaPipelineExample",
			"org.apache.spark.examples.ml.JavaPolynomialExpansionExample",
			"org.apache.spark.examples.ml.JavaPowerIterationClusteringExample",
			"org.apache.spark.examples.ml.JavaPrefixSpanExample",
			"org.apache.spark.examples.ml.JavaQuantileDiscretizerExample",
			"org.apache.spark.examples.ml.JavaRFormulaExample",
			"org.apache.spark.examples.ml.JavaRandomForestClassifierExample",
			"org.apache.spark.examples.ml.JavaRandomForestRegressorExample",
			"org.apache.spark.examples.ml.JavaRobustScalerExample",
			"org.apache.spark.examples.ml.JavaSQLTransformerExample",
			"org.apache.spark.examples.ml.JavaStandardScalerExample",
			"org.apache.spark.examples.ml.JavaStopWordsRemoverExample",
			"org.apache.spark.examples.ml.JavaStringIndexerExample",
			"org.apache.spark.examples.ml.JavaSummarizerExample",
			"org.apache.spark.examples.ml.JavaTfIdfExample",
			"org.apache.spark.examples.ml.JavaTokenizerExample",
			"org.apache.spark.examples.ml.JavaUnivariateFeatureSelectorExample",
			"org.apache.spark.examples.ml.JavaVarianceThresholdSelectorExample",
			"org.apache.spark.examples.ml.JavaVectorAssemblerExample",
			"org.apache.spark.examples.ml.JavaVectorIndexerExample",
			"org.apache.spark.examples.ml.JavaVectorSizeHintExample",
			"org.apache.spark.examples.ml.JavaVectorSlicerExample",
			"org.apache.spark.examples.ml.JavaWord2VecExample",
			"org.apache.spark.examples.mllib.JavaALS",
			"org.apache.spark.examples.mllib.JavaAssociationRulesExample",
			"org.apache.spark.examples.mllib.JavaBinaryClassificationMetricsExample",
			"org.apache.spark.examples.mllib.JavaBisectingKMeansExample",
			"org.apache.spark.examples.mllib.JavaChiSqSelectorExample",
			"org.apache.spark.examples.mllib.JavaCorrelationsExample",
			"org.apache.spark.examples.mllib.JavaDecisionTreeClassificationExample",
			"org.apache.spark.examples.mllib.JavaDecisionTreeRegressionExample",
			"org.apache.spark.examples.mllib.JavaElementwiseProductExample",
			"org.apache.spark.examples.mllib.JavaGaussianMixtureExample",
			"org.apache.spark.examples.mllib.JavaGradientBoostingClassificationExample",
			"org.apache.spark.examples.mllib.JavaGradientBoostingRegressionExample",
			"org.apache.spark.examples.mllib.JavaHypothesisTestingExample",
			"org.apache.spark.examples.mllib.JavaHypothesisTestingKolmogorovSmirnovTestExample",
			"org.apache.spark.examples.mllib.JavaIsotonicRegressionExample",
			"org.apache.spark.examples.mllib.JavaKMeansExample",
			"org.apache.spark.examples.mllib.JavaKernelDensityEstimationExample",
			"org.apache.spark.examples.mllib.JavaLBFGSExample",
			"org.apache.spark.examples.mllib.JavaLatentDirichletAllocationExample",
			"org.apache.spark.examples.mllib.JavaLogisticRegressionWithLBFGSExample",
			"org.apache.spark.examples.mllib.JavaMultiLabelClassificationMetricsExample",
			"org.apache.spark.examples.mllib.JavaMulticlassClassificationMetricsExample",
			"org.apache.spark.examples.mllib.JavaNaiveBayesExample",
			"org.apache.spark.examples.mllib.JavaPCAExample",
			"org.apache.spark.examples.mllib.JavaPowerIterationClusteringExample",
			"org.apache.spark.examples.mllib.JavaPrefixSpanExample",
			"org.apache.spark.examples.mllib.JavaRandomForestClassificationExample",
			"org.apache.spark.examples.mllib.JavaRandomForestRegressionExample",
			"org.apache.spark.examples.mllib.JavaRankingMetricsExample",
			"org.apache.spark.examples.mllib.JavaRecommendationExample",
			"org.apache.spark.examples.mllib.JavaSVDExample",
			"org.apache.spark.examples.mllib.JavaSVMWithSGDExample",
			"org.apache.spark.examples.mllib.JavaSimpleFPGrowth",
			"org.apache.spark.examples.mllib.JavaStratifiedSamplingExample",
			"org.apache.spark.examples.mllib.JavaStreamingTestExample",
			"org.apache.spark.examples.mllib.JavaSummaryStatisticsExample",
			"org.apache.spark.examples.sql.JavaSQLDataSourceExample",
			"org.apache.spark.examples.sql.JavaSparkSQLExample",
			"org.apache.spark.examples.sql.JavaUserDefinedScalar",
			"org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation",
			"org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation",
			"org.apache.spark.examples.sql.hive.JavaSparkHiveExample",
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization",
			"org.apache.spark.examples.sql.streaming.JavaStructuredKafkaWordCount",
			"org.apache.spark.examples.sql.streaming.JavaStructuredKerberizedKafkaWordCount",
			"org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount",
			"org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCountWindowed",
			"org.apache.spark.examples.sql.streaming.JavaStructuredSessionization",
			"org.apache.spark.examples.streaming.JavaCustomReceiver",
			"org.apache.spark.examples.streaming.JavaDirectKafkaWordCount",
			"org.apache.spark.examples.streaming.JavaDirectKerberizedKafkaWordCount",
			"org.apache.spark.examples.streaming.JavaNetworkWordCount",
			"org.apache.spark.examples.streaming.JavaQueueStream",
			"org.apache.spark.examples.streaming.JavaRecord",
			"org.apache.spark.examples.streaming.JavaWordExcludeList",
			"org.apache.spark.examples.streaming.JavaDroppedWordsCounter",
			"org.apache.spark.examples.streaming.JavaRecoverableNetworkWordCount",
			"org.apache.spark.examples.streaming.JavaSqlNetworkWordCount",
			"org.apache.spark.examples.streaming.JavaSparkSessionSingleton",
			"org.apache.spark.examples.streaming.JavaStatefulNetworkWordCount",
			"org.apache.spark.graphx.TripletFields",
			"org.apache.spark.graphx.impl.EdgeActiveness",
			"org.apache.spark.launcher.AbstractAppHandle",
			"org.apache.spark.launcher.AbstractCommandBuilder",
			"org.apache.spark.launcher.AbstractLauncher",
			"org.apache.spark.launcher.ChildProcAppHandle",
			"org.apache.spark.launcher.CommandBuilderUtils",
			"org.apache.spark.launcher.FilteredObjectInputStream",
			"org.apache.spark.launcher.InProcessAppHandle",
			"org.apache.spark.launcher.InProcessLauncher",
			"org.apache.spark.launcher.JavaModuleOptions",
			"org.apache.spark.launcher.LauncherConnection",
			"org.apache.spark.launcher.LauncherProtocol",
			"org.apache.spark.launcher.LauncherServer",
			"org.apache.spark.launcher.Main",
			"org.apache.spark.launcher.NamedThreadFactory",
			"org.apache.spark.launcher.OutputRedirector",
			"org.apache.spark.launcher.SparkAppHandle",
			"org.apache.spark.launcher.SparkClassCommandBuilder",
			"org.apache.spark.launcher.SparkLauncher",
			"org.apache.spark.launcher.SparkSubmitCommandBuilder",
			"org.apache.spark.launcher.SparkSubmitOptionParser",
			"org.apache.spark.launcher.BaseSuite",
			"org.apache.spark.launcher.ChildProcAppHandleSuite",
			"org.apache.spark.launcher.CommandBuilderUtilsSuite",
			"org.apache.spark.launcher.InProcessLauncherSuite",
			"org.apache.spark.launcher.LauncherServerSuite",
			"org.apache.spark.launcher.SparkSubmitCommandBuilderSuite",
			"org.apache.spark.launcher.SparkSubmitOptionParserSuite",
			"org.apache.spark.mllib.JavaPackage",
			"org.apache.spark.SharedSparkSession",
			"org.apache.spark.ml.JavaPipelineSuite",
			"org.apache.spark.ml.attribute.JavaAttributeGroupSuite",
			"org.apache.spark.ml.attribute.JavaAttributeSuite",
			"org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite",
			"org.apache.spark.ml.classification.JavaGBTClassifierSuite",
			"org.apache.spark.ml.classification.JavaLogisticRegressionSuite",
			"org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite",
			"org.apache.spark.ml.classification.JavaNaiveBayesSuite",
			"org.apache.spark.ml.classification.JavaOneVsRestSuite",
			"org.apache.spark.ml.classification.JavaRandomForestClassifierSuite",
			"org.apache.spark.ml.clustering.JavaKMeansSuite",
			"org.apache.spark.ml.feature.JavaBucketizerSuite",
			"org.apache.spark.ml.feature.JavaDCTSuite",
			"org.apache.spark.ml.feature.JavaHashingTFSuite",
			"org.apache.spark.ml.feature.JavaNormalizerSuite",
			"org.apache.spark.ml.feature.JavaPCASuite",
			"org.apache.spark.ml.feature.JavaPolynomialExpansionSuite",
			"org.apache.spark.ml.feature.JavaStandardScalerSuite",
			"org.apache.spark.ml.feature.JavaStopWordsRemoverSuite",
			"org.apache.spark.ml.feature.JavaStringIndexerSuite",
			"org.apache.spark.ml.feature.JavaTokenizerSuite",
			"org.apache.spark.ml.feature.JavaVectorAssemblerSuite",
			"org.apache.spark.ml.feature.JavaVectorIndexerSuite",
			"org.apache.spark.ml.feature.JavaVectorSlicerSuite",
			"org.apache.spark.ml.feature.JavaWord2VecSuite",
			"org.apache.spark.ml.linalg.JavaSQLDataTypesSuite",
			"org.apache.spark.ml.param.JavaParamsSuite",
			"org.apache.spark.ml.param.JavaTestParams",
			"org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite",
			"org.apache.spark.ml.regression.JavaGBTRegressorSuite",
			"org.apache.spark.ml.regression.JavaLinearRegressionSuite",
			"org.apache.spark.ml.regression.JavaRandomForestRegressorSuite",
			"org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite",
			"org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite",
			"org.apache.spark.ml.stat.JavaSummarizerSuite",
			"org.apache.spark.ml.tuning.JavaCrossValidatorSuite",
			"org.apache.spark.ml.util.JavaDefaultReadWriteSuite",
			"org.apache.spark.mllib.classification.JavaLogisticRegressionSuite",
			"org.apache.spark.mllib.classification.JavaNaiveBayesSuite",
			"org.apache.spark.mllib.classification.JavaSVMSuite",
			"org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite",
			"org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite",
			"org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite",
			"org.apache.spark.mllib.clustering.JavaKMeansSuite",
			"org.apache.spark.mllib.clustering.JavaLDASuite",
			"org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite",
			"org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite",
			"org.apache.spark.mllib.feature.JavaTfIdfSuite",
			"org.apache.spark.mllib.feature.JavaWord2VecSuite",
			"org.apache.spark.mllib.fpm.JavaAssociationRulesSuite",
			"org.apache.spark.mllib.fpm.JavaFPGrowthSuite",
			"org.apache.spark.mllib.fpm.JavaPrefixSpanSuite",
			"org.apache.spark.mllib.linalg.JavaMatricesSuite",
			"org.apache.spark.mllib.linalg.JavaVectorsSuite",
			"org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite",
			"org.apache.spark.mllib.random.JavaRandomRDDsSuite",
			"org.apache.spark.mllib.random.StringGenerator",
			"org.apache.spark.mllib.recommendation.JavaALSSuite",
			"org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite",
			"org.apache.spark.mllib.regression.JavaLassoSuite",
			"org.apache.spark.mllib.regression.JavaLinearRegressionSuite",
			"org.apache.spark.mllib.regression.JavaRidgeRegressionSuite",
			"org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite",
			"org.apache.spark.mllib.stat.JavaStatisticsSuite",
			"org.apache.spark.mllib.tree.JavaDecisionTreeSuite",
			"org.apache.spark.mllib.util.JavaMLUtilsSuite",
			"org.apache.spark.network.shuffle.mesos.MesosExternalBlockStoreClient",
			"org.apache.hadoop.net.ServerSocketUtil",
			"org.eclipse.jetty.server.SessionManager",
			"org.eclipse.jetty.server.session.SessionHandler",
			"org.apache.spark.sql.RowFactory",
			"org.apache.spark.sql.catalyst.expressions.ExpressionDescription",
			"org.apache.spark.sql.catalyst.expressions.ExpressionImplUtils",
			"org.apache.spark.sql.catalyst.expressions.ExpressionInfo",
			"org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch",
			"org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch",
			"org.apache.spark.sql.catalyst.expressions.SpecializedGetters",
			"org.apache.spark.sql.catalyst.expressions.SpecializedGettersReader",
			"org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeDataUtils",
			"org.apache.spark.sql.catalyst.expressions.UnsafeMapData",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow",
			"org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch",
			"org.apache.spark.sql.catalyst.expressions.XXH64",
			"org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder",
			"org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter",
			"org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter",
			"org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter",
			"org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtil",
			"org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils",
			"org.apache.spark.sql.connector.ExternalCommandRunner",
			"org.apache.spark.sql.connector.catalog.CatalogExtension",
			"org.apache.spark.sql.connector.catalog.CatalogPlugin",
			"org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension",
			"org.apache.spark.sql.connector.catalog.FunctionCatalog",
			"org.apache.spark.sql.connector.catalog.Identifier",
			"org.apache.spark.sql.connector.catalog.IdentifierImpl",
			"org.apache.spark.sql.connector.catalog.MetadataColumn",
			"org.apache.spark.sql.connector.catalog.NamespaceChange",
			"org.apache.spark.sql.connector.catalog.SessionConfigSupport",
			"org.apache.spark.sql.connector.catalog.StagedTable",
			"org.apache.spark.sql.connector.catalog.StagingTableCatalog",
			"org.apache.spark.sql.connector.catalog.SupportsAtomicPartitionManagement",
			"org.apache.spark.sql.connector.catalog.SupportsCatalogOptions",
			"org.apache.spark.sql.connector.catalog.SupportsDelete",
			"org.apache.spark.sql.connector.catalog.SupportsMetadataColumns",
			"org.apache.spark.sql.connector.catalog.SupportsNamespaces",
			"org.apache.spark.sql.connector.catalog.SupportsPartitionManagement",
			"org.apache.spark.sql.connector.catalog.SupportsRead",
			"org.apache.spark.sql.connector.catalog.SupportsRowLevelOperations",
			"org.apache.spark.sql.connector.catalog.SupportsWrite",
			"org.apache.spark.sql.connector.catalog.Table",
			"org.apache.spark.sql.connector.catalog.TableCapability",
			"org.apache.spark.sql.connector.catalog.TableCatalog",
			"org.apache.spark.sql.connector.catalog.TableChange",
			"org.apache.spark.sql.connector.catalog.TableProvider",
			"org.apache.spark.sql.connector.catalog.TruncatableTable",
			"org.apache.spark.sql.connector.catalog.functions.AggregateFunction",
			"org.apache.spark.sql.connector.catalog.functions.BoundFunction",
			"org.apache.spark.sql.connector.catalog.functions.Function",
			"org.apache.spark.sql.connector.catalog.functions.ScalarFunction",
			"org.apache.spark.sql.connector.catalog.functions.UnboundFunction",
			"org.apache.spark.sql.connector.catalog.index.SupportsIndex",
			"org.apache.spark.sql.connector.catalog.index.TableIndex",
			"org.apache.spark.sql.connector.distributions.ClusteredDistribution",
			"org.apache.spark.sql.connector.distributions.Distribution",
			"org.apache.spark.sql.connector.distributions.Distributions",
			"org.apache.spark.sql.connector.distributions.OrderedDistribution",
			"org.apache.spark.sql.connector.distributions.UnspecifiedDistribution",
			"org.apache.spark.sql.connector.expressions.Cast",
			"org.apache.spark.sql.connector.expressions.Expression",
			"org.apache.spark.sql.connector.expressions.Expressions",
			"org.apache.spark.sql.connector.expressions.GeneralScalarExpression",
			"org.apache.spark.sql.connector.expressions.Literal",
			"org.apache.spark.sql.connector.expressions.NamedReference",
			"org.apache.spark.sql.connector.expressions.NullOrdering",
			"org.apache.spark.sql.connector.expressions.SortDirection",
			"org.apache.spark.sql.connector.expressions.SortOrder",
			"org.apache.spark.sql.connector.expressions.Transform",
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc",
			"org.apache.spark.sql.connector.expressions.aggregate.Aggregation",
			"org.apache.spark.sql.connector.expressions.aggregate.Avg",
			"org.apache.spark.sql.connector.expressions.aggregate.Count",
			"org.apache.spark.sql.connector.expressions.aggregate.CountStar",
			"org.apache.spark.sql.connector.expressions.aggregate.GeneralAggregateFunc",
			"org.apache.spark.sql.connector.expressions.aggregate.Max",
			"org.apache.spark.sql.connector.expressions.aggregate.Min",
			"org.apache.spark.sql.connector.expressions.aggregate.Sum",
			"org.apache.spark.sql.connector.expressions.filter.AlwaysFalse",
			"org.apache.spark.sql.connector.expressions.filter.AlwaysTrue",
			"org.apache.spark.sql.connector.expressions.filter.And",
			"org.apache.spark.sql.connector.expressions.filter.Not",
			"org.apache.spark.sql.connector.expressions.filter.Or",
			"org.apache.spark.sql.connector.expressions.filter.Predicate",
			"org.apache.spark.sql.connector.metric.CustomAvgMetric",
			"org.apache.spark.sql.connector.metric.CustomMetric",
			"org.apache.spark.sql.connector.metric.CustomSumMetric",
			"org.apache.spark.sql.connector.metric.CustomTaskMetric",
			"org.apache.spark.sql.connector.read.Batch",
			"org.apache.spark.sql.connector.read.HasPartitionKey",
			"org.apache.spark.sql.connector.read.InputPartition",
			"org.apache.spark.sql.connector.read.LocalScan",
			"org.apache.spark.sql.connector.read.PartitionReader",
			"org.apache.spark.sql.connector.read.PartitionReaderFactory",
			"org.apache.spark.sql.connector.read.Scan",
			"org.apache.spark.sql.connector.read.ScanBuilder",
			"org.apache.spark.sql.connector.read.Statistics",
			"org.apache.spark.sql.connector.read.SupportsPushDownAggregates",
			"org.apache.spark.sql.connector.read.SupportsPushDownFilters",
			"org.apache.spark.sql.connector.read.SupportsPushDownLimit",
			"org.apache.spark.sql.connector.read.SupportsPushDownRequiredColumns",
			"org.apache.spark.sql.connector.read.SupportsPushDownTableSample",
			"org.apache.spark.sql.connector.read.SupportsPushDownTopN",
			"org.apache.spark.sql.connector.read.SupportsPushDownV2Filters",
			"org.apache.spark.sql.connector.read.SupportsReportPartitioning",
			"org.apache.spark.sql.connector.read.SupportsReportStatistics",
			"org.apache.spark.sql.connector.read.SupportsRuntimeFiltering",
			"org.apache.spark.sql.connector.read.partitioning.KeyGroupedPartitioning",
			"org.apache.spark.sql.connector.read.partitioning.Partitioning",
			"org.apache.spark.sql.connector.read.partitioning.UnknownPartitioning",
			"org.apache.spark.sql.connector.read.streaming.AcceptsLatestSeenOffset",
			"org.apache.spark.sql.connector.read.streaming.CompositeReadLimit",
			"org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader",
			"org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory",
			"org.apache.spark.sql.connector.read.streaming.ContinuousStream",
			"org.apache.spark.sql.connector.read.streaming.MicroBatchStream",
			"org.apache.spark.sql.connector.read.streaming.Offset",
			"org.apache.spark.sql.connector.read.streaming.PartitionOffset",
			"org.apache.spark.sql.connector.read.streaming.ReadAllAvailable",
			"org.apache.spark.sql.connector.read.streaming.ReadLimit",
			"org.apache.spark.sql.connector.read.streaming.ReadMaxFiles",
			"org.apache.spark.sql.connector.read.streaming.ReadMaxRows",
			"org.apache.spark.sql.connector.read.streaming.ReadMinRows",
			"org.apache.spark.sql.connector.read.streaming.ReportsSinkMetrics",
			"org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics",
			"org.apache.spark.sql.connector.read.streaming.SparkDataStream",
			"org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl",
			"org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow",
			"org.apache.spark.sql.connector.util.V2ExpressionSQLBuilder",
			"org.apache.spark.sql.connector.write.BatchWrite",
			"org.apache.spark.sql.connector.write.DataWriter",
			"org.apache.spark.sql.connector.write.DataWriterFactory",
			"org.apache.spark.sql.connector.write.LogicalWriteInfo",
			"org.apache.spark.sql.connector.write.PhysicalWriteInfo",
			"org.apache.spark.sql.connector.write.RequiresDistributionAndOrdering",
			"org.apache.spark.sql.connector.write.RowLevelOperation",
			"org.apache.spark.sql.connector.write.RowLevelOperationBuilder",
			"org.apache.spark.sql.connector.write.RowLevelOperationInfo",
			"org.apache.spark.sql.connector.write.SupportsDynamicOverwrite",
			"org.apache.spark.sql.connector.write.SupportsOverwrite",
			"org.apache.spark.sql.connector.write.SupportsTruncate",
			"org.apache.spark.sql.connector.write.Write",
			"org.apache.spark.sql.connector.write.WriteBuilder",
			"org.apache.spark.sql.connector.write.WriterCommitMessage",
			"org.apache.spark.sql.connector.write.streaming.StreamingDataWriterFactory",
			"org.apache.spark.sql.connector.write.streaming.StreamingWrite",
			"org.apache.spark.sql.streaming.GroupStateTimeout",
			"org.apache.spark.sql.streaming.OutputMode",
			"org.apache.spark.sql.types.DataTypes",
			"org.apache.spark.sql.types.SQLUserDefinedType",
			"org.apache.spark.sql.util.CaseInsensitiveStringMap",
			"org.apache.spark.sql.util.NumericHistogram",
			"org.apache.spark.sql.vectorized.ArrowColumnVector",
			"org.apache.spark.sql.vectorized.ColumnVector",
			"org.apache.spark.sql.vectorized.ColumnarArray",
			"org.apache.spark.sql.vectorized.ColumnarBatch",
			"org.apache.spark.sql.vectorized.ColumnarBatchRow",
			"org.apache.spark.sql.vectorized.ColumnarMap",
			"org.apache.spark.sql.vectorized.ColumnarRow",
			"org.apache.spark.sql.catalyst.expressions.HiveHasherSuite",
			"org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite",
			"org.apache.spark.sql.catalyst.expressions.XXH64Suite",
			"org.apache.spark.sql.connector.catalog.CatalogLoadingSuite",
			"org.apache.spark.sql.connector.catalog.TestCatalogPlugin",
			"org.apache.spark.sql.connector.catalog.ConstructorFailureCatalogPlugin",
			"org.apache.spark.sql.connector.catalog.AccessErrorCatalogPlugin",
			"org.apache.spark.sql.connector.catalog.InvalidCatalogPlugin",
			"org.apache.spark.sql.streaming.JavaGroupStateTimeoutSuite",
			"org.apache.spark.sql.streaming.JavaOutputModeSuite",
			"org.apache.parquet.filter2.predicate.SparkFilterApi",
			"org.apache.parquet.io.ColumnIOUtil",
			"org.apache.spark.api.java.function.FlatMapGroupsWithStateFunction",
			"org.apache.spark.api.java.function.MapGroupsWithStateFunction",
			"org.apache.spark.sql.SaveMode",
			"org.apache.spark.sql.api.java.UDF0",
			"org.apache.spark.sql.api.java.UDF1",
			"org.apache.spark.sql.api.java.UDF10",
			"org.apache.spark.sql.api.java.UDF11",
			"org.apache.spark.sql.api.java.UDF12",
			"org.apache.spark.sql.api.java.UDF13",
			"org.apache.spark.sql.api.java.UDF14",
			"org.apache.spark.sql.api.java.UDF15",
			"org.apache.spark.sql.api.java.UDF16",
			"org.apache.spark.sql.api.java.UDF17",
			"org.apache.spark.sql.api.java.UDF18",
			"org.apache.spark.sql.api.java.UDF19",
			"org.apache.spark.sql.api.java.UDF2",
			"org.apache.spark.sql.api.java.UDF20",
			"org.apache.spark.sql.api.java.UDF21",
			"org.apache.spark.sql.api.java.UDF22",
			"org.apache.spark.sql.api.java.UDF3",
			"org.apache.spark.sql.api.java.UDF4",
			"org.apache.spark.sql.api.java.UDF5",
			"org.apache.spark.sql.api.java.UDF6",
			"org.apache.spark.sql.api.java.UDF7",
			"org.apache.spark.sql.api.java.UDF8",
			"org.apache.spark.sql.api.java.UDF9",
			"org.apache.spark.sql.connector.read.V1Scan",
			"org.apache.spark.sql.connector.write.V1Write",
			"org.apache.spark.sql.execution.BufferedRowIterator",
			"org.apache.spark.sql.execution.RecordBinaryComparator",
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter",
			"org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap",
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter",
			"org.apache.spark.sql.execution.columnar.ColumnDictionary",
			"org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException",
			"org.apache.spark.sql.execution.datasources.orc.OrcArrayColumnVector",
			"org.apache.spark.sql.execution.datasources.orc.OrcAtomicColumnVector",
			"org.apache.spark.sql.execution.datasources.orc.OrcColumnStatistics",
			"org.apache.spark.sql.execution.datasources.orc.OrcColumnVector",
			"org.apache.spark.sql.execution.datasources.orc.OrcColumnVectorUtils",
			"org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader",
			"org.apache.spark.sql.execution.datasources.orc.OrcFooterReader",
			"org.apache.spark.sql.execution.datasources.orc.OrcMapColumnVector",
			"org.apache.spark.sql.execution.datasources.orc.OrcStructColumnVector",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetColumnVector",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetLogRedirector",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetReadState",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory",
			"org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaBinaryPackedReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaByteArrayReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaLengthByteArrayReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedReaderBase",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedValuesReader",
			"org.apache.spark.sql.execution.streaming.Offset",
			"org.apache.spark.sql.execution.vectorized.AggregateHashMap",
			"org.apache.spark.sql.execution.vectorized.ColumnVectorUtils",
			"org.apache.spark.sql.execution.vectorized.ConstantColumnVector",
			"org.apache.spark.sql.execution.vectorized.Dictionary",
			"org.apache.spark.sql.execution.vectorized.MutableColumnarRow",
			"org.apache.spark.sql.execution.vectorized.OffHeapColumnVector",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector",
			"org.apache.spark.sql.execution.vectorized.WritableColumnVector",
			"org.apache.spark.sql.expressions.javalang.typed",
			"org.apache.spark.sql.internal.NonClosableMutableURLClassLoader",
			"org.apache.spark.sql.streaming.Trigger",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.CompatibilityTest",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum",
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.Suit",
			"org.apache.spark.sql.api.java.UDF23Test",
			"org.apache.parquet.column.page.TestDataPage",
			"test.org.apache.spark.sql.Java8DatasetAggregatorSuite",
			"test.org.apache.spark.sql.JavaApplySchemaSuite",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite",
			"test.org.apache.spark.sql.JavaColumnExpressionSuite",
			"test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite",
			"test.org.apache.spark.sql.JavaDataFrameSuite",
			"test.org.apache.spark.sql.JavaDataFrameWriterV2Suite",
			"test.org.apache.spark.sql.JavaDatasetAggregatorSuite",
			"test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase",
			"test.org.apache.spark.sql.JavaDatasetSuite",
			"test.org.apache.spark.sql.JavaHigherOrderFunctionsSuite",
			"test.org.apache.spark.sql.JavaRowSuite",
			"test.org.apache.spark.sql.JavaSaveLoadSuite",
			"test.org.apache.spark.sql.JavaStringLength",
			"test.org.apache.spark.sql.JavaUDAFSuite",
			"test.org.apache.spark.sql.JavaUDFSuite",
			"test.org.apache.spark.sql.MyDoubleAvg",
			"test.org.apache.spark.sql.MyDoubleSum",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource",
			"test.org.apache.spark.sql.connector.JavaRangeInputPartition",
			"test.org.apache.spark.sql.connector.JavaReportStatisticsDataSource",
			"test.org.apache.spark.sql.connector.JavaSchemaRequiredDataSource",
			"test.org.apache.spark.sql.connector.JavaSimpleBatchTable",
			"test.org.apache.spark.sql.connector.JavaSimpleDataSourceV2",
			"test.org.apache.spark.sql.connector.JavaSimpleReaderFactory",
			"test.org.apache.spark.sql.connector.JavaSimpleScanBuilder",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaAverage",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen",
			"test.org.apache.spark.sql.execution.datasources.orc.FakeKeyProvider",
			"test.org.apache.spark.sql.execution.sort.RecordBinaryComparatorSuite",
			"test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite",
			"org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader",
			"org.apache.spark.sql.hive.JavaDataFrameSuite",
			"org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite",
			"org.apache.spark.sql.hive.execution.UDAFEmpty",
			"org.apache.spark.sql.hive.execution.UDFIntegerToString",
			"org.apache.spark.sql.hive.execution.UDFListListInt",
			"org.apache.spark.sql.hive.execution.UDFListString",
			"org.apache.spark.sql.hive.execution.UDFRawList",
			"org.apache.spark.sql.hive.execution.UDFRawMap",
			"org.apache.spark.sql.hive.execution.UDFStringString",
			"org.apache.spark.sql.hive.execution.UDFToIntIntMap",
			"org.apache.spark.sql.hive.execution.UDFToListInt",
			"org.apache.spark.sql.hive.execution.UDFToListMapStringListInt",
			"org.apache.spark.sql.hive.execution.UDFToListString",
			"org.apache.spark.sql.hive.execution.UDFToStringIntMap",
			"org.apache.spark.sql.hive.execution.UDFTwoListList",
			"org.apache.spark.sql.hive.execution.UDFWildcardList",
			"org.apache.spark.sql.hive.test.Complex",
			"org.apache.hive.service.AbstractService",
			"org.apache.hive.service.BreakableService",
			"org.apache.hive.service.CompositeService",
			"org.apache.hive.service.CookieSigner",
			"org.apache.hive.service.FilterService",
			"org.apache.hive.service.Service",
			"org.apache.hive.service.ServiceException",
			"org.apache.hive.service.ServiceOperations",
			"org.apache.hive.service.ServiceStateChangeListener",
			"org.apache.hive.service.ServiceUtils",
			"org.apache.hive.service.auth.AnonymousAuthenticationProviderImpl",
			"org.apache.hive.service.auth.AuthenticationProviderFactory",
			"org.apache.hive.service.auth.CustomAuthenticationProviderImpl",
			"org.apache.hive.service.auth.HiveAuthFactory",
			"org.apache.hive.service.auth.HttpAuthUtils",
			"org.apache.hive.service.auth.HttpAuthenticationException",
			"org.apache.hive.service.auth.KerberosSaslHelper",
			"org.apache.hive.service.auth.LdapAuthenticationProviderImpl",
			"org.apache.hive.service.auth.PamAuthenticationProviderImpl",
			"org.apache.hive.service.auth.PasswdAuthenticationProvider",
			"org.apache.hive.service.auth.PlainSaslHelper",
			"org.apache.hive.service.auth.PlainSaslServer",
			"org.apache.hive.service.auth.SaslQOP",
			"org.apache.hive.service.auth.TSetIpAddressProcessor",
			"org.apache.hive.service.auth.TSubjectAssumingTransport",
			"org.apache.hive.service.cli.CLIService",
			"org.apache.hive.service.cli.CLIServiceClient",
			"org.apache.hive.service.cli.CLIServiceUtils",
			"org.apache.hive.service.cli.ColumnBasedSet",
			"org.apache.hive.service.cli.ColumnDescriptor",
			"org.apache.hive.service.cli.ColumnValue",
			"org.apache.hive.service.cli.FetchOrientation",
			"org.apache.hive.service.cli.FetchType",
			"org.apache.hive.service.cli.GetInfoType",
			"org.apache.hive.service.cli.GetInfoValue",
			"org.apache.hive.service.cli.Handle",
			"org.apache.hive.service.cli.HandleIdentifier",
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.ICLIService",
			"org.apache.hive.service.cli.OperationHandle",
			"org.apache.hive.service.cli.OperationState",
			"org.apache.hive.service.cli.OperationStatus",
			"org.apache.hive.service.cli.OperationType",
			"org.apache.hive.service.cli.RowBasedSet",
			"org.apache.hive.service.cli.RowSet",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.SessionHandle",
			"org.apache.hive.service.cli.TableSchema",
			"org.apache.hive.service.cli.TypeDescriptor",
			"org.apache.hive.service.cli.TypeQualifiers",
			"org.apache.hive.service.cli.operation.ClassicTableTypeMapping",
			"org.apache.hive.service.cli.operation.ExecuteStatementOperation",
			"org.apache.hive.service.cli.operation.GetCatalogsOperation",
			"org.apache.hive.service.cli.operation.GetColumnsOperation",
			"org.apache.hive.service.cli.operation.GetCrossReferenceOperation",
			"org.apache.hive.service.cli.operation.GetFunctionsOperation",
			"org.apache.hive.service.cli.operation.GetPrimaryKeysOperation",
			"org.apache.hive.service.cli.operation.GetSchemasOperation",
			"org.apache.hive.service.cli.operation.GetTableTypesOperation",
			"org.apache.hive.service.cli.operation.GetTablesOperation",
			"org.apache.hive.service.cli.operation.GetTypeInfoOperation",
			"org.apache.hive.service.cli.operation.HiveCommandOperation",
			"org.apache.hive.service.cli.operation.HiveTableTypeMapping",
			"org.apache.hive.service.cli.operation.LogDivertAppender",
			"org.apache.hive.service.cli.operation.MetadataOperation",
			"org.apache.hive.service.cli.operation.Operation",
			"org.apache.hive.service.cli.operation.OperationManager",
			"org.apache.hive.service.cli.operation.SQLOperation",
			"org.apache.hive.service.cli.operation.TableTypeMapping",
			"org.apache.hive.service.cli.operation.TableTypeMappingFactory",
			"org.apache.hive.service.cli.session.HiveSession",
			"org.apache.hive.service.cli.session.HiveSessionBase",
			"org.apache.hive.service.cli.session.HiveSessionHookContext",
			"org.apache.hive.service.cli.session.HiveSessionHookContextImpl",
			"org.apache.hive.service.cli.session.HiveSessionImpl",
			"org.apache.hive.service.cli.session.HiveSessionImplwithUGI",
			"org.apache.hive.service.cli.session.HiveSessionProxy",
			"org.apache.hive.service.cli.session.SessionManager",
			"org.apache.hive.service.cli.thrift.ThriftBinaryCLIService",
			"org.apache.hive.service.cli.thrift.ThriftCLIService",
			"org.apache.hive.service.cli.thrift.ThriftCLIServiceClient",
			"org.apache.hive.service.cli.thrift.ThriftHttpCLIService",
			"org.apache.hive.service.cli.thrift.ThriftHttpServlet",
			"org.apache.hive.service.server.HiveServer2",
			"org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup",
			"org.apache.hive.service.server.ThreadWithGarbageCleanup",
			"org.apache.spark.status.api.v1.streaming.BatchStatus",
			"org.apache.spark.streaming.StreamingContextState",
			"org.apache.spark.streaming.util.WriteAheadLog",
			"org.apache.spark.streaming.util.WriteAheadLogRecordHandle",
			"org.apache.spark.streaming.JavaDurationSuite",
			"org.apache.spark.streaming.JavaMapWithStateSuite",
			"org.apache.spark.streaming.JavaReceiverAPISuite",
			"org.apache.spark.streaming.JavaStreamingListenerAPISuite",
			"org.apache.spark.streaming.JavaTimeSuite",
			"org.apache.spark.streaming.JavaWriteAheadLogSuite",
			"org.apache.spark.streaming.LocalJavaStreamingContext",
			"test.org.apache.spark.streaming.Java8APISuite",
			"test.org.apache.spark.streaming.JavaAPISuite"
		]
	},
	{
		"documentation": "/**\n * Sorts records using an AlphaSort-style key-prefix sort. This sort stores pointers to records\n * alongside a user-defined prefix of the record's sorting key. When the underlying sort algorithm\n * compares records, it will first compare the stored key prefixes; if the prefixes are not equal,\n * then we do not need to traverse the record pointers to compare the actual records. Avoiding these\n * random memory accesses improves cache hit rates.\n */",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter",
		"extends": "",
		"Methods": [
			{
				"signature": "public UnsafeInMemorySorter(\n    final MemoryConsumer consumer,\n    final TaskMemoryManager memoryManager,\n    final RecordComparator recordComparator,\n    final PrefixComparator prefixComparator,\n    int initialSize,\n    boolean canUseRadixSort)",
				"documentation": "/**\n   * If sorting with radix sort, specifies the starting position in the sort buffer where records\n   * with non-null prefixes are kept. Positions [0..nullBoundaryPos) will contain null-prefixed\n   * records, and positions [nullBoundaryPos..pos) non-null prefixed records. This lets us avoid\n   * radix sorting over null values.\n   */"
			},
			{
				"signature": "public UnsafeInMemorySorter(\n      final MemoryConsumer consumer,\n      final TaskMemoryManager memoryManager,\n      final RecordComparator recordComparator,\n      final PrefixComparator prefixComparator,\n      LongArray array,\n      boolean canUseRadixSort)",
				"documentation": ""
			},
			{
				"signature": "private int getUsableCapacity()",
				"documentation": ""
			},
			{
				"signature": "public long getInitialSize()",
				"documentation": ""
			},
			{
				"signature": "public void freeMemory()",
				"documentation": "/**\n   * Free the memory used by pointer array.\n   */"
			},
			{
				"signature": "public int numRecords()",
				"documentation": "/**\n   * @return the number of records that have been inserted into this sorter.\n   */"
			},
			{
				"signature": "public long getSortTimeNanos()",
				"documentation": "/**\n   * @return the total amount of time spent sorting data (in-memory only).\n   */"
			},
			{
				"signature": "public long getMemoryUsage()",
				"documentation": ""
			},
			{
				"signature": "public boolean hasSpaceForAnotherRecord()",
				"documentation": ""
			},
			{
				"signature": "public void expandPointerArray(LongArray newArray)",
				"documentation": ""
			},
			{
				"signature": "public void insertRecord(long recordPointer, long keyPrefix, boolean prefixIsNull)",
				"documentation": "/**\n   * Inserts a record to be sorted. Assumes that the record pointer points to a record length\n   * stored as a uaoSize(4 or 8) bytes integer, followed by the record's bytes.\n   *\n   * @param recordPointer pointer to a record in a data page, encoded by {@link TaskMemoryManager}.\n   * @param keyPrefix a user-defined key prefix\n   */"
			},
			{
				"signature": "public UnsafeSorterIterator getSortedIterator()",
				"documentation": "/**\n   * Return an iterator over record pointers in sorted order. For efficiency, all calls to\n   * {@code next()} will return the same mutable object.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.UnsafeAlignedOffset",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.SortComparator"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.SortComparator",
			"org.apache.spark.util.collection.unsafe.sort.SortedIterator"
		]
	},
	{
		"documentation": "/**\n * Sorts records using an AlphaSort-style key-prefix sort. This sort stores pointers to records\n * alongside a user-defined prefix of the record's sorting key. When the underlying sort algorithm\n * compares records, it will first compare the stored key prefixes; if the prefixes are not equal,\n * then we do not need to traverse the record pointers to compare the actual records. Avoiding these\n * random memory accesses improves cache hit rates.\n */",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.SortComparator",
		"extends": "",
		"Methods": [
			{
				"signature": "SortComparator(\n        RecordComparator recordComparator,\n        PrefixComparator prefixComparator,\n        TaskMemoryManager memoryManager)",
				"documentation": "/**\n * Sorts records using an AlphaSort-style key-prefix sort. This sort stores pointers to records\n * alongside a user-defined prefix of the record's sorting key. When the underlying sort algorithm\n * compares records, it will first compare the stored key prefixes; if the prefixes are not equal,\n * then we do not need to traverse the record pointers to compare the actual records. Avoiding these\n * random memory accesses improves cache hit rates.\n */"
			},
			{
				"signature": "@Override\n    public int compare(RecordPointerAndKeyPrefix r1, RecordPointerAndKeyPrefix r2)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Comparator"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.UnsafeAlignedOffset"
		],
		"usedBy": [
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.SortedIterator",
		"extends": "org.apache.spark.util.collection.unsafe.sort.UnsafeSorterIterator",
		"Methods": [
			{
				"signature": "private SortedIterator(int numRecords, int offset)",
				"documentation": ""
			},
			{
				"signature": "public SortedIterator clone()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int getNumRecords()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void loadNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Object getBaseObject()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long getBaseOffset()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long getCurrentPageNumber()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int getRecordLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long getKeyPrefix()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Cloneable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Supports sorting an array of (record pointer, key prefix) pairs.\n * Used in {@link UnsafeInMemorySorter}.\n * \u003cp\u003e\n * Within each long[] buffer, position {@code 2 * i} holds a pointer to the record at\n * index {@code i}, while position {@code 2 * i + 1} in the array holds an 8-byte key prefix.\n */",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeSortDataFormat",
		"extends": "org.apache.spark.util.collection.SortDataFormat",
		"Methods": [
			{
				"signature": "public UnsafeSortDataFormat(LongArray buffer)",
				"documentation": "/**\n * Supports sorting an array of (record pointer, key prefix) pairs.\n * Used in {@link UnsafeInMemorySorter}.\n * \u003cp\u003e\n * Within each long[] buffer, position {@code 2 * i} holds a pointer to the record at\n * index {@code i}, while position {@code 2 * i + 1} in the array holds an 8-byte key prefix.\n */"
			},
			{
				"signature": "@Override\n  public RecordPointerAndKeyPrefix getKey(LongArray data, int pos)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RecordPointerAndKeyPrefix newKey()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RecordPointerAndKeyPrefix getKey(LongArray data, int pos,\n                                          RecordPointerAndKeyPrefix reuse)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void swap(LongArray data, int pos0, int pos1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void copyElement(LongArray src, int srcPos, LongArray dst, int dstPos)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void copyRange(LongArray src, int srcPos, LongArray dst, int dstPos, int length)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public LongArray allocate(int length)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeSorterIterator",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.collection.unsafe.sort.SortedIterator",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger",
		"extends": "",
		"Methods": [
			{
				"signature": "UnsafeSorterSpillMerger(\n      RecordComparator recordComparator,\n      PrefixComparator prefixComparator,\n      int numSpills)",
				"documentation": ""
			},
			{
				"signature": "public void addSpillIfNotEmpty(UnsafeSorterIterator spillReader) throws IOException",
				"documentation": "/**\n   * Add an UnsafeSorterIterator to this merger\n   */"
			},
			{
				"signature": "public UnsafeSorterIterator getSortedIterator() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public int getNumRecords()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public long getCurrentPageNumber()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void loadNext() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public Object getBaseObject()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public long getBaseOffset()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public int getRecordLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public long getKeyPrefix()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Reads spill files written by {@link UnsafeSorterSpillWriter} (see that class for a description\n * of the file format).\n */",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader",
		"extends": "org.apache.spark.util.collection.unsafe.sort.UnsafeSorterIterator",
		"Methods": [
			{
				"signature": "public UnsafeSorterSpillReader(\n      SerializerManager serializerManager,\n      File file,\n      BlockId blockId) throws IOException",
				"documentation": "/**\n * Reads spill files written by {@link UnsafeSorterSpillWriter} (see that class for a description\n * of the file format).\n */"
			},
			{
				"signature": "@Override\n  public int getNumRecords()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getCurrentPageNumber()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void loadNext() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object getBaseObject()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getBaseOffset()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getRecordLength()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getKeyPrefix()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.io.NioBufferedFileInputStream",
			"org.apache.spark.io.ReadAheadInputStream"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Spills a list of sorted records to disk. Spill files have the following format:\n *\n *   [# of records (int)] [[len (int)][prefix (long)][data (bytes)]...]\n */",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter",
		"extends": "",
		"Methods": [
			{
				"signature": "public UnsafeSorterSpillWriter(\n      BlockManager blockManager,\n      int fileBufferSize,\n      ShuffleWriteMetrics writeMetrics,\n      int numRecordsToWrite) throws IOException",
				"documentation": "/**\n   * The buffer size to use when writing the sorted records to an on-disk file, and\n   * this space used by prefix + len + recordLength must be greater than 4 + 8 bytes.\n   */"
			},
			{
				"signature": "private void writeLongToBuffer(long v, int offset)",
				"documentation": ""
			},
			{
				"signature": "private void writeIntToBuffer(int v, int offset)",
				"documentation": ""
			},
			{
				"signature": "public void write(\n      Object baseObject,\n      long baseOffset,\n      int recordLength,\n      long keyPrefix) throws IOException",
				"documentation": "/**\n   * Write a record to a spill file.\n   *\n   * @param baseObject the base object / memory page containing the record\n   * @param baseOffset the base offset which points directly to the record data.\n   * @param recordLength the length of the record.\n   * @param keyPrefix a sort key prefix\n   */"
			},
			{
				"signature": "public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "public File getFile()",
				"documentation": ""
			},
			{
				"signature": "public UnsafeSorterSpillReader getReader(SerializerManager serializerManager) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public int recordsSpilled()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [
			"org.apache.spark.unsafe.map.MapIterator"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.JavaJdbcRDDSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp() throws ClassNotFoundException, SQLException",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown() throws SQLException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJavaJdbcRDD() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Tests {@link Optional}.\n */",
		"name": "org.apache.spark.api.java.OptionalSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testEmpty()",
				"documentation": "/**\n * Tests {@link Optional}.\n */"
			},
			{
				"signature": "@Test\n  public void testEmptyGet()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAbsent()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAbsentGet()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOf()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOfWithNull()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOfNullable()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFromNullable()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Tests functionality of {@link NioBufferedFileInputStream}\n */",
		"name": "org.apache.spark.io.GenericFileInputStreamSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp() throws IOException",
				"documentation": "/**\n * Tests functionality of {@link NioBufferedFileInputStream}\n */"
			},
			{
				"signature": "@After\n  public void tearDown() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReadOneByte() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReadMultipleBytes() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBytesSkipped() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBytesSkippedAfterRead() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNegativeBytesSkippedAfterRead() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSkipFromFileChannel() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBytesSkippedAfterEOF() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReadPastEOF() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.io.NioBufferedInputStreamSuite",
			"org.apache.spark.io.ReadAheadInputStreamSuite"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Tests functionality of {@link NioBufferedFileInputStream}\n */",
		"name": "org.apache.spark.io.NioBufferedInputStreamSuite",
		"extends": "org.apache.spark.io.GenericFileInputStreamSuite",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp() throws IOException",
				"documentation": "/**\n * Tests functionality of {@link NioBufferedFileInputStream}\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Tests functionality of {@link ReadAheadInputStreamSuite}\n */",
		"name": "org.apache.spark.io.ReadAheadInputStreamSuite",
		"extends": "org.apache.spark.io.GenericFileInputStreamSuite",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp() throws IOException",
				"documentation": "/**\n * Tests functionality of {@link ReadAheadInputStreamSuite}\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * These tests require the Spark assembly to be built before they can be run.\n */",
		"name": "org.apache.spark.launcher.SparkLauncherSuite",
		"extends": "org.apache.spark.launcher.BaseSuite",
		"Methods": [
			{
				"signature": "@Test\n  public void testSparkArgumentHandling() throws Exception",
				"documentation": "/**\n * These tests require the Spark assembly to be built before they can be run.\n */"
			},
			{
				"signature": "@Test\n  public void testChildProcLauncher() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInProcessLauncher() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void inProcessLauncherTestImpl() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInProcessLauncherDoesNotKillJvm() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInProcessLauncherGetError() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSparkLauncherGetError() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void restoreSystemProperties(Map\u003cObject, Object\u003e properties)",
				"documentation": ""
			},
			{
				"signature": "private void waitForSparkContextShutdown() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.launcher.SparkLauncherSuite.SparkLauncherTestApp",
			"org.apache.spark.launcher.SparkLauncherSuite.InProcessTestApp",
			"org.apache.spark.launcher.SparkLauncherSuite.ErrorInProcessTestApp"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.SparkLauncherSuite.SparkLauncherTestApp",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.SparkLauncherSuite.InProcessTestApp",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n     * SPARK-23020: there's a race caused by a child app finishing too quickly. This would cause\n     * the InProcessAppHandle to dispose of itself even before the child connection was properly\n     * established, so no state changes would be detected for the application and its final\n     * state would be LOST.\n     *\n     * It's not really possible to fix that race safely in the handle code itself without changing\n     * the way in-process apps talk to the launcher library, so we work around that in the test by\n     * synchronizing on this object.\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Similar to {@link InProcessTestApp} except it throws an exception\n   */",
		"name": "org.apache.spark.launcher.SparkLauncherSuite.ErrorInProcessTestApp",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n   * Similar to {@link InProcessTestApp} except it throws an exception\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.memory.TaskMemoryManagerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void leakedPageMemoryIsDetected()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void encodePageNumberAndOffsetOffHeap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void encodePageNumberAndOffsetOnHeap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void freeingPageSetsPageNumberToSpecialConstant()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void freeingPageDirectlyInAllocatorTriggersAssertionError()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void callingFreePageOnDirectlyAllocatedPageTriggersAssertionError()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cooperativeSpilling()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cooperativeSpilling2()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void selfSpillIsLowestPriorities()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void prefersSmallestBigEnoughAllocation()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void shouldNotForceSpillingInDifferentModes()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void offHeapConfigurationBackwardsCompatibility()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.memory.TestMemoryConsumer",
		"extends": "org.apache.spark.memory.MemoryConsumer",
		"Methods": [
			{
				"signature": "public TestMemoryConsumer(TaskMemoryManager memoryManager, MemoryMode mode)",
				"documentation": ""
			},
			{
				"signature": "public TestMemoryConsumer(TaskMemoryManager memoryManager)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long spill(long size, MemoryConsumer trigger) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void use(long size)",
				"documentation": ""
			},
			{
				"signature": "public void free(long size)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public void freePage(MemoryBlock page)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.memory.TestPartialSpillingMemoryConsumer"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite",
			"org.apache.spark.unsafe.map.AbstractBytesToBytesMapSuite",
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite",
			"test.org.apache.spark.sql.execution.sort.RecordBinaryComparatorSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A TestMemoryConsumer which, when asked to spill, releases only enough memory to satisfy the\n * request rather than releasing all its memory.\n */",
		"name": "org.apache.spark.memory.TestPartialSpillingMemoryConsumer",
		"extends": "org.apache.spark.memory.TestMemoryConsumer",
		"Methods": [
			{
				"signature": "public TestPartialSpillingMemoryConsumer(TaskMemoryManager memoryManager, MemoryMode mode)",
				"documentation": "/**\n * A TestMemoryConsumer which, when asked to spill, releases only enough memory to satisfy the\n * request rather than releasing all its memory.\n */"
			},
			{
				"signature": "public TestPartialSpillingMemoryConsumer(TaskMemoryManager memoryManager)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long spill(long size, MemoryConsumer trigger) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public long getSpilledBytes()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.resource.JavaResourceProfileSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testResourceProfileAccessFromJava() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.serializer.ContainsProxyClass",
		"extends": "",
		"Methods": [],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.serializer.ContainsProxyClass.MyClass",
			"org.apache.spark.serializer.MyInvocationHandler"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.serializer.MyInterface",
			"org.apache.spark.serializer.ContainsProxyClass.MyClass",
			"org.apache.spark.serializer.MyInvocationHandler"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.serializer.MyInterface",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.serializer.ContainsProxyClass.MyClass"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.serializer.ContainsProxyClass.MyClass",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void myMethod()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.serializer.MyInterface",
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.serializer.ContainsProxyClass"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.serializer.MyInvocationHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.lang.reflect.InvocationHandler",
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.serializer.ContainsProxyClass"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A simple Serializer implementation to make sure the API is Java-friendly.\n */",
		"name": "org.apache.spark.serializer.TestJavaSerializerImpl",
		"extends": "Serializer",
		"Methods": [
			{
				"signature": "@Override\n  public SerializerInstance newInstance()",
				"documentation": "/**\n * A simple Serializer implementation to make sure the API is Java-friendly.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.serializer.TestJavaSerializerImpl.SerializerInstanceImpl",
			"org.apache.spark.serializer.TestJavaSerializerImpl.SerializationStreamImpl",
			"org.apache.spark.serializer.TestJavaSerializerImpl.DeserializationStreamImpl"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.serializer.TestJavaSerializerImpl.SerializerInstanceImpl",
		"extends": "SerializerInstance",
		"Methods": [
			{
				"signature": "@Override\n      public \u003cT\u003e ByteBuffer serialize(T t, ClassTag\u003cT\u003e evidence$1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public \u003cT\u003e T deserialize(ByteBuffer bytes, ClassLoader loader, ClassTag\u003cT\u003e evidence$1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public \u003cT\u003e T deserialize(ByteBuffer bytes, ClassTag\u003cT\u003e evidence$1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public SerializationStream serializeStream(OutputStream s)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DeserializationStream deserializeStream(InputStream s)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.serializer.TestJavaSerializerImpl.SerializationStreamImpl",
		"extends": "SerializationStream",
		"Methods": [
			{
				"signature": "@Override\n    public \u003cT\u003e SerializationStream writeObject(T t, ClassTag\u003cT\u003e evidence$1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void flush()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.serializer.TestJavaSerializerImpl.DeserializationStreamImpl",
		"extends": "DeserializationStream",
		"Methods": [
			{
				"signature": "@Override\n    public \u003cT\u003e T readObject(ClassTag\u003cT\u003e evidence$1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.PackedRecordPointerSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void heap() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void offHeap() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void maximumPartitionIdCanBeEncoded()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void partitionIdsGreaterThanMaximumPartitionIdWillOverflowOrTriggerError()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void maximumOffsetInPageCanBeEncoded()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void offsetsPastMaxOffsetInPageWillOverflow()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite",
		"extends": "org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite",
		"Methods": [
			{
				"signature": "@Override\n  protected boolean shouldUseRadixSort()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "protected boolean shouldUseRadixSort()",
				"documentation": ""
			},
			{
				"signature": "private static String getStringFromDataPage(Object baseObject, long baseOffset, int strLength)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSortingEmptyInput()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBasicSorting() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSortingManyNumbers() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.memory.TaskMemoryManager",
			"org.apache.spark.memory.TestMemoryConsumer"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void setUp() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private UnsafeShuffleWriter\u003cObject, Object\u003e createWriter(boolean transferToEnabled)\n    throws SparkException",
				"documentation": ""
			},
			{
				"signature": "private UnsafeShuffleWriter\u003cObject, Object\u003e createWriter(\n    boolean transferToEnabled,\n    IndexShuffleBlockResolver blockResolver) throws SparkException",
				"documentation": ""
			},
			{
				"signature": "private void assertSpillFilesWereCleanedUp()",
				"documentation": ""
			},
			{
				"signature": "private List\u003cTuple2\u003cObject, Object\u003e\u003e readRecordsFromFile() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mustCallWriteBeforeSuccessfulStop() throws IOException, SparkException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void doNotNeedToCallWriteBeforeUnsuccessfulStop() throws IOException, SparkException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeFailurePropagates() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeEmptyIterator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeWithoutSpilling() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeChecksumFileWithoutSpill() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeChecksumFileWithSpill() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void testMergingSpills(\n      final boolean transferToEnabled,\n      String compressionCodecName,\n      boolean encrypt) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void testMergingSpills(\n      boolean transferToEnabled,\n      boolean encrypted) throws IOException, SparkException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithTransferToAndLZF() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithFileStreamAndLZF() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithTransferToAndLZ4() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithFileStreamAndLZ4() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithTransferToAndSnappy() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithFileStreamAndSnappy() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithTransferToAndNoCompression() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithFileStreamAndNoCompression() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithCompressionAndEncryption() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithFileStreamAndCompressionAndEncryption() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithCompressionAndEncryptionSlowPath() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithEncryptionAndNoCompression() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mergeSpillsWithFileStreamAndEncryptionAndNoCompression() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeEnoughDataToTriggerSpill() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeEnoughRecordsToTriggerSortBufferExpansionAndSpillRadixOff() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeEnoughRecordsToTriggerSortBufferExpansionAndSpillRadixOn() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void writeEnoughRecordsToTriggerSortBufferExpansionAndSpill() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeRecordsThatAreBiggerThanDiskWriteBufferSize() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeRecordsThatAreBiggerThanMaxRecordSize() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void spillFilesAreDeletedWhenStoppingAfterError() throws IOException, SparkException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPeakMemoryUsed() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.shuffle.ShuffleChecksumTestHelper"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.LimitedInputStream",
			"org.apache.spark.network.shuffle.checksum.ShuffleChecksumHelper",
			"org.apache.spark.memory.TaskMemoryManager",
			"org.apache.spark.shuffle.sort.io.LocalDiskShuffleExecutorComponents",
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.PandaException",
			"org.apache.spark.shuffle.sort.BadRecords"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.PandaException",
			"org.apache.spark.shuffle.sort.BadRecords"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.PandaException",
		"extends": "RuntimeException",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.shuffle.sort.BadRecords",
		"extends": "scala.collection.AbstractIterator",
		"Methods": [
			{
				"signature": "@Override public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override public Product2\u003cObject, Object\u003e next()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.map.AbstractBytesToBytesMapSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setup() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "private static byte[] getByteArray(Object base, long offset, int size)",
				"documentation": ""
			},
			{
				"signature": "private byte[] getRandomByteArray(int numWords)",
				"documentation": ""
			},
			{
				"signature": "private static boolean arrayEquals(\n      byte[] expected,\n      Object base,\n      long offset,\n      long actualLengthBytes)",
				"documentation": "/**\n   * Fast equality checking for byte arrays, since these comparisons are a bottleneck\n   * in our stress tests.\n   */"
			},
			{
				"signature": "@Test\n  public void emptyMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void setAndRetrieveAKey()",
				"documentation": ""
			},
			{
				"signature": "private void iteratorTestBase(boolean destructive, boolean isWithKeyIndex) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void iteratorTest() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void destructiveIteratorTest() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void iteratorWithKeyIndexTest() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void iteratingOverDataPagesWithWastedSpace() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedTestWithRecordsLargerThanPageSize()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void failureToAllocateFirstPage()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void failureToGrow()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void spillInIterator() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void multipleValuesForSameKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void initialCapacityBoundsChecking()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPeakMemoryUsed()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void avoidDeadlock() throws InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void freeAfterFailedReset()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite",
			"org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.array.ByteArrayMethods",
			"org.apache.spark.memory.TestMemoryConsumer"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite",
		"extends": "org.apache.spark.unsafe.map.AbstractBytesToBytesMapSuite",
		"Methods": [
			{
				"signature": "@Override\n  protected boolean useOffHeapMemoryAllocator()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite",
		"extends": "org.apache.spark.unsafe.map.AbstractBytesToBytesMapSuite",
		"Methods": [
			{
				"signature": "@Override\n  protected boolean useOffHeapMemoryAllocator()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.SerializableConfigurationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSerializableConfiguration()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This codes generates a int array which fails the standard TimSort.\n *\n * The blog that reported the bug\n * http://www.envisage-project.eu/timsort-specification-and-verification/\n *\n * This codes was originally wrote by Stijn de Gouw, modified by Evan Yu to adapt to\n * our test suite.\n *\n * https://github.com/abstools/java-timsort-bug\n * https://github.com/abstools/java-timsort-bug/blob/master/LICENSE\n */",
		"name": "org.apache.spark.util.collection.TestTimSort",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int[] getTimSortBugTestSet(int length)",
				"documentation": "/**\n   * Returns an array of integers that demonstrate the bug in TimSort\n   */"
			},
			{
				"signature": "private static int minRunLength(int n)",
				"documentation": ""
			},
			{
				"signature": "private static int[] createArray(List\u003cLong\u003e runs, int length)",
				"documentation": ""
			},
			{
				"signature": "private static List\u003cLong\u003e runsJDKWorstCase(int minRun, int length)",
				"documentation": "/**\n   * Fills \u003ccode\u003eruns\u003c/code\u003e with a sequence of run lengths of the form\u003cbr\u003e\n   * Y_n     x_{n,1}   x_{n,2}   ... x_{n,l_n} \u003cbr\u003e\n   * Y_{n-1} x_{n-1,1} x_{n-1,2} ... x_{n-1,l_{n-1}} \u003cbr\u003e\n   * ... \u003cbr\u003e\n   * Y_1     x_{1,1}   x_{1,2}   ... x_{1,l_1}\u003cbr\u003e\n   * The Y_i's are chosen to satisfy the invariant throughout execution,\n   * but the x_{i,j}'s are merged (by \u003ccode\u003eTimSort.mergeCollapse\u003c/code\u003e)\n   * into an X_i that violates the invariant.\n   *\n   * @param length The sum of all run lengths that will be added to \u003ccode\u003eruns\u003c/code\u003e.\n   */"
			},
			{
				"signature": "private static void generateJDKWrongElem(List\u003cLong\u003e runs, int minRun, long X)",
				"documentation": "/**\n   * Adds a sequence x_1, ..., x_n of run lengths to \u003ccode\u003eruns\u003c/code\u003e such that:\u003cbr\u003e\n   * 1. X = x_1 + ... + x_n \u003cbr\u003e\n   * 2. x_j \u003e= minRun for all j \u003cbr\u003e\n   * 3. x_1 + ... + x_{j-2}  \u003c  x_j  \u003c  x_1 + ... + x_{j-1} for all j \u003cbr\u003e\n   * These conditions guarantee that TimSort merges all x_j's one by one\n   * (resulting in X) using only merges on the second-to-last element.\n   *\n   * @param X The sum of the sequence that should be added to runs.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite",
		"extends": "org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite",
		"Methods": [
			{
				"signature": "@Override\n  protected boolean shouldUseRadixSort()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public int compare(\n      Object leftBaseObject,\n      long leftBaseOffset,\n      int leftBaseLength,\n      Object rightBaseObject,\n      long rightBaseOffset,\n      int rightBaseLength)",
				"documentation": ""
			},
			{
				"signature": "protected boolean shouldUseRadixSort()",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void setUp() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "private void assertSpillFilesWereCleanedUp()",
				"documentation": ""
			},
			{
				"signature": "private static void insertNumber(UnsafeExternalSorter sorter, int value) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private static void insertRecord(\n      UnsafeExternalSorter sorter,\n      int[] record,\n      long prefix) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private UnsafeExternalSorter newSorter() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSortingOnlyByPrefix() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSortingEmptyArrays() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSortTimeMetric() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void spillingOccursInResponseToMemoryPressure() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFillingPage() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sortingRecordsThatExceedPageSize() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void forcedSpillingWithReadIterator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void forcedSpillingNullsWithReadIterator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void forcedSpillingWithFullyReadIterator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void forcedSpillingWithNotReadIterator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void forcedSpillingWithoutComparator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDiskSpilledBytes() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPeakMemoryUsed() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGetIterator() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoOOMDuringSpill() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void verifyIntIterator(UnsafeSorterIterator iter, int start, int end)\n      throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.memory.TaskMemoryManager"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterRadixSortSuite",
		"extends": "org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite",
		"Methods": [
			{
				"signature": "@Override\n  protected boolean shouldUseRadixSort()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "protected boolean shouldUseRadixSort()",
				"documentation": ""
			},
			{
				"signature": "private static String getStringFromDataPage(Object baseObject, long baseOffset, int length)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSortingEmptyInput()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSortingOnlyByIntegerPrefix() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public int compare(\n        Object leftBaseObject,\n        long leftBaseOffset,\n        int leftBaseLength,\n        Object rightBaseObject,\n        long rightBaseOffset,\n        int rightBaseLength)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoOOMDuringReset()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public int compare(\n              Object leftBaseObject,\n              long leftBaseOffset,\n              int leftBaseLength,\n              Object rightBaseObject,\n              long rightBaseOffset,\n              int rightBaseLength)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterRadixSortSuite"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.memory.TaskMemoryManager",
			"org.apache.spark.memory.TestMemoryConsumer"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Most of these tests replicate org.apache.spark.JavaAPISuite using java 8\n * lambda syntax.\n */",
		"name": "test.org.apache.spark.Java8RDDAPISuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": "/**\n * Most of these tests replicate org.apache.spark.JavaAPISuite using java 8\n * lambda syntax.\n */"
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foreachWithAnonymousClass()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foreach()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void groupBy()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void leftOuterJoin()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foldReduce()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foldByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void reduceByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void map()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void flatMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mapsFromPairsToPairs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mapPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sequenceFile()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void zip()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void zipPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void keyBy()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mapOnPairRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void collectPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void collectAsMapWithIntArrayValues()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.JavaAPISuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  @Test\n  public void sparkContextUnion()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void intersection()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sample()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomSplit()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sortByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void repartitionAndSortWithinPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public int numPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public int getPartition(Object key)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void filterByRange()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void emptyRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sortBy()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foreach()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foreachPartition()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void toLocalIterator()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void zipWithUniqueId()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void zipWithIndex()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void lookup()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void groupBy()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void groupByOnPairRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void keyByOnPairRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cogroup()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cogroup3()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cogroup4()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void leftOuterJoin()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foldReduce()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void treeReduce()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void treeAggregate()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void treeAggregateWithFinalAggregateOnExecutor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void aggregateByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foldByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void reduceByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void approximateResults()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void take()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void isEmpty()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void cartesian()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void javaDoubleRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void javaDoubleRDDHistoGram()",
				"documentation": ""
			},
			{
				"signature": "double[] expected_buckets =",
				"documentation": ""
			},
			{
				"signature": "long[] expected_counts =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void max()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void min()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalMax()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naturalMin()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void takeOrdered()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void top()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void reduce()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void reduceOnJavaDoubleRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fold()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void aggregate()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void map()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void flatMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mapsFromPairsToPairs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mapPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mapPartitionsWithIndex()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void getNumPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void repartition()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void persist()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void withResources()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void iterator()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void glom()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void textFiles() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void wholeTextFiles() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void textFilesCompressed()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sequenceFile()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void binaryFiles() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void binaryFilesCaching() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void binaryRecords() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void writeWithNewAPIHadoopFile()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  @Test\n  public void readWithNewAPIHadoopFile() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void objectFilesOfInts()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void objectFilesOfComplexTypes()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  @Test\n  public void hadoopFile()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  @Test\n  public void hadoopFileCompressed()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void zip()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void zipPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void keyBy()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void checkpointAndComputation()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void checkpointAndRestore()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void combineByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void mapOnPairRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void collectPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void countApproxDistinct()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void countApproxDistinctByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void collectAsMapWithIntArrayValues()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  @Test\n  public void collectAsMapAndSerialize() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sampleByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sampleByKeyExact()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void collectUnderlyingScalaRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void collectAsync() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void takeAsync() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void foreachAsync() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void countAsync() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAsyncActionCancellation() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAsyncActionErrorWrapping() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRegisterKryoClasses()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGetPersistentRDDs()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.JavaAPISuite.DoubleComparator",
			"test.org.apache.spark.JavaAPISuite.AddInts",
			"test.org.apache.spark.JavaAPISuite.SomeCustomClass"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.JavaAPISuite.DoubleComparator",
			"test.org.apache.spark.JavaAPISuite.AddInts",
			"test.org.apache.spark.JavaAPISuite.SomeCustomClass",
			"test.org.apache.spark.JavaAPISuite.BuggyMapFunction",
			"test.org.apache.spark.JavaAPISuite.Class1",
			"test.org.apache.spark.JavaAPISuite.Class2"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.JavaAPISuite.DoubleComparator",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public int compare(Double o1, Double o2)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Comparator",
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.JavaAPISuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.JavaAPISuite.AddInts",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public Integer call(Integer a, Integer b)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Function2"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.JavaAPISuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.JavaAPISuite.SomeCustomClass",
		"extends": "",
		"Methods": [
			{
				"signature": "SomeCustomClass()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.JavaAPISuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.JavaAPISuite.BuggyMapFunction",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public T call(T x)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Function"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.JavaAPISuite.Class1",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.JavaAPISuite.Class2",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Java apps can use both Java-friendly JavaSparkContext and Scala SparkContext.\n */",
		"name": "test.org.apache.spark.JavaSparkContextSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void javaSparkContext() throws IOException",
				"documentation": "/**\n * Java apps can use both Java-friendly JavaSparkContext and Scala SparkContext.\n */"
			},
			{
				"signature": "@Test\n  public void scalaSparkContext()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Something to make sure that TaskContext can be used in Java.\n */",
		"name": "test.org.apache.spark.JavaTaskContextCompileCheck",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void test()",
				"documentation": "/**\n * Something to make sure that TaskContext can be used in Java.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.JavaTaskContextCompileCheck.JavaTaskCompletionListenerImpl",
			"test.org.apache.spark.JavaTaskContextCompileCheck.JavaTaskFailureListenerImpl"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.JavaTaskContextCompileCheck.JavaTaskCompletionListenerImpl",
			"test.org.apache.spark.JavaTaskContextCompileCheck.JavaTaskFailureListenerImpl"
		]
	},
	{
		"documentation": "/**\n   * A simple implementation of TaskCompletionListener that makes sure TaskCompletionListener and\n   * TaskContext is Java friendly.\n   */",
		"name": "test.org.apache.spark.JavaTaskContextCompileCheck.JavaTaskCompletionListenerImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void onTaskCompletion(TaskContext context)",
				"documentation": "/**\n   * A simple implementation of TaskCompletionListener that makes sure TaskCompletionListener and\n   * TaskContext is Java friendly.\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.util.TaskCompletionListener"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.JavaTaskContextCompileCheck"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A simple implementation of TaskCompletionListener that makes sure TaskCompletionListener and\n   * TaskContext is Java friendly.\n   */",
		"name": "test.org.apache.spark.JavaTaskContextCompileCheck.JavaTaskFailureListenerImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void onTaskFailure(TaskContext context, Throwable error)",
				"documentation": "/**\n   * A simple implementation of TaskCompletionListener that makes sure TaskCompletionListener and\n   * TaskContext is Java friendly.\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.util.TaskFailureListener"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.JavaTaskContextCompileCheck"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Logistic regression based classification.\n *\n * This is an example implementation for learning how to use Spark. For more conventional use,\n * please refer to org.apache.spark.ml.classification.LogisticRegression.\n */",
		"name": "org.apache.spark.examples.JavaHdfsLR",
		"extends": "",
		"Methods": [
			{
				"signature": "static void showWarning()",
				"documentation": "/**\n * Logistic regression based classification.\n *\n * This is an example implementation for learning how to use Spark. For more conventional use,\n * please refer to org.apache.spark.ml.classification.LogisticRegression.\n */"
			},
			{
				"signature": "public static double dot(double[] a, double[] b)",
				"documentation": ""
			},
			{
				"signature": "public static void printWeights(double[] a)",
				"documentation": ""
			},
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.JavaHdfsLR.DataPoint",
			"org.apache.spark.examples.JavaHdfsLR.ParsePoint",
			"org.apache.spark.examples.JavaHdfsLR.VectorSum",
			"org.apache.spark.examples.JavaHdfsLR.ComputeGradient"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.JavaHdfsLR.DataPoint",
			"org.apache.spark.examples.JavaHdfsLR.ParsePoint",
			"org.apache.spark.examples.JavaHdfsLR.VectorSum",
			"org.apache.spark.examples.JavaHdfsLR.ComputeGradient"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.JavaHdfsLR.DataPoint",
		"extends": "",
		"Methods": [
			{
				"signature": "DataPoint(double[] x, double y)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.JavaHdfsLR"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.JavaHdfsLR.ParsePoint",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public DataPoint call(String line)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.api.java.function.Function"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.JavaHdfsLR"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.JavaHdfsLR.VectorSum",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public double[] call(double[] a, double[] b)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.api.java.function.Function2"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.JavaHdfsLR"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.JavaHdfsLR.ComputeGradient",
		"extends": "",
		"Methods": [
			{
				"signature": "ComputeGradient(double[] weights)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public double[] call(DataPoint p)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.api.java.function.Function"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.JavaHdfsLR"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Executes a roll up-style query against Apache logs.\n *\n * Usage: JavaLogQuery [logFile]\n */",
		"name": "org.apache.spark.examples.JavaLogQuery",
		"extends": "",
		"Methods": [
			{
				"signature": "public static Tuple3\u003cString, String, String\u003e extractKey(String line)",
				"documentation": ""
			},
			{
				"signature": "public static Stats extractStats(String line)",
				"documentation": ""
			},
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.JavaLogQuery.Stats"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.JavaLogQuery.Stats"
		]
	},
	{
		"documentation": "/** Tracks the total query count and number of aggregate bytes for a particular group. */",
		"name": "org.apache.spark.examples.JavaLogQuery.Stats",
		"extends": "",
		"Methods": [
			{
				"signature": "public Stats(int count, int numBytes)",
				"documentation": "/** Tracks the total query count and number of aggregate bytes for a particular group. */"
			},
			{
				"signature": "public Stats merge(Stats other)",
				"documentation": ""
			},
			{
				"signature": "public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.JavaLogQuery"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Computes the PageRank of URLs from an input file. Input file should\n * be in format of:\n * URL         neighbor URL\n * URL         neighbor URL\n * URL         neighbor URL\n * ...\n * where URL and their neighbors are separated by space(s).\n *\n * This is an example implementation for learning how to use Spark. For more conventional use,\n * please refer to org.apache.spark.graphx.lib.PageRank\n *\n * Example Usage:\n * \u003cpre\u003e\n * bin/run-example JavaPageRank data/mllib/pagerank_data.txt 10\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.JavaPageRank",
		"extends": "",
		"Methods": [
			{
				"signature": "static void showWarning()",
				"documentation": "/**\n * Computes the PageRank of URLs from an input file. Input file should\n * be in format of:\n * URL         neighbor URL\n * URL         neighbor URL\n * URL         neighbor URL\n * ...\n * where URL and their neighbors are separated by space(s).\n *\n * This is an example implementation for learning how to use Spark. For more conventional use,\n * please refer to org.apache.spark.graphx.lib.PageRank\n *\n * Example Usage:\n * \u003cpre\u003e\n * bin/run-example JavaPageRank data/mllib/pagerank_data.txt 10\n * \u003c/pre\u003e\n */"
			},
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.JavaPageRank.Sum"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.JavaPageRank.Sum"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.JavaPageRank.Sum",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public Double call(Double a, Double b)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.api.java.function.Function2"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.JavaPageRank"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Computes an approximation to pi\n * Usage: JavaSparkPi [partitions]\n */",
		"name": "org.apache.spark.examples.JavaSparkPi",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Computes an approximation to pi\n * Usage: JavaSparkPi [partitions]\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Example of using Spark's status APIs from Java.\n */",
		"name": "org.apache.spark.examples.JavaStatusTrackerDemo",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.JavaStatusTrackerDemo.IdentityWithDelay"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.JavaStatusTrackerDemo.IdentityWithDelay"
		]
	},
	{
		"documentation": "/**\n * Example of using Spark's status APIs from Java.\n */",
		"name": "org.apache.spark.examples.JavaStatusTrackerDemo.IdentityWithDelay",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public T call(T x) throws Exception",
				"documentation": "/**\n * Example of using Spark's status APIs from Java.\n */"
			}
		],
		"interfaces": [
			"org.apache.spark.api.java.function.Function"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.JavaStatusTrackerDemo"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Transitive closure on a graph, implemented in Java.\n * Usage: JavaTC [partitions]\n */",
		"name": "org.apache.spark.examples.JavaTC",
		"extends": "",
		"Methods": [
			{
				"signature": "static List\u003cTuple2\u003cInteger, Integer\u003e\u003e generateGraph()",
				"documentation": "/**\n * Transitive closure on a graph, implemented in Java.\n * Usage: JavaTC [partitions]\n */"
			},
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			},
			{
				"signature": "do",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.JavaTC.ProjectFn"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.JavaTC.ProjectFn"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.JavaTC.ProjectFn",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public Tuple2\u003cInteger, Integer\u003e call(Tuple2\u003cInteger, Tuple2\u003cInteger, Integer\u003e\u003e triple)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.api.java.function.PairFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.JavaTC"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.JavaWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating AFTSurvivalRegression.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaAFTSurvivalRegressionExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaAFTSurvivalRegressionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating AFTSurvivalRegression.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaAFTSurvivalRegressionExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaALSExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.ml.JavaALSExample.Rating"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaALSExample.Rating",
		"extends": "",
		"Methods": [
			{
				"signature": "public Rating()",
				"documentation": ""
			},
			{
				"signature": "public Rating(int userId, int movieId, float rating, long timestamp)",
				"documentation": ""
			},
			{
				"signature": "public int getUserId()",
				"documentation": ""
			},
			{
				"signature": "public int getMovieId()",
				"documentation": ""
			},
			{
				"signature": "public float getRating()",
				"documentation": ""
			},
			{
				"signature": "public long getTimestamp()",
				"documentation": ""
			},
			{
				"signature": "public static Rating parseRating(String str)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaBinarizerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating bisecting k-means clustering.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaBisectingKMeansExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaBisectingKMeansExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating bisecting k-means clustering.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaBisectingKMeansExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating BucketedRandomProjectionLSH.\n * Run with:\n *   bin/run-example ml.JavaBucketedRandomProjectionLSHExample\n */",
		"name": "org.apache.spark.examples.ml.JavaBucketedRandomProjectionLSHExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating BucketedRandomProjectionLSH.\n * Run with:\n *   bin/run-example ml.JavaBucketedRandomProjectionLSHExample\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example for Bucketizer.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaBucketizerExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaBucketizerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example for Bucketizer.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaBucketizerExample\n * \u003c/pre\u003e\n */"
			},
			{
				"signature": "double[] splits =",
				"documentation": "/**\n * An example for Bucketizer.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaBucketizerExample\n * \u003c/pre\u003e\n */"
			},
			{
				"signature": "double[][] splitsArray =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaChiSqSelectorExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example for Chi-square hypothesis testing.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaChiSquareTestExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaChiSquareTestExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example for Chi-square hypothesis testing.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaChiSquareTestExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example for computing correlation matrix.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaCorrelationExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaCorrelationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example for computing correlation matrix.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaCorrelationExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaCountVectorizerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaDCTExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaDecisionTreeClassificationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaDecisionTreeRegressionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Unlabeled instance type, Spark SQL can infer schema from Java Beans.\n */",
		"name": "org.apache.spark.examples.ml.JavaDocument",
		"extends": "",
		"Methods": [
			{
				"signature": "public JavaDocument(long id, String text)",
				"documentation": "/**\n * Unlabeled instance type, Spark SQL can infer schema from Java Beans.\n */"
			},
			{
				"signature": "public long getId()",
				"documentation": ""
			},
			{
				"signature": "public String getText()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.examples.ml.JavaLabeledDocument"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaElementwiseProductExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Java example for Estimator, Transformer, and Param.\n */",
		"name": "org.apache.spark.examples.ml.JavaEstimatorTransformerParamExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Java example for Estimator, Transformer, and Param.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaFMClassifierExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaFMRegressorExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating FPGrowth.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaFPGrowthExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaFPGrowthExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating FPGrowth.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaFPGrowthExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaFeatureHasherExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating Gaussian Mixture Model.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaGaussianMixtureExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaGaussianMixtureExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating Gaussian Mixture Model.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaGaussianMixtureExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating generalized linear regression.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaGeneralizedLinearRegressionExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaGeneralizedLinearRegressionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating generalized linear regression.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaGeneralizedLinearRegressionExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaGradientBoostedTreeClassifierExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaGradientBoostedTreeRegressorExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating Imputer.\n * Run with:\n *   bin/run-example ml.JavaImputerExample\n */",
		"name": "org.apache.spark.examples.ml.JavaImputerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating Imputer.\n * Run with:\n *   bin/run-example ml.JavaImputerExample\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaIndexToStringExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaInteractionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating IsotonicRegression.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaIsotonicRegressionExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaIsotonicRegressionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating IsotonicRegression.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaIsotonicRegressionExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating k-means clustering.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaKMeansExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaKMeansExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating k-means clustering.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaKMeansExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating LDA.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaLDAExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaLDAExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating LDA.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaLDAExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Labeled instance type, Spark SQL can infer schema from Java Beans.\n */",
		"name": "org.apache.spark.examples.ml.JavaLabeledDocument",
		"extends": "org.apache.spark.examples.ml.JavaDocument",
		"Methods": [
			{
				"signature": "public JavaLabeledDocument(long id, String text, double label)",
				"documentation": "/**\n * Labeled instance type, Spark SQL can infer schema from Java Beans.\n */"
			},
			{
				"signature": "public double getLabel()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaLinearRegressionWithElasticNetExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaLinearSVCExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaLogisticRegressionSummaryExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaLogisticRegressionWithElasticNetExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaMaxAbsScalerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating MinHashLSH.\n * Run with:\n *   bin/run-example ml.JavaMinHashLSHExample\n */",
		"name": "org.apache.spark.examples.ml.JavaMinHashLSHExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating MinHashLSH.\n * Run with:\n *   bin/run-example ml.JavaMinHashLSHExample\n */"
			},
			{
				"signature": "int[] indices =",
				"documentation": "/**\n * An example demonstrating MinHashLSH.\n * Run with:\n *   bin/run-example ml.JavaMinHashLSHExample\n */"
			},
			{
				"signature": "double[] values =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaMinMaxScalerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Java example for Model Selection via Cross Validation.\n */",
		"name": "org.apache.spark.examples.ml.JavaModelSelectionViaCrossValidationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Java example for Model Selection via Cross Validation.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Java example demonstrating model selection using TrainValidationSplit.\n *\n * Run with\n * {{{\n * bin/run-example ml.JavaModelSelectionViaTrainValidationSplitExample\n * }}}\n */",
		"name": "org.apache.spark.examples.ml.JavaModelSelectionViaTrainValidationSplitExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Java example demonstrating model selection using TrainValidationSplit.\n *\n * Run with\n * {{{\n * bin/run-example ml.JavaModelSelectionViaTrainValidationSplitExample\n * }}}\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaMulticlassLogisticRegressionWithElasticNetExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example for Multilayer Perceptron Classification.\n */",
		"name": "org.apache.spark.examples.ml.JavaMultilayerPerceptronClassifierExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example for Multilayer Perceptron Classification.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaNGramExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example for Naive Bayes Classification.\n */",
		"name": "org.apache.spark.examples.ml.JavaNaiveBayesExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example for Naive Bayes Classification.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaNormalizerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaOneHotEncoderExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example of Multiclass to Binary Reduction with One Vs Rest,\n * using Logistic Regression as the base classifier.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaOneVsRestExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaOneVsRestExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example of Multiclass to Binary Reduction with One Vs Rest,\n * using Logistic Regression as the base classifier.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaOneVsRestExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaPCAExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Java example for simple text document 'Pipeline'.\n */",
		"name": "org.apache.spark.examples.ml.JavaPipelineExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Java example for simple text document 'Pipeline'.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaPolynomialExpansionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaPowerIterationClusteringExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example demonstrating PrefixSpan.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaPrefixSpanExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaPrefixSpanExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example demonstrating PrefixSpan.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaPrefixSpanExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaQuantileDiscretizerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaRFormulaExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaRandomForestClassifierExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaRandomForestRegressorExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaRobustScalerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaSQLTransformerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaStandardScalerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaStopWordsRemoverExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory",
			"org.apache.spark.sql.types.DataTypes"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaStringIndexerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaSummarizerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaTfIdfExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaTokenizerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example for UnivariateFeatureSelector.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaUnivariateFeatureSelectorExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaUnivariateFeatureSelectorExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example for UnivariateFeatureSelector.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaUnivariateFeatureSelectorExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example for VarianceThresholdSelector.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaVarianceThresholdSelectorExample\n * \u003c/pre\u003e\n */",
		"name": "org.apache.spark.examples.ml.JavaVarianceThresholdSelectorExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * An example for VarianceThresholdSelector.\n * Run with\n * \u003cpre\u003e\n * bin/run-example ml.JavaVarianceThresholdSelectorExample\n * \u003c/pre\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaVectorAssemblerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaVectorIndexerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaVectorSizeHintExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaVectorSlicerExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			},
			{
				"signature": "Attribute[] attrs =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.ml.JavaWord2VecExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Example using MLlib ALS from Java.\n */",
		"name": "org.apache.spark.examples.mllib.JavaALS",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.mllib.JavaALS.ParseRating",
			"org.apache.spark.examples.mllib.JavaALS.FeaturesToString"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.mllib.JavaALS.ParseRating",
			"org.apache.spark.examples.mllib.JavaALS.FeaturesToString"
		]
	},
	{
		"documentation": "/**\n * Example using MLlib ALS from Java.\n */",
		"name": "org.apache.spark.examples.mllib.JavaALS.ParseRating",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public Rating call(String line)",
				"documentation": "/**\n * Example using MLlib ALS from Java.\n */"
			}
		],
		"interfaces": [
			"org.apache.spark.api.java.function.Function"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.mllib.JavaALS"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaALS.FeaturesToString",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public String call(Tuple2\u003cObject, double[]\u003e element)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.api.java.function.Function"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.mllib.JavaALS"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaAssociationRulesExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaBinaryClassificationMetricsExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Java example for bisecting k-means clustering.\n */",
		"name": "org.apache.spark.examples.mllib.JavaBisectingKMeansExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Java example for bisecting k-means clustering.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaChiSqSelectorExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaCorrelationsExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaDecisionTreeClassificationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaDecisionTreeRegressionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaElementwiseProductExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaGaussianMixtureExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaGradientBoostingClassificationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaGradientBoostingRegressionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaHypothesisTestingExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaHypothesisTestingKolmogorovSmirnovTestExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaIsotonicRegressionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaKMeansExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaKernelDensityEstimationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaLBFGSExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaLatentDirichletAllocationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Example for LogisticRegressionWithLBFGS.\n */",
		"name": "org.apache.spark.examples.mllib.JavaLogisticRegressionWithLBFGSExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Example for LogisticRegressionWithLBFGS.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaMultiLabelClassificationMetricsExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaMulticlassClassificationMetricsExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaNaiveBayesExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Example for compute principal components on a 'RowMatrix'.\n */",
		"name": "org.apache.spark.examples.mllib.JavaPCAExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Example for compute principal components on a 'RowMatrix'.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Java example for graph clustering using power iteration clustering (PIC).\n */",
		"name": "org.apache.spark.examples.mllib.JavaPowerIterationClusteringExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Java example for graph clustering using power iteration clustering (PIC).\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaPrefixSpanExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaRandomForestClassificationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaRandomForestRegressionExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaRankingMetricsExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			},
			{
				"signature": "Integer[] kVector =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaRecommendationExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Example for SingularValueDecomposition.\n */",
		"name": "org.apache.spark.examples.mllib.JavaSVDExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Example for SingularValueDecomposition.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Example for SVMWithSGD.\n */",
		"name": "org.apache.spark.examples.mllib.JavaSVMWithSGDExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": "/**\n * Example for SVMWithSGD.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaSimpleFPGrowth",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaStratifiedSamplingExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Perform streaming testing using Welch's 2-sample t-test on a stream of data, where the data\n * stream arrives as text files in a directory. Stops when the two groups are statistically\n * significant (p-value \u003c 0.05) or after a user-specified timeout in number of batches is exceeded.\n *\n * The rows of the text files must be in the form `Boolean, Double`. For example:\n *   false, -3.92\n *   true, 99.32\n *\n * Usage:\n *   JavaStreamingTestExample \u003cdataDir\u003e \u003cbatchDuration\u003e \u003cnumBatchesTimeout\u003e\n *\n * To run on your local machine using the directory `dataDir` with 5 seconds between each batch and\n * a timeout after 100 insignificant batches, call:\n *    $ bin/run-example mllib.JavaStreamingTestExample dataDir 5 100\n *\n * As you add text files to `dataDir` the significance test wil continually update every\n * `batchDuration` seconds until the test becomes significant (p-value \u003c 0.05) or the number of\n * batches processed exceeds `numBatchesTimeout`.\n */",
		"name": "org.apache.spark.examples.mllib.JavaStreamingTestExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Perform streaming testing using Welch's 2-sample t-test on a stream of data, where the data\n * stream arrives as text files in a directory. Stops when the two groups are statistically\n * significant (p-value \u003c 0.05) or after a user-specified timeout in number of batches is exceeded.\n *\n * The rows of the text files must be in the form `Boolean, Double`. For example:\n *   false, -3.92\n *   true, 99.32\n *\n * Usage:\n *   JavaStreamingTestExample \u003cdataDir\u003e \u003cbatchDuration\u003e \u003cnumBatchesTimeout\u003e\n *\n * To run on your local machine using the directory `dataDir` with 5 seconds between each batch and\n * a timeout after 100 insignificant batches, call:\n *    $ bin/run-example mllib.JavaStreamingTestExample dataDir 5 100\n *\n * As you add text files to `dataDir` the significance test wil continually update every\n * `batchDuration` seconds until the test becomes significant (p-value \u003c 0.05) or the number of\n * batches processed exceeds `numBatchesTimeout`.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.mllib.JavaSummaryStatisticsExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaSQLDataSourceExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			},
			{
				"signature": "private static void runGenericFileSourceOptionsExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runBasicDataSourceExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runBasicParquetExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runParquetSchemaMergingExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runJsonDatasetExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runCsvDatasetExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runTextDatasetExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runJdbcDatasetExample(SparkSession spark)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.sql.JavaSQLDataSourceExample.Square",
			"org.apache.spark.examples.sql.JavaSQLDataSourceExample.Cube"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaSQLDataSourceExample.Square",
		"extends": "",
		"Methods": [
			{
				"signature": "public int getValue()",
				"documentation": ""
			},
			{
				"signature": "public void setValue(int value)",
				"documentation": ""
			},
			{
				"signature": "public int getSquare()",
				"documentation": ""
			},
			{
				"signature": "public void setSquare(int square)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaSQLDataSourceExample.Cube",
		"extends": "",
		"Methods": [
			{
				"signature": "public int getValue()",
				"documentation": ""
			},
			{
				"signature": "public void setValue(int value)",
				"documentation": ""
			},
			{
				"signature": "public int getCube()",
				"documentation": ""
			},
			{
				"signature": "public void setCube(int cube)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaSparkSQLExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws AnalysisException",
				"documentation": ""
			},
			{
				"signature": "private static void runBasicDataFrameExample(SparkSession spark) throws AnalysisException",
				"documentation": ""
			},
			{
				"signature": "private static void runDatasetCreationExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runInferSchemaExample(SparkSession spark)",
				"documentation": ""
			},
			{
				"signature": "private static void runProgrammaticSchemaExample(SparkSession spark)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory",
			"org.apache.spark.sql.types.DataTypes"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.sql.JavaSparkSQLExample.Person"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaSparkSQLExample.Person",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getName()",
				"documentation": ""
			},
			{
				"signature": "public void setName(String name)",
				"documentation": ""
			},
			{
				"signature": "public long getAge()",
				"documentation": ""
			},
			{
				"signature": "public void setAge(long age)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaUserDefinedScalar",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation.Average",
			"org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation.MyAverage"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation.Employee",
			"org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation.Average",
			"org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation.MyAverage"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation.Employee",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getName()",
				"documentation": ""
			},
			{
				"signature": "public void setName(String name)",
				"documentation": ""
			},
			{
				"signature": "public long getSalary()",
				"documentation": ""
			},
			{
				"signature": "public void setSalary(long salary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation.Average",
		"extends": "",
		"Methods": [
			{
				"signature": "public Average()",
				"documentation": ""
			},
			{
				"signature": "public Average(long sum, long count)",
				"documentation": ""
			},
			{
				"signature": "public long getSum()",
				"documentation": ""
			},
			{
				"signature": "public void setSum(long sum)",
				"documentation": ""
			},
			{
				"signature": "public long getCount()",
				"documentation": ""
			},
			{
				"signature": "public void setCount(long count)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation.MyAverage",
		"extends": "org.apache.spark.sql.expressions.Aggregator",
		"Methods": [
			{
				"signature": "public Average zero()",
				"documentation": ""
			},
			{
				"signature": "public Average reduce(Average buffer, Employee employee)",
				"documentation": ""
			},
			{
				"signature": "public Average merge(Average b1, Average b2)",
				"documentation": ""
			},
			{
				"signature": "public Double finish(Average reduction)",
				"documentation": ""
			},
			{
				"signature": "public Encoder\u003cAverage\u003e bufferEncoder()",
				"documentation": ""
			},
			{
				"signature": "public Encoder\u003cDouble\u003e outputEncoder()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.sql.JavaUserDefinedTypedAggregation"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation.Average",
			"org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation.MyAverage"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation.Average",
			"org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation.MyAverage"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation.Average",
		"extends": "",
		"Methods": [
			{
				"signature": "public Average()",
				"documentation": ""
			},
			{
				"signature": "public Average(long sum, long count)",
				"documentation": ""
			},
			{
				"signature": "public long getSum()",
				"documentation": ""
			},
			{
				"signature": "public void setSum(long sum)",
				"documentation": ""
			},
			{
				"signature": "public long getCount()",
				"documentation": ""
			},
			{
				"signature": "public void setCount(long count)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation.MyAverage",
		"extends": "org.apache.spark.sql.expressions.Aggregator",
		"Methods": [
			{
				"signature": "public Average zero()",
				"documentation": ""
			},
			{
				"signature": "public Average reduce(Average buffer, Long data)",
				"documentation": ""
			},
			{
				"signature": "public Average merge(Average b1, Average b2)",
				"documentation": ""
			},
			{
				"signature": "public Double finish(Average reduction)",
				"documentation": ""
			},
			{
				"signature": "public Encoder\u003cAverage\u003e bufferEncoder()",
				"documentation": ""
			},
			{
				"signature": "public Encoder\u003cDouble\u003e outputEncoder()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.sql.JavaUserDefinedUntypedAggregation"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.hive.JavaSparkHiveExample",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.sql.hive.JavaSparkHiveExample.Record"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.hive.JavaSparkHiveExample.Record",
		"extends": "",
		"Methods": [
			{
				"signature": "public int getKey()",
				"documentation": ""
			},
			{
				"signature": "public void setKey(int key)",
				"documentation": ""
			},
			{
				"signature": "public String getValue()",
				"documentation": ""
			},
			{
				"signature": "public void setValue(String value)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Sessionize events in UTF8 encoded, '\\n' delimited text received from the network.\n * Each line composes an event, and the line should match to the json format.\n * \u003cp\u003e\n * The schema of the event is following:\n * - user_id: String\n * - event_type: String\n * - timestamp: Long\n * \u003cp\u003e\n * The supported types are following:\n * - NEW_EVENT\n * - CLOSE_SESSION\n * \u003cp\u003e\n * This example focuses to demonstrate the complex sessionization which uses two conditions\n * on closing session; conditions are following:\n * - No further event is provided for the user ID within 5 seconds\n * - An event having CLOSE_SESSION as event_type is provided for the user ID\n * \u003cp\u003e\n * Usage: JavaStructuredComplexSessionization \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Structured Streaming\n * would connect to receive data.\n * \u003cp\u003e\n * To run this on your local machine, you need to first run a Netcat server\n * `$ nc -lk 9999`\n * and then run the example\n * `$ bin/run-example sql.streaming.JavaStructuredComplexSessionization\n * localhost 9999`\n * \u003cp\u003e\n * Here's a set of events for example:\n *\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 13}\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 10}\n * {\"user_id\": \"user1\", \"event_type\": \"CLOSE_SESSION\", \"timestamp\": 15}\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 17}\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 19}\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 29}\n *\n * {\"user_id\": \"user2\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 45}\n *\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 65}\n *\n * and results (the output can be split across micro-batches):\n *\n * +-----+----------+---------+\n * |   id|durationMs|numEvents|\n * +-----+----------+---------+\n * |user1|      5000|        3|\n * |user1|      7000|        2|\n * |user1|      5000|        1|\n * |user2|      5000|        1|\n * +-----+----------+---------+\n * (The last event is not reflected into output due to watermark.)\n * \u003cp\u003e\n * Note that there're three different sessions for 'user1'. The events in first two sessions\n * are occurred within gap duration for nearest events, but they don't compose a single session\n * due to the event of CLOSE_SESSION.\n * \u003cp\u003e\n * Also note that the implementation is simplified one. This example doesn't address\n * - UPDATE MODE (the semantic is not clear for session window with event time processing)\n * - partial merge (events in session which are earlier than watermark can be aggregated)\n * - other possible optimizations (especially the implementation is ported from Scala example)\n */",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Sessionize events in UTF8 encoded, '\\n' delimited text received from the network.\n * Each line composes an event, and the line should match to the json format.\n * \u003cp\u003e\n * The schema of the event is following:\n * - user_id: String\n * - event_type: String\n * - timestamp: Long\n * \u003cp\u003e\n * The supported types are following:\n * - NEW_EVENT\n * - CLOSE_SESSION\n * \u003cp\u003e\n * This example focuses to demonstrate the complex sessionization which uses two conditions\n * on closing session; conditions are following:\n * - No further event is provided for the user ID within 5 seconds\n * - An event having CLOSE_SESSION as event_type is provided for the user ID\n * \u003cp\u003e\n * Usage: JavaStructuredComplexSessionization \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Structured Streaming\n * would connect to receive data.\n * \u003cp\u003e\n * To run this on your local machine, you need to first run a Netcat server\n * `$ nc -lk 9999`\n * and then run the example\n * `$ bin/run-example sql.streaming.JavaStructuredComplexSessionization\n * localhost 9999`\n * \u003cp\u003e\n * Here's a set of events for example:\n *\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 13}\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 10}\n * {\"user_id\": \"user1\", \"event_type\": \"CLOSE_SESSION\", \"timestamp\": 15}\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 17}\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 19}\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 29}\n *\n * {\"user_id\": \"user2\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 45}\n *\n * {\"user_id\": \"user1\", \"event_type\": \"NEW_EVENT\", \"timestamp\": 65}\n *\n * and results (the output can be split across micro-batches):\n *\n * +-----+----------+---------+\n * |   id|durationMs|numEvents|\n * +-----+----------+---------+\n * |user1|      5000|        3|\n * |user1|      7000|        2|\n * |user1|      5000|        1|\n * |user2|      5000|        1|\n * +-----+----------+---------+\n * (The last event is not reflected into output due to watermark.)\n * \u003cp\u003e\n * Note that there're three different sessions for 'user1'. The events in first two sessions\n * are occurred within gap duration for nearest events, but they don't compose a single session\n * due to the event of CLOSE_SESSION.\n * \u003cp\u003e\n * Also note that the implementation is simplified one. This example doesn't address\n * - UPDATE MODE (the semantic is not clear for session window with event time processing)\n * - partial merge (events in session which are earlier than watermark can be aggregated)\n * - other possible optimizations (especially the implementation is ported from Scala example)\n */"
			},
			{
				"signature": "private Iterator\u003cSession\u003e handleEvict(String userId, GroupState\u003cSessions\u003e state)",
				"documentation": ""
			},
			{
				"signature": "private void mergeSessions(List\u003cSessionAcc\u003e sessionAccs, GroupState\u003cSessions\u003e state)",
				"documentation": ""
			},
			{
				"signature": "@Override\n          public Iterator\u003cSession\u003e call(\n              String userId, Iterator\u003cRow\u003e events, GroupState\u003cSessions\u003e state)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.Sessions",
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.SessionEvent",
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.SessionAcc",
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.Session",
			"org.apache.spark.sql.streaming.GroupStateTimeout",
			"org.apache.spark.sql.streaming.OutputMode",
			"org.apache.spark.api.java.function.FlatMapGroupsWithStateFunction"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.Sessions",
			"org.apache.spark.examples.sql.streaming.EventTypes",
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.SessionEvent",
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.SessionAcc",
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.Session"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.Sessions",
		"extends": "",
		"Methods": [
			{
				"signature": "public List\u003cSessionAcc\u003e getSessions()",
				"documentation": ""
			},
			{
				"signature": "public void setSessions(List\u003cSessionAcc\u003e sessions)",
				"documentation": ""
			},
			{
				"signature": "public static Sessions newInstance(List\u003cSessionAcc\u003e sessions)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.streaming.EventTypes",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.SessionEvent",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getUserId()",
				"documentation": ""
			},
			{
				"signature": "public void setUserId(String userId)",
				"documentation": ""
			},
			{
				"signature": "public EventTypes getEventType()",
				"documentation": ""
			},
			{
				"signature": "public void setEventType(EventTypes eventType)",
				"documentation": ""
			},
			{
				"signature": "public Timestamp getStartTimestamp()",
				"documentation": ""
			},
			{
				"signature": "public void setStartTimestamp(Timestamp startTimestamp)",
				"documentation": ""
			},
			{
				"signature": "public Timestamp getEndTimestamp()",
				"documentation": ""
			},
			{
				"signature": "public void setEndTimestamp(Timestamp endTimestamp)",
				"documentation": ""
			},
			{
				"signature": "public static SessionEvent newInstance(String userId, String eventTypeStr,\n                                           Timestamp startTimestamp, long gapDuration)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.SessionAcc",
		"extends": "",
		"Methods": [
			{
				"signature": "public Timestamp startTime()",
				"documentation": ""
			},
			{
				"signature": "public Timestamp endTime()",
				"documentation": ""
			},
			{
				"signature": "public List\u003cSessionEvent\u003e getEvents()",
				"documentation": ""
			},
			{
				"signature": "public void setEvents(List\u003cSessionEvent\u003e events)",
				"documentation": ""
			},
			{
				"signature": "public static SessionAcc newInstance(SessionEvent event)",
				"documentation": ""
			},
			{
				"signature": "public static SessionAcc newInstance(List\u003cSessionEvent\u003e events)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization.Session",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getId()",
				"documentation": ""
			},
			{
				"signature": "public void setId(String id)",
				"documentation": ""
			},
			{
				"signature": "public long getDuration()",
				"documentation": ""
			},
			{
				"signature": "public void setDuration(long duration)",
				"documentation": ""
			},
			{
				"signature": "public int getNumEvents()",
				"documentation": ""
			},
			{
				"signature": "public void setNumEvents(int numEvents)",
				"documentation": ""
			},
			{
				"signature": "public static Session newInstance(String id, long duration, int numEvents)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Consumes messages from one or more topics in Kafka and does wordcount.\n * Usage: JavaStructuredKafkaWordCount \u003cbootstrap-servers\u003e \u003csubscribe-type\u003e \u003ctopics\u003e\n *   \u003cbootstrap-servers\u003e The Kafka \"bootstrap.servers\" configuration. A\n *   comma-separated list of host:port.\n *   \u003csubscribe-type\u003e There are three kinds of type, i.e. 'assign', 'subscribe',\n *   'subscribePattern'.\n *   |- \u003cassign\u003e Specific TopicPartitions to consume. Json string\n *   |  {\"topicA\":[0,1],\"topicB\":[2,4]}.\n *   |- \u003csubscribe\u003e The topic list to subscribe. A comma-separated list of\n *   |  topics.\n *   |- \u003csubscribePattern\u003e The pattern used to subscribe to topic(s).\n *   |  Java regex string.\n *   |- Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be\n *   |  specified for Kafka source.\n *   \u003ctopics\u003e Different value format depends on the value of 'subscribe-type'.\n *\n * Example:\n *    `$ bin/run-example \\\n *      sql.streaming.JavaStructuredKafkaWordCount host1:port1,host2:port2 \\\n *      subscribe topic1,topic2`\n */",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredKafkaWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Consumes messages from one or more topics in Kafka and does wordcount.\n * Usage: JavaStructuredKafkaWordCount \u003cbootstrap-servers\u003e \u003csubscribe-type\u003e \u003ctopics\u003e\n *   \u003cbootstrap-servers\u003e The Kafka \"bootstrap.servers\" configuration. A\n *   comma-separated list of host:port.\n *   \u003csubscribe-type\u003e There are three kinds of type, i.e. 'assign', 'subscribe',\n *   'subscribePattern'.\n *   |- \u003cassign\u003e Specific TopicPartitions to consume. Json string\n *   |  {\"topicA\":[0,1],\"topicB\":[2,4]}.\n *   |- \u003csubscribe\u003e The topic list to subscribe. A comma-separated list of\n *   |  topics.\n *   |- \u003csubscribePattern\u003e The pattern used to subscribe to topic(s).\n *   |  Java regex string.\n *   |- Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be\n *   |  specified for Kafka source.\n *   \u003ctopics\u003e Different value format depends on the value of 'subscribe-type'.\n *\n * Example:\n *    `$ bin/run-example \\\n *      sql.streaming.JavaStructuredKafkaWordCount host1:port1,host2:port2 \\\n *      subscribe topic1,topic2`\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Consumes messages from one or more topics in Kafka and does wordcount.\n * Usage: JavaStructuredKerberizedKafkaWordCount \u003cbootstrap-servers\u003e \u003csubscribe-type\u003e \u003ctopics\u003e\n *   \u003cbootstrap-servers\u003e The Kafka \"bootstrap.servers\" configuration. A\n *   comma-separated list of host:port.\n *   \u003csubscribe-type\u003e There are three kinds of type, i.e. 'assign', 'subscribe',\n *   'subscribePattern'.\n *   |- \u003cassign\u003e Specific TopicPartitions to consume. Json string\n *   |  {\"topicA\":[0,1],\"topicB\":[2,4]}.\n *   |- \u003csubscribe\u003e The topic list to subscribe. A comma-separated list of\n *   |  topics.\n *   |- \u003csubscribePattern\u003e The pattern used to subscribe to topic(s).\n *   |  Java regex string.\n *   |- Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be\n *   |  specified for Kafka source.\n *   \u003ctopics\u003e Different value format depends on the value of 'subscribe-type'.\n *\n * Example:\n *   Yarn client:\n *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n *      --conf \\\n *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n *      --master yarn\n *      sql.streaming.JavaStructuredKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n *      subscribe topic1,topic2\n *   Yarn cluster:\n *    $ bin/run-example --files \\\n *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n *      --driver-java-options \\\n *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n *      -Djava.security.krb5.conf=./krb5.conf\" \\\n *      --conf \\\n *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n *      --master yarn --deploy-mode cluster \\\n *      sql.streaming.JavaStructuredKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n *      subscribe topic1,topic2\n *\n * kafka_jaas.conf can manually create, template as:\n *   KafkaClient {\n *     com.sun.security.auth.module.Krb5LoginModule required\n *     keyTab=\"./kafka.service.keytab\"\n *     useKeyTab=true\n *     storeKey=true\n *     useTicketCache=false\n *     serviceName=\"kafka\"\n *     principal=\"kafka/host@EXAMPLE.COM\";\n *   };\n * kafka_driver_jaas.conf (used by yarn client) and kafka_jaas.conf are basically the same\n * except for some differences at 'keyTab'. In kafka_driver_jaas.conf, 'keyTab' should be\n * \"${keytab_path}/kafka.service.keytab\".\n * In addition, for IBM JVMs, please use 'com.ibm.security.auth.module.Krb5LoginModule'\n * instead of 'com.sun.security.auth.module.Krb5LoginModule'.\n *\n * Note that this example uses SASL_PLAINTEXT for simplicity; however,\n * SASL_PLAINTEXT has no SSL encryption and likely be less secure. Please consider\n * using SASL_SSL in production.\n */",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredKerberizedKafkaWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Consumes messages from one or more topics in Kafka and does wordcount.\n * Usage: JavaStructuredKerberizedKafkaWordCount \u003cbootstrap-servers\u003e \u003csubscribe-type\u003e \u003ctopics\u003e\n *   \u003cbootstrap-servers\u003e The Kafka \"bootstrap.servers\" configuration. A\n *   comma-separated list of host:port.\n *   \u003csubscribe-type\u003e There are three kinds of type, i.e. 'assign', 'subscribe',\n *   'subscribePattern'.\n *   |- \u003cassign\u003e Specific TopicPartitions to consume. Json string\n *   |  {\"topicA\":[0,1],\"topicB\":[2,4]}.\n *   |- \u003csubscribe\u003e The topic list to subscribe. A comma-separated list of\n *   |  topics.\n *   |- \u003csubscribePattern\u003e The pattern used to subscribe to topic(s).\n *   |  Java regex string.\n *   |- Only one of \"assign, \"subscribe\" or \"subscribePattern\" options can be\n *   |  specified for Kafka source.\n *   \u003ctopics\u003e Different value format depends on the value of 'subscribe-type'.\n *\n * Example:\n *   Yarn client:\n *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n *      --conf \\\n *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n *      --master yarn\n *      sql.streaming.JavaStructuredKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n *      subscribe topic1,topic2\n *   Yarn cluster:\n *    $ bin/run-example --files \\\n *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n *      --driver-java-options \\\n *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n *      -Djava.security.krb5.conf=./krb5.conf\" \\\n *      --conf \\\n *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n *      --master yarn --deploy-mode cluster \\\n *      sql.streaming.JavaStructuredKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n *      subscribe topic1,topic2\n *\n * kafka_jaas.conf can manually create, template as:\n *   KafkaClient {\n *     com.sun.security.auth.module.Krb5LoginModule required\n *     keyTab=\"./kafka.service.keytab\"\n *     useKeyTab=true\n *     storeKey=true\n *     useTicketCache=false\n *     serviceName=\"kafka\"\n *     principal=\"kafka/host@EXAMPLE.COM\";\n *   };\n * kafka_driver_jaas.conf (used by yarn client) and kafka_jaas.conf are basically the same\n * except for some differences at 'keyTab'. In kafka_driver_jaas.conf, 'keyTab' should be\n * \"${keytab_path}/kafka.service.keytab\".\n * In addition, for IBM JVMs, please use 'com.ibm.security.auth.module.Krb5LoginModule'\n * instead of 'com.sun.security.auth.module.Krb5LoginModule'.\n *\n * Note that this example uses SASL_PLAINTEXT for simplicity; however,\n * SASL_PLAINTEXT has no SSL encryption and likely be less secure. Please consider\n * using SASL_SSL in production.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network.\n *\n * Usage: JavaStructuredNetworkWordCount \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Structured Streaming\n * would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example sql.streaming.JavaStructuredNetworkWordCount\n *    localhost 9999`\n */",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network.\n *\n * Usage: JavaStructuredNetworkWordCount \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Structured Streaming\n * would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example sql.streaming.JavaStructuredNetworkWordCount\n *    localhost 9999`\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network over a\n * sliding window of configurable duration. Each line from the network is tagged\n * with a timestamp that is used to determine the windows into which it falls.\n *\n * Usage: JavaStructuredNetworkWordCountWindowed \u003chostname\u003e \u003cport\u003e \u003cwindow duration\u003e\n *   [\u003cslide duration\u003e]\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Structured Streaming\n * would connect to receive data.\n * \u003cwindow duration\u003e gives the size of window, specified as integer number of seconds\n * \u003cslide duration\u003e gives the amount of time successive windows are offset from one another,\n * given in the same units as above. \u003cslide duration\u003e should be less than or equal to\n * \u003cwindow duration\u003e. If the two are equal, successive windows have no overlap. If\n * \u003cslide duration\u003e is not provided, it defaults to \u003cwindow duration\u003e.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example sql.streaming.JavaStructuredNetworkWordCountWindowed\n *    localhost 9999 \u003cwindow duration in seconds\u003e [\u003cslide duration in seconds\u003e]`\n *\n * One recommended \u003cwindow duration\u003e, \u003cslide duration\u003e pair is 10, 5\n */",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCountWindowed",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network over a\n * sliding window of configurable duration. Each line from the network is tagged\n * with a timestamp that is used to determine the windows into which it falls.\n *\n * Usage: JavaStructuredNetworkWordCountWindowed \u003chostname\u003e \u003cport\u003e \u003cwindow duration\u003e\n *   [\u003cslide duration\u003e]\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Structured Streaming\n * would connect to receive data.\n * \u003cwindow duration\u003e gives the size of window, specified as integer number of seconds\n * \u003cslide duration\u003e gives the amount of time successive windows are offset from one another,\n * given in the same units as above. \u003cslide duration\u003e should be less than or equal to\n * \u003cwindow duration\u003e. If the two are equal, successive windows have no overlap. If\n * \u003cslide duration\u003e is not provided, it defaults to \u003cwindow duration\u003e.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example sql.streaming.JavaStructuredNetworkWordCountWindowed\n *    localhost 9999 \u003cwindow duration in seconds\u003e [\u003cslide duration in seconds\u003e]`\n *\n * One recommended \u003cwindow duration\u003e, \u003cslide duration\u003e pair is 10, 5\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network.\n * \u003cp\u003e\n * Usage: JavaStructuredSessionization \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Structured Streaming\n * would connect to receive data.\n * \u003cp\u003e\n * To run this on your local machine, you need to first run a Netcat server\n * `$ nc -lk 9999`\n * and then run the example\n * `$ bin/run-example sql.streaming.JavaStructuredSessionization\n * localhost 9999`\n */",
		"name": "org.apache.spark.examples.sql.streaming.JavaStructuredSessionization",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network.\n * \u003cp\u003e\n * Usage: JavaStructuredSessionization \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Structured Streaming\n * would connect to receive data.\n * \u003cp\u003e\n * To run this on your local machine, you need to first run a Netcat server\n * `$ nc -lk 9999`\n * and then run the example\n * `$ bin/run-example sql.streaming.JavaStructuredSessionization\n * localhost 9999`\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Custom Receiver that receives data over a socket. Received bytes is interpreted as\n * text and \\n delimited lines are considered as records. They are then counted and printed.\n *\n * Usage: JavaCustomReceiver \u003cmaster\u003e \u003chostname\u003e \u003cport\u003e\n *   \u003cmaster\u003e is the Spark master URL. In local mode, \u003cmaster\u003e should be 'local[n]' with n \u003e 1.\n *   \u003chostname\u003e and \u003cport\u003e of the TCP server that Spark Streaming would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example org.apache.spark.examples.streaming.JavaCustomReceiver localhost 9999`\n */",
		"name": "org.apache.spark.examples.streaming.JavaCustomReceiver",
		"extends": "org.apache.spark.streaming.receiver.Receiver",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Custom Receiver that receives data over a socket. Received bytes is interpreted as\n * text and \\n delimited lines are considered as records. They are then counted and printed.\n *\n * Usage: JavaCustomReceiver \u003cmaster\u003e \u003chostname\u003e \u003cport\u003e\n *   \u003cmaster\u003e is the Spark master URL. In local mode, \u003cmaster\u003e should be 'local[n]' with n \u003e 1.\n *   \u003chostname\u003e and \u003cport\u003e of the TCP server that Spark Streaming would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example org.apache.spark.examples.streaming.JavaCustomReceiver localhost 9999`\n */"
			},
			{
				"signature": "public JavaCustomReceiver(String host_ , int port_)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onStart()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onStop()",
				"documentation": ""
			},
			{
				"signature": "private void receive()",
				"documentation": "/** Create a socket connection and receive data until receiver is stopped */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Consumes messages from one or more topics in Kafka and does wordcount.\n * Usage: JavaDirectKafkaWordCount \u003cbrokers\u003e \u003cgroupId\u003e \u003ctopics\u003e\n *   \u003cbrokers\u003e is a list of one or more Kafka brokers\n *   \u003cgroupId\u003e is a consumer group name to consume from topics\n *   \u003ctopics\u003e is a list of one or more kafka topics to consume from\n *\n * Example:\n *    $ bin/run-example streaming.JavaDirectKafkaWordCount broker1-host:port,broker2-host:port \\\n *      consumer-group topic1,topic2\n */",
		"name": "org.apache.spark.examples.streaming.JavaDirectKafkaWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Consumes messages from one or more topics in Kafka and does wordcount.\n * Usage: JavaDirectKafkaWordCount \u003cbrokers\u003e \u003cgroupId\u003e \u003ctopics\u003e\n *   \u003cbrokers\u003e is a list of one or more Kafka brokers\n *   \u003cgroupId\u003e is a consumer group name to consume from topics\n *   \u003ctopics\u003e is a list of one or more kafka topics to consume from\n *\n * Example:\n *    $ bin/run-example streaming.JavaDirectKafkaWordCount broker1-host:port,broker2-host:port \\\n *      consumer-group topic1,topic2\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Consumes messages from one or more topics in Kafka and does wordcount.\n * Usage: JavaDirectKerberizedKafkaWordCount \u003cbrokers\u003e \u003cgroupId\u003e \u003ctopics\u003e\n *   \u003cbrokers\u003e is a list of one or more Kafka brokers\n *   \u003cgroupId\u003e is a consumer group name to consume from topics\n *   \u003ctopics\u003e is a list of one or more kafka topics to consume from\n *\n * Example:\n *   Yarn client:\n *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n *      --conf \\\n *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n *      --master yarn\n *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n *      consumer-group topic1,topic2\n *   Yarn cluster:\n *    $ bin/run-example --files \\\n *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n *      --driver-java-options \\\n *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n *      -Djava.security.krb5.conf=./krb5.conf\" \\\n *      --conf \\\n *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n *      --master yarn --deploy-mode cluster \\\n *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n *      consumer-group topic1,topic2\n *\n * kafka_jaas.conf can manually create, template as:\n *   KafkaClient {\n *     com.sun.security.auth.module.Krb5LoginModule required\n *     keyTab=\"./kafka.service.keytab\"\n *     useKeyTab=true\n *     storeKey=true\n *     useTicketCache=false\n *     serviceName=\"kafka\"\n *     principal=\"kafka/host@EXAMPLE.COM\";\n *   };\n * kafka_driver_jaas.conf (used by yarn client) and kafka_jaas.conf are basically the same\n * except for some differences at 'keyTab'. In kafka_driver_jaas.conf, 'keyTab' should be\n * \"${keytab_path}/kafka.service.keytab\".\n * In addition, for IBM JVMs, please use 'com.ibm.security.auth.module.Krb5LoginModule'\n * instead of 'com.sun.security.auth.module.Krb5LoginModule'.\n *\n * Note that this example uses SASL_PLAINTEXT for simplicity; however,\n * SASL_PLAINTEXT has no SSL encryption and likely be less secure. Please consider\n * using SASL_SSL in production.\n */",
		"name": "org.apache.spark.examples.streaming.JavaDirectKerberizedKafkaWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Consumes messages from one or more topics in Kafka and does wordcount.\n * Usage: JavaDirectKerberizedKafkaWordCount \u003cbrokers\u003e \u003cgroupId\u003e \u003ctopics\u003e\n *   \u003cbrokers\u003e is a list of one or more Kafka brokers\n *   \u003cgroupId\u003e is a consumer group name to consume from topics\n *   \u003ctopics\u003e is a list of one or more kafka topics to consume from\n *\n * Example:\n *   Yarn client:\n *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n *      --conf \\\n *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n *      --master yarn\n *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n *      consumer-group topic1,topic2\n *   Yarn cluster:\n *    $ bin/run-example --files \\\n *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n *      --driver-java-options \\\n *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n *      -Djava.security.krb5.conf=./krb5.conf\" \\\n *      --conf \\\n *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n *      --master yarn --deploy-mode cluster \\\n *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n *      consumer-group topic1,topic2\n *\n * kafka_jaas.conf can manually create, template as:\n *   KafkaClient {\n *     com.sun.security.auth.module.Krb5LoginModule required\n *     keyTab=\"./kafka.service.keytab\"\n *     useKeyTab=true\n *     storeKey=true\n *     useTicketCache=false\n *     serviceName=\"kafka\"\n *     principal=\"kafka/host@EXAMPLE.COM\";\n *   };\n * kafka_driver_jaas.conf (used by yarn client) and kafka_jaas.conf are basically the same\n * except for some differences at 'keyTab'. In kafka_driver_jaas.conf, 'keyTab' should be\n * \"${keytab_path}/kafka.service.keytab\".\n * In addition, for IBM JVMs, please use 'com.ibm.security.auth.module.Krb5LoginModule'\n * instead of 'com.sun.security.auth.module.Krb5LoginModule'.\n *\n * Note that this example uses SASL_PLAINTEXT for simplicity; however,\n * SASL_PLAINTEXT has no SSL encryption and likely be less secure. Please consider\n * using SASL_SSL in production.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network every second.\n *\n * Usage: JavaNetworkWordCount \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Spark Streaming would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example org.apache.spark.examples.streaming.JavaNetworkWordCount localhost 9999`\n */",
		"name": "org.apache.spark.examples.streaming.JavaNetworkWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Counts words in UTF8 encoded, '\\n' delimited text received from the network every second.\n *\n * Usage: JavaNetworkWordCount \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Spark Streaming would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example org.apache.spark.examples.streaming.JavaNetworkWordCount localhost 9999`\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.examples.streaming.JavaQueueStream",
		"extends": "",
		"Methods": [
			{
				"signature": "private JavaQueueStream()",
				"documentation": ""
			},
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Java Bean class to be used with the example JavaSqlNetworkWordCount. */",
		"name": "org.apache.spark.examples.streaming.JavaRecord",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getWord()",
				"documentation": "/** Java Bean class to be used with the example JavaSqlNetworkWordCount. */"
			},
			{
				"signature": "public void setWord(String word)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Use this singleton to get or register a Broadcast variable.\n */",
		"name": "org.apache.spark.examples.streaming.JavaWordExcludeList",
		"extends": "",
		"Methods": [
			{
				"signature": "public static Broadcast\u003cList\u003cString\u003e\u003e getInstance(JavaSparkContext jsc)",
				"documentation": "/**\n * Use this singleton to get or register a Broadcast variable.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Use this singleton to get or register an Accumulator.\n */",
		"name": "org.apache.spark.examples.streaming.JavaDroppedWordsCounter",
		"extends": "",
		"Methods": [
			{
				"signature": "public static LongAccumulator getInstance(JavaSparkContext jsc)",
				"documentation": "/**\n * Use this singleton to get or register an Accumulator.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Counts words in text encoded with UTF8 received from the network every second. This example also\n * shows how to use lazily instantiated singleton instances for Accumulator and Broadcast so that\n * they can be registered on driver failures.\n *\n * Usage: JavaRecoverableNetworkWordCount \u003chostname\u003e \u003cport\u003e \u003ccheckpoint-directory\u003e \u003coutput-file\u003e\n *   \u003chostname\u003e and \u003cport\u003e describe the TCP server that Spark Streaming would connect to receive\n *   data. \u003ccheckpoint-directory\u003e directory to HDFS-compatible file system which checkpoint data\n *   \u003coutput-file\u003e file to which the word counts will be appended\n *\n * \u003ccheckpoint-directory\u003e and \u003coutput-file\u003e must be absolute paths\n *\n * To run this on your local machine, you need to first run a Netcat server\n *\n *      `$ nc -lk 9999`\n *\n * and run the example as\n *\n *      `$ ./bin/run-example org.apache.spark.examples.streaming.JavaRecoverableNetworkWordCount \\\n *              localhost 9999 ~/checkpoint/ ~/out`\n *\n * If the directory ~/checkpoint/ does not exist (e.g. running for the first time), it will create\n * a new StreamingContext (will print \"Creating new context\" to the console). Otherwise, if\n * checkpoint data exists in ~/checkpoint/, then it will create StreamingContext from\n * the checkpoint data.\n *\n * Refer to the online documentation for more details.\n */",
		"name": "org.apache.spark.examples.streaming.JavaRecoverableNetworkWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "private static JavaStreamingContext createContext(String ip,\n                                                    int port,\n                                                    String checkpointDirectory,\n                                                    String outputPath)",
				"documentation": "/**\n * Counts words in text encoded with UTF8 received from the network every second. This example also\n * shows how to use lazily instantiated singleton instances for Accumulator and Broadcast so that\n * they can be registered on driver failures.\n *\n * Usage: JavaRecoverableNetworkWordCount \u003chostname\u003e \u003cport\u003e \u003ccheckpoint-directory\u003e \u003coutput-file\u003e\n *   \u003chostname\u003e and \u003cport\u003e describe the TCP server that Spark Streaming would connect to receive\n *   data. \u003ccheckpoint-directory\u003e directory to HDFS-compatible file system which checkpoint data\n *   \u003coutput-file\u003e file to which the word counts will be appended\n *\n * \u003ccheckpoint-directory\u003e and \u003coutput-file\u003e must be absolute paths\n *\n * To run this on your local machine, you need to first run a Netcat server\n *\n *      `$ nc -lk 9999`\n *\n * and run the example as\n *\n *      `$ ./bin/run-example org.apache.spark.examples.streaming.JavaRecoverableNetworkWordCount \\\n *              localhost 9999 ~/checkpoint/ ~/out`\n *\n * If the directory ~/checkpoint/ does not exist (e.g. running for the first time), it will create\n * a new StreamingContext (will print \"Creating new context\" to the console). Otherwise, if\n * checkpoint data exists in ~/checkpoint/, then it will create StreamingContext from\n * the checkpoint data.\n *\n * Refer to the online documentation for more details.\n */"
			},
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Use DataFrames and SQL to count words in UTF8 encoded, '\\n' delimited text received from the\n * network every second.\n *\n * Usage: JavaSqlNetworkWordCount \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Spark Streaming would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example org.apache.spark.examples.streaming.JavaSqlNetworkWordCount localhost 9999`\n */",
		"name": "org.apache.spark.examples.streaming.JavaSqlNetworkWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Use DataFrames and SQL to count words in UTF8 encoded, '\\n' delimited text received from the\n * network every second.\n *\n * Usage: JavaSqlNetworkWordCount \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Spark Streaming would connect to receive data.\n *\n * To run this on your local machine, you need to first run a Netcat server\n *    `$ nc -lk 9999`\n * and then run the example\n *    `$ bin/run-example org.apache.spark.examples.streaming.JavaSqlNetworkWordCount localhost 9999`\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/** Lazily instantiated singleton instance of SparkSession */",
		"name": "org.apache.spark.examples.streaming.JavaSparkSessionSingleton",
		"extends": "",
		"Methods": [
			{
				"signature": "public static SparkSession getInstance(SparkConf sparkConf)",
				"documentation": "/** Lazily instantiated singleton instance of SparkSession */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Counts words cumulatively in UTF8 encoded, '\\n' delimited text received from the network every\n * second starting with initial value of word count.\n * Usage: JavaStatefulNetworkWordCount \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Spark Streaming would connect to receive\n * data.\n * \u003cp\u003e\n * To run this on your local machine, you need to first run a Netcat server\n * `$ nc -lk 9999`\n * and then run the example\n * `$ bin/run-example\n * org.apache.spark.examples.streaming.JavaStatefulNetworkWordCount localhost 9999`\n */",
		"name": "org.apache.spark.examples.streaming.JavaStatefulNetworkWordCount",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] args) throws Exception",
				"documentation": "/**\n * Counts words cumulatively in UTF8 encoded, '\\n' delimited text received from the network every\n * second starting with initial value of word count.\n * Usage: JavaStatefulNetworkWordCount \u003chostname\u003e \u003cport\u003e\n * \u003chostname\u003e and \u003cport\u003e describe the TCP server that Spark Streaming would connect to receive\n * data.\n * \u003cp\u003e\n * To run this on your local machine, you need to first run a Netcat server\n * `$ nc -lk 9999`\n * and then run the example\n * `$ bin/run-example\n * org.apache.spark.examples.streaming.JavaStatefulNetworkWordCount localhost 9999`\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a subset of the fields of an [[EdgeTriplet]] or [[EdgeContext]]. This allows the\n * system to populate only those fields for efficiency.\n */",
		"name": "org.apache.spark.graphx.TripletFields",
		"extends": "",
		"Methods": [
			{
				"signature": "public TripletFields()",
				"documentation": "/** Constructs a default TripletFields in which all fields are included. */"
			},
			{
				"signature": "public TripletFields(boolean useSrc, boolean useDst, boolean useEdge)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Criteria for filtering edges based on activeness. For internal use only.\n */",
		"name": "org.apache.spark.graphx.impl.EdgeActiveness",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.AbstractAppHandle",
		"extends": "",
		"Methods": [
			{
				"signature": "protected AbstractAppHandle(LauncherServer server)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkAppHandle.State getState()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getAppId()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void stop()",
				"documentation": ""
			},
			{
				"signature": "void setConnection(LauncherServer.ServerConnection connection)",
				"documentation": ""
			},
			{
				"signature": "LauncherConnection getConnection()",
				"documentation": ""
			},
			{
				"signature": "boolean isDisposed()",
				"documentation": ""
			},
			{
				"signature": "void setState(SparkAppHandle.State s)",
				"documentation": ""
			},
			{
				"signature": "void setState(SparkAppHandle.State s, boolean force)",
				"documentation": ""
			},
			{
				"signature": "void setAppId(String appId)",
				"documentation": ""
			},
			{
				"signature": "private void fireEvent(boolean isInfoChanged)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.launcher.SparkAppHandle"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.ChildProcAppHandle",
			"org.apache.spark.launcher.InProcessAppHandle"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Abstract Spark command builder that defines common functionality.\n */",
		"name": "org.apache.spark.launcher.AbstractCommandBuilder",
		"extends": "",
		"Methods": [
			{
				"signature": "AbstractCommandBuilder()",
				"documentation": "/**\n * Abstract Spark command builder that defines common functionality.\n */"
			},
			{
				"signature": "List\u003cString\u003e buildJavaCommand(String extraClassPath) throws IOException",
				"documentation": "/**\n   * Builds a list of arguments to run java.\n   *\n   * This method finds the java executable to use and appends JVM-specific options for running a\n   * class with Spark in the classpath. It also loads options from the \"java-opts\" file in the\n   * configuration directory being used.\n   *\n   * Callers should still add at least the class to run, as well as any arguments to pass to the\n   * class.\n   */"
			},
			{
				"signature": "void addOptionString(List\u003cString\u003e cmd, String options)",
				"documentation": ""
			},
			{
				"signature": "List\u003cString\u003e buildClassPath(String appClassPath) throws IOException",
				"documentation": "/**\n   * Builds the classpath for the application. Returns a list with one classpath entry per element;\n   * each entry is formatted in the way expected by \u003ci\u003ejava.net.URLClassLoader\u003c/i\u003e (more\n   * specifically, with trailing slashes for directories).\n   */"
			},
			{
				"signature": "private void addToClassPath(Set\u003cString\u003e cp, String entries)",
				"documentation": "/**\n   * Adds entries to the classpath.\n   *\n   * @param cp List to which the new entries are appended.\n   * @param entries New classpath entries (separated by File.pathSeparator).\n   */"
			},
			{
				"signature": "String getScalaVersion()",
				"documentation": ""
			},
			{
				"signature": "String getSparkHome()",
				"documentation": ""
			},
			{
				"signature": "String getenv(String key)",
				"documentation": ""
			},
			{
				"signature": "void setPropertiesFile(String path)",
				"documentation": ""
			},
			{
				"signature": "Map\u003cString, String\u003e getEffectiveConfig() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private Properties loadPropertiesFile() throws IOException",
				"documentation": "/**\n   * Loads the configuration file for the application, if it exists. This is either the\n   * user-specified properties file, or the spark-defaults.conf file under the Spark configuration\n   * directory.\n   */"
			},
			{
				"signature": "private String getConfDir()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.SparkClassCommandBuilder",
			"org.apache.spark.launcher.SparkSubmitCommandBuilder"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base class for launcher implementations.\n *\n * @since Spark 2.3.0\n */",
		"name": "org.apache.spark.launcher.AbstractLauncher",
		"extends": "",
		"Methods": [
			{
				"signature": "AbstractLauncher()",
				"documentation": "/**\n * Base class for launcher implementations.\n *\n * @since Spark 2.3.0\n */"
			},
			{
				"signature": "public T setPropertiesFile(String path)",
				"documentation": "/**\n   * Set a custom properties file with Spark configuration for the application.\n   *\n   * @param path Path to custom properties file to use.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T setConf(String key, String value)",
				"documentation": "/**\n   * Set a single configuration value for the application.\n   *\n   * @param key Configuration key.\n   * @param value The value to use.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T setAppName(String appName)",
				"documentation": "/**\n   * Set the application name.\n   *\n   * @param appName Application name.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T setMaster(String master)",
				"documentation": "/**\n   * Set the Spark master for the application.\n   *\n   * @param master Spark master.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T setDeployMode(String mode)",
				"documentation": "/**\n   * Set the deploy mode for the application.\n   *\n   * @param mode Deploy mode.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T setAppResource(String resource)",
				"documentation": "/**\n   * Set the main application resource. This should be the location of a jar file for Scala/Java\n   * applications, or a python script for PySpark applications.\n   *\n   * @param resource Path to the main application resource.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T setMainClass(String mainClass)",
				"documentation": "/**\n   * Sets the application class name for Java/Scala applications.\n   *\n   * @param mainClass Application's main class.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T addSparkArg(String arg)",
				"documentation": "/**\n   * Adds a no-value argument to the Spark invocation. If the argument is known, this method\n   * validates whether the argument is indeed a no-value argument, and throws an exception\n   * otherwise.\n   * \u003cp\u003e\n   * Use this method with caution. It is possible to create an invalid Spark command by passing\n   * unknown arguments to this method, since those are allowed for forward compatibility.\n   *\n   * @since 1.5.0\n   * @param arg Argument to add.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T addSparkArg(String name, String value)",
				"documentation": "/**\n   * Adds an argument with a value to the Spark invocation. If the argument name corresponds to\n   * a known argument, the code validates that the argument actually expects a value, and throws\n   * an exception otherwise.\n   * \u003cp\u003e\n   * It is safe to add arguments modified by other methods in this class (such as\n   * {@link #setMaster(String)} - the last invocation will be the one to take effect.\n   * \u003cp\u003e\n   * Use this method with caution. It is possible to create an invalid Spark command by passing\n   * unknown arguments to this method, since those are allowed for forward compatibility.\n   *\n   * @since 1.5.0\n   * @param name Name of argument to add.\n   * @param value Value of the argument.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T addAppArgs(String... args)",
				"documentation": "/**\n   * Adds command line arguments for the application.\n   *\n   * @param args Arguments to pass to the application's main class.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T addJar(String jar)",
				"documentation": "/**\n   * Adds a jar file to be submitted with the application.\n   *\n   * @param jar Path to the jar file.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T addFile(String file)",
				"documentation": "/**\n   * Adds a file to be submitted with the application.\n   *\n   * @param file Path to the file.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T addPyFile(String file)",
				"documentation": "/**\n   * Adds a python file / zip / egg to be submitted with the application.\n   *\n   * @param file Path to the file.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public T setVerbose(boolean verbose)",
				"documentation": "/**\n   * Enables verbose reporting for SparkSubmit.\n   *\n   * @param verbose Whether to enable verbose output.\n   * @return This launcher.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.InProcessLauncher",
			"org.apache.spark.launcher.SparkLauncher"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.AbstractLauncher.ArgumentValidator"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.launcher.AbstractLauncher.ArgumentValidator"
		]
	},
	{
		"documentation": "/**\n   * Starts a Spark application.\n   *\n   * \u003cp\u003e\n   * This method returns a handle that provides information about the running application and can\n   * be used to do basic interaction with it.\n   * \u003cp\u003e\n   * The returned handle assumes that the application will instantiate a single SparkContext\n   * during its lifetime. Once that context reports a final state (one that indicates the\n   * SparkContext has stopped), the handle will not perform new state transitions, so anything\n   * that happens after that cannot be monitored. If the underlying application is launched as\n   * a child process, {@link SparkAppHandle#kill()} can still be used to kill the child process.\n   *\n   * @since 1.6.0\n   * @param listeners Listeners to add to the handle before the app is launched.\n   * @return A handle for the launched application.\n   */",
		"name": "org.apache.spark.launcher.AbstractLauncher.ArgumentValidator",
		"extends": "org.apache.spark.launcher.SparkSubmitOptionParser",
		"Methods": [
			{
				"signature": "ArgumentValidator(boolean hasValue)",
				"documentation": "/**\n   * Starts a Spark application.\n   *\n   * \u003cp\u003e\n   * This method returns a handle that provides information about the running application and can\n   * be used to do basic interaction with it.\n   * \u003cp\u003e\n   * The returned handle assumes that the application will instantiate a single SparkContext\n   * during its lifetime. Once that context reports a final state (one that indicates the\n   * SparkContext has stopped), the handle will not perform new state transitions, so anything\n   * that happens after that cannot be monitored. If the underlying application is launched as\n   * a child process, {@link SparkAppHandle#kill()} can still be used to kill the child process.\n   *\n   * @since 1.6.0\n   * @param listeners Listeners to add to the handle before the app is launched.\n   * @return A handle for the launched application.\n   */"
			},
			{
				"signature": "@Override\n    protected boolean handle(String opt, String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected boolean handleUnknown(String opt)",
				"documentation": ""
			},
			{
				"signature": "protected void handleExtraArgs(List\u003cString\u003e extra)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.AbstractLauncher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Handle implementation for monitoring apps started as a child process.\n */",
		"name": "org.apache.spark.launcher.ChildProcAppHandle",
		"extends": "org.apache.spark.launcher.AbstractAppHandle",
		"Methods": [
			{
				"signature": "ChildProcAppHandle(LauncherServer server)",
				"documentation": "/**\n * Handle implementation for monitoring apps started as a child process.\n */"
			},
			{
				"signature": "@Override\n  public Optional\u003cThrowable\u003e getError()",
				"documentation": "/**\n   * Parses the logs of {@code spark-submit} and returns the last exception thrown.\n   * \u003cp\u003e\n   * Since {@link SparkLauncher} runs {@code spark-submit} in a sub-process, it's difficult to\n   * accurately retrieve the full {@link Throwable} from the {@code spark-submit} process.\n   * This method parses the logs of the sub-process and provides a best-effort attempt at\n   * returning the last exception thrown by the {@code spark-submit} process. Only the exception\n   * message is parsed, the associated stacktrace is meaningless.\n   *\n   * @return an {@link Optional} containing a {@link RuntimeException} with the parsed\n   * exception, otherwise returns a {@link Optional#EMPTY}\n   */"
			},
			{
				"signature": "void setChildProc(Process childProc, String loggerName, InputStream logStream)",
				"documentation": ""
			},
			{
				"signature": "void monitorChild()",
				"documentation": "/**\n   * Wait for the child process to exit and update the handle's state if necessary, according to\n   * the exit code.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Helper methods for command builders.\n */",
		"name": "org.apache.spark.launcher.CommandBuilderUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "static boolean isEmpty(String s)",
				"documentation": "/** Returns whether the given string is null or empty. */"
			},
			{
				"signature": "static String join(String sep, String... elements)",
				"documentation": "/** Joins a list of strings using the given separator. */"
			},
			{
				"signature": "static String join(String sep, Iterable\u003cString\u003e elements)",
				"documentation": "/** Joins a list of strings using the given separator. */"
			},
			{
				"signature": "static String firstNonEmptyValue(String key, Map\u003c?, ?\u003e... maps)",
				"documentation": "/**\n   * Returns the first non-empty value mapped to the given key in the given maps, or null otherwise.\n   */"
			},
			{
				"signature": "static String firstNonEmpty(String... candidates)",
				"documentation": "/** Returns the first non-empty, non-null string in the given list, or null otherwise. */"
			},
			{
				"signature": "static String getLibPathEnvName()",
				"documentation": "/** Returns the name of the env variable that holds the native library path. */"
			},
			{
				"signature": "static boolean isWindows()",
				"documentation": "/** Returns whether the OS is Windows. */"
			},
			{
				"signature": "static void mergeEnvPathList(Map\u003cString, String\u003e userEnv, String envKey, String pathList)",
				"documentation": "/**\n   * Updates the user environment, appending the given pathList to the existing value of the given\n   * environment variable (or setting it if it hasn't yet been set).\n   */"
			},
			{
				"signature": "static List\u003cString\u003e parseOptionString(String s)",
				"documentation": "/**\n   * Parse a string as if it were a list of arguments, following bash semantics.\n   * For example:\n   *\n   * Input: \"\\\"ab cd\\\" efgh 'i \\\" j'\"\n   * Output: [ \"ab cd\", \"efgh\", \"i \\\" j\" ]\n   */"
			},
			{
				"signature": "static void checkNotNull(Object o, String arg)",
				"documentation": "/** Throws IllegalArgumentException if the given object is null. */"
			},
			{
				"signature": "static void checkArgument(boolean check, String msg, Object... args)",
				"documentation": "/** Throws IllegalArgumentException with the given message if the check is false. */"
			},
			{
				"signature": "static void checkState(boolean check, String msg, Object... args)",
				"documentation": "/** Throws IllegalStateException with the given message if the check is false. */"
			},
			{
				"signature": "static String quoteForBatchScript(String arg)",
				"documentation": "/**\n   * Quote a command argument for a command to be run by a Windows batch script, if the argument\n   * needs quoting. Arguments only seem to need quotes in batch scripts if they have certain\n   * special characters, some of which need extra (and different) escaping.\n   *\n   *  For example:\n   *    original single argument: ab=\"cde fgh\"\n   *    quoted: \"ab^=\"\"cde fgh\"\"\"\n   */"
			},
			{
				"signature": "static String quoteForCommandString(String s)",
				"documentation": "/**\n   * Quotes a string so that it can be used in a command string.\n   * Basically, just add simple escapes. E.g.:\n   *    original single argument : ab \"cd\" ef\n   *    after: \"ab \\\"cd\\\" ef\"\n   *\n   * This can be parsed back into a single argument by python's \"shlex.split()\" function.\n   */"
			},
			{
				"signature": "static int javaMajorVersion(String javaVersion)",
				"documentation": "/**\n   * Get the major version of the java version string supplied. This method\n   * accepts any JEP-223-compliant strings (9-ea, 9+100), as well as legacy\n   * version strings such as 1.7.0_79\n   */"
			},
			{
				"signature": "static String findJarsDir(String sparkHome, String scalaVersion, boolean failIfNotFound)",
				"documentation": "/**\n   * Find the location of the Spark jars dir, depending on whether we're looking at a build\n   * or a distribution directory.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.SparkLauncher"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An object input stream that only allows classes used by the launcher protocol to be in the\n * serialized stream. See SPARK-20922.\n */",
		"name": "org.apache.spark.launcher.FilteredObjectInputStream",
		"extends": "java.io.ObjectInputStream",
		"Methods": [
			{
				"signature": "FilteredObjectInputStream(InputStream is) throws IOException",
				"documentation": "/**\n * An object input stream that only allows classes used by the launcher protocol to be in the\n * serialized stream. See SPARK-20922.\n */"
			},
			{
				"signature": "@Override\n  protected Class\u003c?\u003e resolveClass(ObjectStreamClass desc)\n      throws IOException, ClassNotFoundException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.InProcessAppHandle",
		"extends": "org.apache.spark.launcher.AbstractAppHandle",
		"Methods": [
			{
				"signature": "InProcessAppHandle(LauncherServer server)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Optional\u003cThrowable\u003e getError()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * In-process launcher for Spark applications.\n * \u003cp\u003e\n * Use this class to start Spark applications programmatically. Applications launched using this\n * class will run in the same process as the caller.\n * \u003cp\u003e\n * Because Spark only supports a single active instance of \u003ccode\u003eSparkContext\u003c/code\u003e per JVM, code\n * that uses this class should be careful about which applications are launched. It's recommended\n * that this launcher only be used to launch applications in cluster mode.\n * \u003cp\u003e\n * Also note that, when running applications in client mode, JVM-related configurations (like\n * driver memory or configs which modify the driver's class path) do not take effect. Logging\n * configuration is also inherited from the parent application.\n *\n * @since Spark 2.3.0\n */",
		"name": "org.apache.spark.launcher.InProcessLauncher",
		"extends": "org.apache.spark.launcher.AbstractLauncher",
		"Methods": [
			{
				"signature": "@Override\n  public SparkAppHandle startApplication(SparkAppHandle.Listener... listeners) throws IOException",
				"documentation": "/**\n   * Starts a Spark application.\n   *\n   * @see AbstractLauncher#startApplication(SparkAppHandle.Listener...)\n   * @param listeners Listeners to add to the handle before the app is launched.\n   * @return A handle for the launched application.\n   */"
			},
			{
				"signature": "@Override\n  InProcessLauncher self()",
				"documentation": ""
			},
			{
				"signature": "Method findSparkSubmit() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.InProcessLauncherSuite.TestInProcessLauncher"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This helper class is used to place the all `--add-opens` options\n * required by Spark when using Java 17. `DEFAULT_MODULE_OPTIONS` has added\n * `-XX:+IgnoreUnrecognizedVMOptions` to be compatible with Java 8 and Java 11.\n */",
		"name": "org.apache.spark.launcher.JavaModuleOptions",
		"extends": "",
		"Methods": [
			{
				"signature": "private static final String[] DEFAULT_MODULE_OPTIONS =",
				"documentation": "/**\n * This helper class is used to place the all `--add-opens` options\n * required by Spark when using Java 17. `DEFAULT_MODULE_OPTIONS` has added\n * `-XX:+IgnoreUnrecognizedVMOptions` to be compatible with Java 8 and Java 11.\n */"
			},
			{
				"signature": "public static String defaultModuleOptions()",
				"documentation": "/**\n     * Returns the default Java options related to `--add-opens' and\n     * `-XX:+IgnoreUnrecognizedVMOptions` used by Spark.\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Encapsulates a connection between a launcher server and client. This takes care of the\n * communication (sending and receiving messages), while processing of messages is left for\n * the implementations.\n */",
		"name": "org.apache.spark.launcher.LauncherConnection",
		"extends": "",
		"Methods": [
			{
				"signature": "LauncherConnection(Socket socket) throws IOException",
				"documentation": "/**\n * Encapsulates a connection between a launcher server and client. This takes care of the\n * communication (sending and receiving messages), while processing of messages is left for\n * the implementations.\n */"
			},
			{
				"signature": "@Override\n  public void run()",
				"documentation": ""
			},
			{
				"signature": "boolean isOpen()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Closeable",
			"Runnable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.ServerConnection",
			"org.apache.spark.launcher.LauncherServerSuite.TestClient"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Message definitions for the launcher communication protocol. These messages must remain\n * backwards-compatible, so that the launcher can talk to older versions of Spark that support\n * the protocol.\n */",
		"name": "org.apache.spark.launcher.LauncherProtocol",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.LauncherProtocol.Hello",
			"org.apache.spark.launcher.LauncherProtocol.SetAppId",
			"org.apache.spark.launcher.LauncherProtocol.SetState"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.launcher.LauncherProtocol.Message",
			"org.apache.spark.launcher.LauncherProtocol.Hello",
			"org.apache.spark.launcher.LauncherProtocol.SetAppId",
			"org.apache.spark.launcher.LauncherProtocol.SetState",
			"org.apache.spark.launcher.LauncherProtocol.Stop"
		]
	},
	{
		"documentation": "/** Spark conf key used to propagate the app secret for in-process launches. */",
		"name": "org.apache.spark.launcher.LauncherProtocol.Message",
		"extends": "",
		"Methods": [],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.LauncherProtocol.Hello",
			"org.apache.spark.launcher.LauncherProtocol.SetAppId",
			"org.apache.spark.launcher.LauncherProtocol.SetState",
			"org.apache.spark.launcher.LauncherProtocol.Stop"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Hello message, sent from client to server.\n   */",
		"name": "org.apache.spark.launcher.LauncherProtocol.Hello",
		"extends": "org.apache.spark.launcher.LauncherProtocol.Message",
		"Methods": [
			{
				"signature": "Hello(String secret, String version)",
				"documentation": "/**\n   * Hello message, sent from client to server.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.LauncherProtocol"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * SetAppId message, sent from client to server.\n   */",
		"name": "org.apache.spark.launcher.LauncherProtocol.SetAppId",
		"extends": "org.apache.spark.launcher.LauncherProtocol.Message",
		"Methods": [
			{
				"signature": "SetAppId(String appId)",
				"documentation": "/**\n   * SetAppId message, sent from client to server.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.LauncherProtocol"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * SetState message, sent from client to server.\n   */",
		"name": "org.apache.spark.launcher.LauncherProtocol.SetState",
		"extends": "org.apache.spark.launcher.LauncherProtocol.Message",
		"Methods": [
			{
				"signature": "SetState(SparkAppHandle.State state)",
				"documentation": "/**\n   * SetState message, sent from client to server.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.LauncherProtocol"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Stop message, send from server to client to stop the application.\n   */",
		"name": "org.apache.spark.launcher.LauncherProtocol.Stop",
		"extends": "org.apache.spark.launcher.LauncherProtocol.Message",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A server that listens locally for connections from client launched by the library. Each client\n * has a secret that it needs to send to the server to identify itself and establish the session.\n *\n * I/O is currently blocking (one thread per client). Clients have a limited time to connect back\n * to the server, otherwise the server will ignore the connection.\n *\n * === Architecture Overview ===\n *\n * The launcher server is used when Spark apps are launched as separate processes than the calling\n * app. It looks more or less like the following:\n *\n *         -----------------------                       -----------------------\n *         |      User App       |     spark-submit      |      Spark App      |\n *         |                     |  -------------------\u003e |                     |\n *         |         ------------|                       |-------------        |\n *         |         |           |        hello          |            |        |\n *         |         | L. Server |\u003c----------------------| L. Backend |        |\n *         |         |           |                       |            |        |\n *         |         -------------                       -----------------------\n *         |               |     |                              ^\n *         |               v     |                              |\n *         |        -------------|                              |\n *         |        |            |      \u003cper-app channel\u003e       |\n *         |        | App Handle |\u003c------------------------------\n *         |        |            |\n *         -----------------------\n *\n * The server is started on demand and remains active while there are active or outstanding clients,\n * to avoid opening too many ports when multiple clients are launched. Each client is given a unique\n * secret, and have a limited amount of time to connect back\n * ({@link SparkLauncher#CHILD_CONNECTION_TIMEOUT}), at which point the server will throw away\n * that client's state. A client is only allowed to connect back to the server once.\n *\n * The launcher server listens on the localhost only, so it doesn't need access controls (aside from\n * the per-app secret) nor encryption. It thus requires that the launched app has a local process\n * that communicates with the server. In cluster mode, this means that the client that launches the\n * application must remain alive for the duration of the application (or until the app handle is\n * disconnected).\n */",
		"name": "org.apache.spark.launcher.LauncherServer",
		"extends": "",
		"Methods": [
			{
				"signature": "do",
				"documentation": "/** For creating secrets used for communication with child processes. */"
			},
			{
				"signature": "private LauncherServer() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "void ref()",
				"documentation": ""
			},
			{
				"signature": "void unref()",
				"documentation": ""
			},
			{
				"signature": "int getPort()",
				"documentation": ""
			},
			{
				"signature": "void unregister(AbstractAppHandle handle)",
				"documentation": "/**\n   * Removes the client handle from the pending list (in case it's still there), and unrefs\n   * the server.\n   */"
			},
			{
				"signature": "private void acceptConnections()",
				"documentation": ""
			},
			{
				"signature": "@Override\n          public void run()",
				"documentation": ""
			},
			{
				"signature": "private long getConnectionTimeout()",
				"documentation": ""
			},
			{
				"signature": "private String createSecret()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.ServerConnection"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.launcher.ServerConnection"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.ServerConnection",
		"extends": "org.apache.spark.launcher.LauncherConnection",
		"Methods": [
			{
				"signature": "ServerConnection(Socket socket, TimerTask timeout) throws IOException",
				"documentation": ""
			},
			{
				"signature": "void setConnectionThread(Thread t)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void handle(Message msg) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void waitForClose() throws IOException",
				"documentation": "/**\n     * Wait for the remote side to close the connection so that any pending data is processed.\n     * This ensures any changes reported by the child application take effect.\n     *\n     * This method allows a short period for the above to happen (same amount of time as the\n     * connection timeout, which is configurable). This should be fine for well-behaved\n     * applications, where they close the connection around the same time the app handle detects the\n     * app has finished.\n     *\n     * In case the connection is not closed within the grace period, this method forcefully closes\n     * it and any subsequent data that may arrive will be ignored.\n     */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.LauncherServer"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Command line interface for the Spark launcher. Used internally by Spark scripts.\n */",
		"name": "org.apache.spark.launcher.Main",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void main(String[] argsArray) throws Exception",
				"documentation": "/**\n   * Usage: Main [class] [class args]\n   * \u003cp\u003e\n   * This CLI works in two different modes:\n   * \u003cul\u003e\n   *   \u003cli\u003e\"spark-submit\": if \u003ci\u003eclass\u003c/i\u003e is \"org.apache.spark.deploy.SparkSubmit\", the\n   *   {@link SparkLauncher} class is used to launch a Spark application.\u003c/li\u003e\n   *   \u003cli\u003e\"spark-class\": if another class is provided, an internal Spark class is run.\u003c/li\u003e\n   * \u003c/ul\u003e\n   *\n   * This class works in tandem with the \"bin/spark-class\" script on Unix-like systems, and\n   * \"bin/spark-class2.cmd\" batch script on Windows to execute the final command.\n   * \u003cp\u003e\n   * On Unix-like systems, the output is a list of command arguments, separated by the NULL\n   * character. On Windows, the output is a command line suitable for direct execution from the\n   * script.\n   */"
			},
			{
				"signature": "private static List\u003cString\u003e buildCommand(\n      AbstractCommandBuilder builder,\n      Map\u003cString, String\u003e env,\n      boolean printLaunchCommand) throws IOException, IllegalArgumentException",
				"documentation": "/**\n   * Prepare spark commands with the appropriate command builder.\n   * If printLaunchCommand is set then the commands will be printed to the stderr.\n   */"
			},
			{
				"signature": "private static String prepareWindowsCommand(List\u003cString\u003e cmd, Map\u003cString, String\u003e childEnv)",
				"documentation": "/**\n   * Prepare a command line for execution from a Windows batch script.\n   *\n   * The method quotes all arguments so that spaces are handled as expected. Quotes within arguments\n   * are \"double quoted\" (which is batch for escaping a quote). This page has more details about\n   * quoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html\n   */"
			},
			{
				"signature": "private static List\u003cString\u003e prepareBashCommand(List\u003cString\u003e cmd, Map\u003cString, String\u003e childEnv)",
				"documentation": "/**\n   * Prepare the command for execution from a bash script. The final command will have commands to\n   * set up any needed environment variables needed by the child process.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.Main.MainClassOptionParser"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.launcher.Main.MainClassOptionParser"
		]
	},
	{
		"documentation": "/**\n   * A parser used when command line parsing fails for spark-submit. It's used as a best-effort\n   * at trying to identify the class the user wanted to invoke, since that may require special\n   * usage strings (handled by SparkSubmitArguments).\n   */",
		"name": "org.apache.spark.launcher.Main.MainClassOptionParser",
		"extends": "org.apache.spark.launcher.SparkSubmitOptionParser",
		"Methods": [
			{
				"signature": "@Override\n    protected boolean handle(String opt, String value)",
				"documentation": "/**\n   * A parser used when command line parsing fails for spark-submit. It's used as a best-effort\n   * at trying to identify the class the user wanted to invoke, since that may require special\n   * usage strings (handled by SparkSubmitArguments).\n   */"
			},
			{
				"signature": "@Override\n    protected boolean handleUnknown(String opt)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void handleExtraArgs(List\u003cString\u003e extra)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.Main"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.NamedThreadFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "NamedThreadFactory(String nameFormat)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Thread newThread(Runnable r)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.concurrent.ThreadFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Redirects lines read from a given input stream to a j.u.l.Logger (at INFO level).\n */",
		"name": "org.apache.spark.launcher.OutputRedirector",
		"extends": "",
		"Methods": [
			{
				"signature": "OutputRedirector(InputStream in, String loggerName, ThreadFactory tf)",
				"documentation": "/**\n * Redirects lines read from a given input stream to a j.u.l.Logger (at INFO level).\n */"
			},
			{
				"signature": "OutputRedirector(\n      InputStream in,\n      String loggerName,\n      ThreadFactory tf,\n      ChildProcAppHandle callback)",
				"documentation": ""
			},
			{
				"signature": "private void redirect()",
				"documentation": ""
			},
			{
				"signature": "void stop()",
				"documentation": "/**\n   * This method just stops the output of the process from showing up in the local logs.\n   * The child's output will still be read (and, thus, the redirect thread will still be\n   * alive) to avoid the child process hanging because of lack of output buffer.\n   */"
			},
			{
				"signature": "boolean isAlive()",
				"documentation": ""
			},
			{
				"signature": "Throwable getError()",
				"documentation": ""
			},
			{
				"signature": "private static boolean containsIgnoreCase(String str, String searchStr)",
				"documentation": "/**\n   * Copied from Apache Commons Lang {@code StringUtils#containsIgnoreCase(String, String)}\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A handle to a running Spark application.\n * \u003cp\u003e\n * Provides runtime information about the underlying Spark application, and actions to control it.\n *\n * @since 1.6.0\n */",
		"name": "org.apache.spark.launcher.SparkAppHandle",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.launcher.AbstractAppHandle"
		],
		"uses": [
			"org.apache.spark.launcher.State"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.launcher.State",
			"org.apache.spark.launcher.Listener"
		]
	},
	{
		"documentation": "/**\n   * Represents the application's state. A state can be \"final\", in which case it will not change\n   * after it's reached, and means the application is not running anymore.\n   *\n   * @since 1.6.0\n   */",
		"name": "org.apache.spark.launcher.State",
		"extends": "",
		"Methods": [
			{
				"signature": "State(boolean isFinal)",
				"documentation": "/** The Spark Submit JVM exited with a unknown status. */"
			},
			{
				"signature": "public boolean isFinal()",
				"documentation": "/**\n     * Whether this state is a final state, meaning the application is not running anymore\n     * once it's reached.\n     */"
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.SparkAppHandle"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Listener for updates to a handle's state. The callbacks do not receive information about\n   * what exactly has changed, just that an update has occurred.\n   *\n   * @since 1.6.0\n   */",
		"name": "org.apache.spark.launcher.Listener",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Command builder for internal Spark classes.\n * \u003cp\u003e\n * This class handles building the command to launch all internal Spark classes except for\n * SparkSubmit (which is handled by {@link SparkSubmitCommandBuilder} class.\n */",
		"name": "org.apache.spark.launcher.SparkClassCommandBuilder",
		"extends": "org.apache.spark.launcher.AbstractCommandBuilder",
		"Methods": [
			{
				"signature": "SparkClassCommandBuilder(String className, List\u003cString\u003e classArgs)",
				"documentation": "/**\n * Command builder for internal Spark classes.\n * \u003cp\u003e\n * This class handles building the command to launch all internal Spark classes except for\n * SparkSubmit (which is handled by {@link SparkSubmitCommandBuilder} class.\n */"
			},
			{
				"signature": "@Override\n  public List\u003cString\u003e buildCommand(Map\u003cString, String\u003e env)\n      throws IOException, IllegalArgumentException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Launcher for Spark applications.\n * \u003cp\u003e\n * Use this class to start Spark applications programmatically. The class uses a builder pattern\n * to allow clients to configure the Spark application and launch it as a child process.\n * \u003c/p\u003e\n */",
		"name": "org.apache.spark.launcher.SparkLauncher",
		"extends": "org.apache.spark.launcher.AbstractLauncher",
		"Methods": [
			{
				"signature": "public static void setConfig(String name, String value)",
				"documentation": "/**\n   * Set a configuration value for the launcher library. These config values do not affect the\n   * launched application, but rather the behavior of the launcher library itself when managing\n   * applications.\n   *\n   * @since 1.6.0\n   * @param name Config name.\n   * @param value Config value.\n   */"
			},
			{
				"signature": "public SparkLauncher()",
				"documentation": ""
			},
			{
				"signature": "public SparkLauncher(Map\u003cString, String\u003e env)",
				"documentation": "/**\n   * Creates a launcher that will set the given environment variables in the child.\n   *\n   * @param env Environment variables to set.\n   */"
			},
			{
				"signature": "public SparkLauncher setJavaHome(String javaHome)",
				"documentation": "/**\n   * Set a custom JAVA_HOME for launching the Spark application.\n   *\n   * @param javaHome Path to the JAVA_HOME to use.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public SparkLauncher setSparkHome(String sparkHome)",
				"documentation": "/**\n   * Set a custom Spark installation location for the application.\n   *\n   * @param sparkHome Path to the Spark installation to use.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public SparkLauncher directory(File dir)",
				"documentation": "/**\n   * Sets the working directory of spark-submit.\n   *\n   * @param dir The directory to set as spark-submit's working directory.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public SparkLauncher redirectError()",
				"documentation": "/**\n   * Specifies that stderr in spark-submit should be redirected to stdout.\n   *\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public SparkLauncher redirectError(ProcessBuilder.Redirect to)",
				"documentation": "/**\n   * Redirects error output to the specified Redirect.\n   *\n   * @param to The method of redirection.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public SparkLauncher redirectOutput(ProcessBuilder.Redirect to)",
				"documentation": "/**\n   * Redirects standard output to the specified Redirect.\n   *\n   * @param to The method of redirection.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public SparkLauncher redirectError(File errFile)",
				"documentation": "/**\n   * Redirects error output to the specified File.\n   *\n   * @param errFile The file to which stderr is written.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public SparkLauncher redirectOutput(File outFile)",
				"documentation": "/**\n   * Redirects error output to the specified File.\n   *\n   * @param outFile The file to which stdout is written.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "public SparkLauncher redirectToLog(String loggerName)",
				"documentation": "/**\n   * Sets all output to be logged and redirected to a logger with the specified name.\n   *\n   * @param loggerName The name of the logger to log stdout and stderr.\n   * @return This launcher.\n   */"
			},
			{
				"signature": "@Override\n  public SparkLauncher setPropertiesFile(String path)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher setConf(String key, String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher setAppName(String appName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher setMaster(String master)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher setDeployMode(String mode)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher setAppResource(String resource)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher setMainClass(String mainClass)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher addSparkArg(String arg)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher addSparkArg(String name, String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher addAppArgs(String... args)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher addJar(String jar)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher addFile(String file)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher addPyFile(String file)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SparkLauncher setVerbose(boolean verbose)",
				"documentation": ""
			},
			{
				"signature": "public Process launch() throws IOException",
				"documentation": "/**\n   * Launches a sub-process that will start the configured Spark application.\n   * \u003cp\u003e\n   * The {@link #startApplication(SparkAppHandle.Listener...)} method is preferred when launching\n   * Spark, since it provides better control of the child application.\n   *\n   * @return A process handle for the Spark app.\n   */"
			},
			{
				"signature": "@Override\n  public SparkAppHandle startApplication(SparkAppHandle.Listener... listeners) throws IOException",
				"documentation": "/**\n   * Starts a Spark application.\n   *\n   * \u003cp\u003e\n   * Applications launched by this launcher run as child processes. The child's stdout and stderr\n   * are merged and written to a logger (see \u003ccode\u003ejava.util.logging\u003c/code\u003e) only if redirection\n   * has not otherwise been configured on this \u003ccode\u003eSparkLauncher\u003c/code\u003e. The logger's name can be\n   * defined by setting {@link #CHILD_PROCESS_LOGGER_NAME} in the app's configuration. If that\n   * option is not set, the code will try to derive a name from the application's name or main\n   * class / script file. If those cannot be determined, an internal, unique name will be used.\n   * In all cases, the logger name will start with \"org.apache.spark.launcher.app\", to fit more\n   * easily into the configuration of commonly-used logging systems.\n   *\n   * @since 1.6.0\n   * @see AbstractLauncher#startApplication(SparkAppHandle.Listener...)\n   * @param listeners Listeners to add to the handle before the app is launched.\n   * @return A handle for the launched application.\n   */"
			},
			{
				"signature": "private ProcessBuilder createBuilder() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  SparkLauncher self()",
				"documentation": ""
			},
			{
				"signature": "String findSparkSubmit()",
				"documentation": ""
			},
			{
				"signature": "private String getLoggerName() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.ChildProcAppHandleSuite.TestSparkLauncher"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.CommandBuilderUtils"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Special command builder for handling a CLI invocation of SparkSubmit.\n * \u003cp\u003e\n * This builder adds command line parsing compatible with SparkSubmit. It handles setting\n * driver-side options and special parsing behavior needed for the special-casing certain internal\n * Spark applications.\n * \u003cp\u003e\n * This class has also some special features to aid launching shells (pyspark and sparkR) and also\n * examples.\n */",
		"name": "org.apache.spark.launcher.SparkSubmitCommandBuilder",
		"extends": "org.apache.spark.launcher.AbstractCommandBuilder",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n   * This map must match the class names for available special classes, since this modifies the way\n   * command line parsing works. This maps the class name to the resource to use when calling\n   * spark-submit.\n   */"
			},
			{
				"signature": "SparkSubmitCommandBuilder()",
				"documentation": "/**\n   * This constructor is used when creating a user-configurable launcher. It allows the\n   * spark-submit argument list to be modified after creation.\n   */"
			},
			{
				"signature": "SparkSubmitCommandBuilder(List\u003cString\u003e args)",
				"documentation": "/**\n   * This constructor is used when invoking spark-submit; it parses and validates arguments\n   * provided by the user on the command line.\n   */"
			},
			{
				"signature": "@Override\n  public List\u003cString\u003e buildCommand(Map\u003cString, String\u003e env)\n      throws IOException, IllegalArgumentException",
				"documentation": ""
			},
			{
				"signature": "List\u003cString\u003e buildSparkSubmitArgs()",
				"documentation": ""
			},
			{
				"signature": "private List\u003cString\u003e buildSparkSubmitCommand(Map\u003cString, String\u003e env)\n      throws IOException, IllegalArgumentException",
				"documentation": ""
			},
			{
				"signature": "private void checkJavaOptions(String javaOptions)",
				"documentation": ""
			},
			{
				"signature": "private List\u003cString\u003e buildPySparkShellCommand(Map\u003cString, String\u003e env) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private List\u003cString\u003e buildSparkRCommand(Map\u003cString, String\u003e env) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void constructEnvVarArgs(\n      Map\u003cString, String\u003e env,\n      String submitArgsEnvVariable) throws IOException",
				"documentation": ""
			},
			{
				"signature": "boolean isClientMode(Map\u003cString, String\u003e userProps)",
				"documentation": ""
			},
			{
				"signature": "private boolean isThriftServer(String mainClass)",
				"documentation": "/**\n   * Return whether the given main class represents a thrift server.\n   */"
			},
			{
				"signature": "private String findExamplesAppJar()",
				"documentation": ""
			},
			{
				"signature": "private List\u003cString\u003e findExamplesJars()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.OptionParser"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.launcher.OptionParser"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.OptionParser",
		"extends": "org.apache.spark.launcher.SparkSubmitOptionParser",
		"Methods": [
			{
				"signature": "OptionParser(boolean errorOnUnknownArgs)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected boolean handle(String opt, String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected boolean handleUnknown(String opt)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void handleExtraArgs(List\u003cString\u003e extra)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.SparkSubmitCommandBuilder"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Parser for spark-submit command line options.\n * \u003cp\u003e\n * This class encapsulates the parsing code for spark-submit command line options, so that there\n * is a single list of options that needs to be maintained (well, sort of, but it makes it harder\n * to break things).\n */",
		"name": "org.apache.spark.launcher.SparkSubmitOptionParser",
		"extends": "",
		"Methods": [
			{
				"signature": "final String[][] opts =",
				"documentation": "/**\n   * This is the canonical list of spark-submit options. Each entry in the array contains the\n   * different aliases for the same option; the first element of each entry is the \"official\"\n   * name of the option, passed to {@link #handle(String, String)}.\n   * \u003cp\u003e\n   * Options not listed here nor in the \"switch\" list below will result in a call to\n   * {@link #handleUnknown(String)}.\n   * \u003cp\u003e\n   * These two arrays are visible for tests.\n   */"
			},
			{
				"signature": "final String[][] switches =",
				"documentation": "/**\n   * List of switches (command line options that do not take parameters) recognized by spark-submit.\n   */"
			},
			{
				"signature": "protected final void parse(List\u003cString\u003e args)",
				"documentation": "/**\n   * Parse a list of spark-submit command line options.\n   * \u003cp\u003e\n   * See SparkSubmitArguments.scala for a more formal description of available options.\n   *\n   * @throws IllegalArgumentException If an error is found during parsing.\n   */"
			},
			{
				"signature": "protected boolean handle(String opt, String value)",
				"documentation": "/**\n   * Callback for when an option with an argument is parsed.\n   *\n   * @param opt The long name of the cli option (might differ from actual command line).\n   * @param value The value. This will be \u003ci\u003enull\u003c/i\u003e if the option does not take a value.\n   * @return Whether to continue parsing the argument list.\n   */"
			},
			{
				"signature": "protected boolean handleUnknown(String opt)",
				"documentation": "/**\n   * Callback for when an unrecognized option is parsed.\n   *\n   * @param opt Unrecognized option from the command line.\n   * @return Whether to continue parsing the argument list.\n   */"
			},
			{
				"signature": "protected void handleExtraArgs(List\u003cString\u003e extra)",
				"documentation": "/**\n   * Callback for remaining command line arguments after either {@link #handle(String, String)} or\n   * {@link #handleUnknown(String)} return \"false\". This will be called at the end of parsing even\n   * when there are no remaining arguments.\n   *\n   * @param extra List of remaining arguments.\n   */"
			},
			{
				"signature": "private String findCliOption(String name, String[][] available)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.AbstractLauncher.ArgumentValidator",
			"org.apache.spark.launcher.Main.MainClassOptionParser",
			"org.apache.spark.launcher.OptionParser",
			"org.apache.spark.launcher.SparkSubmitOptionParserSuite.DummyParser"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Handles configuring the JUL -\u003e SLF4J bridge, and provides some utility methods for tests.\n */",
		"name": "org.apache.spark.launcher.BaseSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * Handles configuring the JUL -\u003e SLF4J bridge, and provides some utility methods for tests.\n */"
			},
			{
				"signature": "@After\n  public void postChecks()",
				"documentation": ""
			},
			{
				"signature": "protected void waitFor(final SparkAppHandle handle) throws Exception",
				"documentation": ""
			},
			{
				"signature": "protected void eventually(Duration timeout, Duration period, Runnable check) throws Exception",
				"documentation": "/**\n   * Call a closure that performs a check every \"period\" until it succeeds, or the timeout\n   * elapses.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.launcher.SparkLauncherSuite",
			"org.apache.spark.launcher.ChildProcAppHandleSuite",
			"org.apache.spark.launcher.InProcessLauncherSuite",
			"org.apache.spark.launcher.LauncherServerSuite",
			"org.apache.spark.launcher.SparkSubmitCommandBuilderSuite",
			"org.apache.spark.launcher.SparkSubmitOptionParserSuite"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.ChildProcAppHandleSuite",
		"extends": "org.apache.spark.launcher.BaseSuite",
		"Methods": [
			{
				"signature": "@AfterClass\n  public static void cleanupClass() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@BeforeClass\n  public static void setupClass() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void cleanupLog()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRedirectsSimple() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRedirectLastWins() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRedirectToLog() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRedirectErrorToLog() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRedirectOutputToLog() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNoRedirectToLog() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBadLogRedirect() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRedirectErrorTwiceFails() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testProcMonitorWithOutputRedirection() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testProcMonitorWithLogRedirection() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFailedChildProc() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.ChildProcAppHandleSuite.TestSparkLauncher",
			"org.apache.spark.launcher.ChildProcAppHandleSuite.LogAppender"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.launcher.ChildProcAppHandleSuite.TestSparkLauncher",
			"org.apache.spark.launcher.ChildProcAppHandleSuite.LogAppender"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.ChildProcAppHandleSuite.TestSparkLauncher",
		"extends": "org.apache.spark.launcher.SparkLauncher",
		"Methods": [
			{
				"signature": "TestSparkLauncher()",
				"documentation": ""
			},
			{
				"signature": "TestSparkLauncher(int ec)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    String findSparkSubmit()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.ChildProcAppHandleSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A log4j appender used by child apps of this test. It records all messages logged through it in\n   * memory so the test can check them.\n   */",
		"name": "org.apache.spark.launcher.ChildProcAppHandleSuite.LogAppender",
		"extends": "org.apache.logging.log4j.core.appender.AbstractAppender",
		"Methods": [
			{
				"signature": "protected LogAppender(String name,\n                          Filter filter,\n                          Layout\u003c? extends Serializable\u003e layout,\n                          boolean ignoreExceptions)",
				"documentation": "/**\n   * A log4j appender used by child apps of this test. It records all messages logged through it in\n   * memory so the test can check them.\n   */"
			},
			{
				"signature": "@Override\n    public void append(LogEvent event)",
				"documentation": ""
			},
			{
				"signature": "@PluginFactory\n    public static LogAppender createAppender(\n            @PluginAttribute(\"name\") String name,\n            @PluginElement(\"Layout\") Layout\u003c? extends Serializable\u003e layout,\n            @PluginElement(\"Filter\") final Filter filter,\n            @PluginAttribute(\"otherAttribute\") String otherAttribute)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.ChildProcAppHandleSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.CommandBuilderUtilsSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testValidOptionStrings()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInvalidOptionStrings()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testWindowsBatchQuoting()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPythonArgQuoting()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJavaMajorVersion()",
				"documentation": ""
			},
			{
				"signature": "private static void testOpt(String opts, List\u003cString\u003e expected)",
				"documentation": ""
			},
			{
				"signature": "private static void testInvalidOpt(String opts)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.InProcessLauncherSuite",
		"extends": "org.apache.spark.launcher.BaseSuite",
		"Methods": [
			{
				"signature": "@Before\n  public void testSetup()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLauncher() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKill() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testErrorPropagation() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private SparkAppHandle startTest(String test) throws Exception",
				"documentation": ""
			},
			{
				"signature": "public static void runTest(String[] args)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        protected boolean handle(String opt, String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        protected boolean handleUnknown(String opt)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        protected void handleExtraArgs(List\u003cString\u003e extra)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.InProcessLauncherSuite.TestInProcessLauncher"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.launcher.InProcessLauncherSuite.TestInProcessLauncher"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.InProcessLauncherSuite.TestInProcessLauncher",
		"extends": "org.apache.spark.launcher.InProcessLauncher",
		"Methods": [
			{
				"signature": "@Override\n    Method findSparkSubmit() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.InProcessLauncherSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.LauncherServerSuite",
		"extends": "org.apache.spark.launcher.BaseSuite",
		"Methods": [
			{
				"signature": "@Test\n  public void testLauncherServerReuse() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCommunication() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTimeout() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSparkSubmitVmShutsDown() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testStreamFiltering() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAppHandleDisconnect() throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void close(Closeable c)",
				"documentation": ""
			},
			{
				"signature": "private void waitForError(TestClient client, String secret) throws Exception",
				"documentation": "/**\n   * Try a few times to get a client-side error, since the client-side socket may not reflect the\n   * server-side close immediately.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.LauncherServerSuite.TestClient"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.launcher.LauncherServerSuite.TestClient",
			"org.apache.spark.launcher.LauncherServerSuite.EvilPayload"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.LauncherServerSuite.TestClient",
		"extends": "org.apache.spark.launcher.LauncherConnection",
		"Methods": [
			{
				"signature": "TestClient(Socket s) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void handle(Message msg) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.LauncherServerSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.LauncherServerSuite.EvilPayload",
		"extends": "LauncherProtocol.Message",
		"Methods": [
			{
				"signature": "private void readObject(ObjectInputStream stream) throws IOException, ClassNotFoundException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.SparkSubmitCommandBuilderSuite",
		"extends": "org.apache.spark.launcher.BaseSuite",
		"Methods": [
			{
				"signature": "@BeforeClass\n  public static void setUp() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@AfterClass\n  public static void cleanUp() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDriverCmdBuilder() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testClusterCmdBuilder() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCliHelpAndNoArg() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCliKillAndStatus() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCliParser() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testShellCliParser() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAlternateSyntaxParsing() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPySparkLauncher() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPySparkFallback() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSparkRShell() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExamplesRunnerNoArg()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExamplesRunnerNoMainClass() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExamplesRunnerWithMasterNoMainClass() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExamplesRunner() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExamplesRunnerPrimaryResource() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMissingAppResource()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIsClientMode()",
				"documentation": ""
			},
			{
				"signature": "private void testCmdBuilder(boolean isDriver, boolean useDefaultPropertyFile) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private boolean contains(String needle, String[] haystack)",
				"documentation": ""
			},
			{
				"signature": "private Map\u003cString, String\u003e parseConf(List\u003cString\u003e cmd, SparkSubmitOptionParser parser)",
				"documentation": ""
			},
			{
				"signature": "private String findArgValue(List\u003cString\u003e cmd, String name)",
				"documentation": ""
			},
			{
				"signature": "private boolean findInStringList(String list, String sep, String needle)",
				"documentation": ""
			},
			{
				"signature": "private SparkSubmitCommandBuilder newCommandBuilder(List\u003cString\u003e args)",
				"documentation": ""
			},
			{
				"signature": "private List\u003cString\u003e buildCommand(List\u003cString\u003e args, Map\u003cString, String\u003e env) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private void testCLIOpts(String appResource, String opt, List\u003cString\u003e params) throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.SparkSubmitOptionParserSuite",
		"extends": "org.apache.spark.launcher.BaseSuite",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAllOptions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExtraOptions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMissingArg()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEqualSeparatedOption()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.launcher.SparkSubmitOptionParserSuite.DummyParser"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.launcher.SparkSubmitOptionParserSuite.DummyParser"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.launcher.SparkSubmitOptionParserSuite.DummyParser",
		"extends": "org.apache.spark.launcher.SparkSubmitOptionParser",
		"Methods": [
			{
				"signature": "@Override\n    protected boolean handle(String opt, String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected boolean handleUnknown(String opt)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    protected void handleExtraArgs(List\u003cString\u003e extra)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.launcher.SparkSubmitOptionParserSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A dummy class as a workaround to show the package doc of \u003ccode\u003espark.mllib\u003c/code\u003e in generated\n * Java API docs.\n * @see \u003ca href=\"http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4492654\" target=\"_blank\"\u003e\n *      JDK-4492654\u003c/a\u003e\n */",
		"name": "org.apache.spark.mllib.JavaPackage",
		"extends": "",
		"Methods": [
			{
				"signature": "private JavaPackage()",
				"documentation": "/**\n * A dummy class as a workaround to show the package doc of \u003ccode\u003espark.mllib\u003c/code\u003e in generated\n * Java API docs.\n * @see \u003ca href=\"http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4492654\" target=\"_blank\"\u003e\n *      JDK-4492654\u003c/a\u003e\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.AlphaComponent"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.SharedSparkSession",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test Pipeline construction and fitting in Java.\n */",
		"name": "org.apache.spark.ml.JavaPipelineSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": "/**\n * Test Pipeline construction and fitting in Java.\n */"
			},
			{
				"signature": "@Test\n  public void pipeline()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.attribute.JavaAttributeGroupSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testAttributeGroup()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.attribute.JavaAttributeSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testAttributeType()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNumericAttribute()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNominalAttribute()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryAttribute()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runDT()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.classification.JavaGBTClassifierSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runDT()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.classification.JavaLogisticRegressionSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void logisticRegressionDefaultParams()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void logisticRegressionWithSetters()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void logisticRegressionPredictorClassifierMethods()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void logisticRegressionTrainingSummary()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void testMLPC()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.classification.JavaNaiveBayesSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "public void validatePrediction(Dataset\u003cRow\u003e predictionAndLabels)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void naiveBayesDefaultParams()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNaiveBayes()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.classification.JavaOneVsRestSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "double[] coefficients =",
				"documentation": ""
			},
			{
				"signature": "double[] xMean =",
				"documentation": ""
			},
			{
				"signature": "double[] xVariance =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void oneVsRestDefaultParams()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.classification.JavaRandomForestClassifierSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runDT()",
				"documentation": ""
			},
			{
				"signature": "String[] realStrategies =",
				"documentation": ""
			},
			{
				"signature": "String[] integerStrategies =",
				"documentation": ""
			},
			{
				"signature": "String[] invalidStrategies =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.clustering.JavaKMeansSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fitAndTransform()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaBucketizerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void bucketizerTest()",
				"documentation": ""
			},
			{
				"signature": "double[] splits =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void bucketizerMultipleColumnsTest()",
				"documentation": ""
			},
			{
				"signature": "double[][] splitsArray =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaDCTSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void javaCompatibilityTest()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaHashingTFSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void hashingTF()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaNormalizerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void normalizer()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaPCASuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void testPCA()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.ml.feature.JavaPCASuite.VectorPair"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaPCASuite.VectorPair",
		"extends": "",
		"Methods": [
			{
				"signature": "public void setFeatures(Vector features)",
				"documentation": ""
			},
			{
				"signature": "public Vector getFeatures()",
				"documentation": ""
			},
			{
				"signature": "public void setExpected(Vector expected)",
				"documentation": ""
			},
			{
				"signature": "public Vector getExpected()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaPolynomialExpansionSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void polynomialExpansionTest()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaStandardScalerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void standardScaler()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaStopWordsRemoverSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void javaCompatibilityTest()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory",
			"org.apache.spark.sql.types.DataTypes"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaStringIndexerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void testStringIndexer()",
				"documentation": ""
			},
			{
				"signature": "private Row cr(Object... values)",
				"documentation": "/**\n   * An alias for RowFactory.create.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaTokenizerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void regexTokenizer()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaVectorAssemblerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void testVectorAssembler()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaVectorIndexerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void vectorIndexerAPI()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaVectorSlicerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void vectorSlice()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.feature.JavaWord2VecSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void testJavaWord2Vec()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.linalg.JavaSQLDataTypesSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testSQLDataTypes()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test Param and related classes in Java\n */",
		"name": "org.apache.spark.ml.param.JavaParamsSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testParams()",
				"documentation": "/**\n * Test Param and related classes in Java\n */"
			},
			{
				"signature": "@Test\n  public void testParamValidate()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A subclass of Params for testing.\n */",
		"name": "org.apache.spark.ml.param.JavaTestParams",
		"extends": "JavaParams",
		"Methods": [
			{
				"signature": "public JavaTestParams()",
				"documentation": "/**\n * A subclass of Params for testing.\n */"
			},
			{
				"signature": "public JavaTestParams(String uid)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String uid()",
				"documentation": ""
			},
			{
				"signature": "public IntParam myIntParam()",
				"documentation": ""
			},
			{
				"signature": "public int getMyIntParam()",
				"documentation": ""
			},
			{
				"signature": "public JavaTestParams setMyIntParam(int value)",
				"documentation": ""
			},
			{
				"signature": "public DoubleParam myDoubleParam()",
				"documentation": ""
			},
			{
				"signature": "public double getMyDoubleParam()",
				"documentation": ""
			},
			{
				"signature": "public JavaTestParams setMyDoubleParam(double value)",
				"documentation": ""
			},
			{
				"signature": "public Param\u003cString\u003e myStringParam()",
				"documentation": ""
			},
			{
				"signature": "public String getMyStringParam()",
				"documentation": ""
			},
			{
				"signature": "public JavaTestParams setMyStringParam(String value)",
				"documentation": ""
			},
			{
				"signature": "public DoubleArrayParam myDoubleArrayParam()",
				"documentation": ""
			},
			{
				"signature": "public double[] getMyDoubleArrayParam()",
				"documentation": ""
			},
			{
				"signature": "public JavaTestParams setMyDoubleArrayParam(double[] value)",
				"documentation": ""
			},
			{
				"signature": "private void init()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public JavaTestParams copy(ParamMap extra)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runDT()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.regression.JavaGBTRegressorSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runDT()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.regression.JavaLinearRegressionSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void linearRegressionDefaultParams()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void linearRegressionWithSetters()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.regression.JavaRandomForestRegressorSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runDT()",
				"documentation": ""
			},
			{
				"signature": "String[] realStrategies =",
				"documentation": ""
			},
			{
				"signature": "String[] integerStrategies =",
				"documentation": ""
			},
			{
				"signature": "String[] invalidStrategies =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test LibSVMRelation in Java.\n */",
		"name": "org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": "/**\n * Test LibSVMRelation in Java.\n */"
			},
			{
				"signature": "@Override\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void verifyLibSVMDF()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.stat.JavaKolmogorovSmirnovTestSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKSTestCDF()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKSTestNamedDistribution()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.stat.JavaSummarizerSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSummarizer()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.tuning.JavaCrossValidatorSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void crossValidationWithLogisticRegression()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.ml.util.JavaDefaultReadWriteSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDefaultReadWrite() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.classification.JavaLogisticRegressionSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "int validatePrediction(List\u003cLabeledPoint\u003e validationData, LogisticRegressionModel model)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runLRUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runLRUsingStaticMethods()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.classification.JavaNaiveBayesSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "private static int validatePrediction(List\u003cLabeledPoint\u003e points, NaiveBayesModel model)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runUsingStaticMethods()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPredictJavaRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testModelTypeSetters()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.classification.JavaSVMSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "int validatePrediction(List\u003cLabeledPoint\u003e validationData, SVMModel model)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runSVMUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "double[] weights =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runSVMUsingStaticMethods()",
				"documentation": ""
			},
			{
				"signature": "double[] weights =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void javaAPI()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void twoDimensionalData()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runGaussianMixture()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.clustering.JavaKMeansSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runKMeansUsingStaticMethods()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runKMeansUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPredictJavaRDD()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.clustering.JavaLDASuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void localLDAModel()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void distributedLDAModel()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void onlineOptimizerCompatibility()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void localLdaMethods()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void javaAPI()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Override\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void rankingMetrics()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.feature.JavaTfIdfSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void tfIdf()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void tfIdfMinimumDocumentFrequency()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.feature.JavaWord2VecSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void word2Vec()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.fpm.JavaAssociationRulesSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runAssociationRules()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.fpm.JavaFPGrowthSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runFPGrowth()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runFPGrowthSaveLoad()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.fpm.JavaPrefixSpanSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void runPrefixSpan()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runPrefixSpanSaveLoad()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.linalg.JavaMatricesSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void randMatrixConstruction()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void identityMatrixConstruction()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void diagonalMatrixConstruction()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void zerosMatrixConstruction()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sparseDenseConversion()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void concatenateMatrices()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.linalg.JavaVectorsSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void denseArrayConstruction()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sparseArrayConstruction()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void rowMatrixQRDecomposition()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.random.JavaRandomRDDsSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void testUniformRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNormalRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLNormalRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPoissonRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExponentialRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGammaRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUniformVectorRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNormalVectorRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLogNormalVectorRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPoissonVectorRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExponentialVectorRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGammaVectorRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testArbitrary()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRandomVectorRDD()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.random.StringGenerator",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public String nextValue()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public StringGenerator copy()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setSeed(long seed)",
				"documentation": ""
			}
		],
		"interfaces": [
			"RandomDataGenerator",
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.recommendation.JavaALSSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "private void validatePrediction(\n    MatrixFactorizationModel model,\n    int users,\n    int products,\n    double[] trueRatings,\n    double matchThreshold,\n    boolean implicitPrefs,\n    double[] truePrefs)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runALSUsingStaticMethods()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runALSUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runImplicitALSUsingStaticMethods()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runImplicitALSUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runImplicitALSWithNegativeWeight()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runRecommend()",
				"documentation": ""
			},
			{
				"signature": "private static void validateRecommendations(Rating[] recommendations, int howMany)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "private static List\u003cTuple3\u003cDouble, Double, Double\u003e\u003e generateIsotonicInput(double[] labels)",
				"documentation": ""
			},
			{
				"signature": "private IsotonicRegressionModel runIsotonicRegression(double[] labels)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIsotonicRegressionJavaRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIsotonicRegressionPredictionsJavaRDD()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.regression.JavaLassoSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "int validatePrediction(List\u003cLabeledPoint\u003e validationData, LassoModel model)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runLassoUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "double[] weights =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runLassoUsingStaticMethods()",
				"documentation": ""
			},
			{
				"signature": "double[] weights =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.regression.JavaLinearRegressionSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "private static int validatePrediction(\n      List\u003cLabeledPoint\u003e validationData, LinearRegressionModel model)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runLinearRegressionUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "double[] weights =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runLinearRegressionUsingStaticMethods()",
				"documentation": ""
			},
			{
				"signature": "double[] weights =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPredictJavaRDD()",
				"documentation": ""
			},
			{
				"signature": "double[] weights =",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.regression.JavaRidgeRegressionSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "private static double predictionError(List\u003cLabeledPoint\u003e validationData,\n                                        RidgeRegressionModel model)",
				"documentation": ""
			},
			{
				"signature": "private static List\u003cLabeledPoint\u003e generateRidgeData(int numPoints, int numFeatures, double std)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runRidgeRegressionUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runRidgeRegressionUsingStaticMethods()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void javaAPI()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.stat.JavaStatisticsSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCorr()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void kolmogorovSmirnovTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void chiSqTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void streamingTest()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.tree.JavaDecisionTreeSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "private static int validatePrediction(\n      List\u003cLabeledPoint\u003e validationData, DecisionTreeModel model)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runDTUsingConstructor()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void runDTUsingStaticMethods()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.mllib.util.JavaMLUtilsSuite",
		"extends": "org.apache.spark.SharedSparkSession",
		"Methods": [
			{
				"signature": "@Test\n  public void testConvertVectorColumnsToAndFromML()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testConvertMatrixColumnsToAndFromML()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A client for talking to the external shuffle service in Mesos coarse-grained mode.\n *\n * This is used by the Spark driver to register with each external shuffle service on the cluster.\n * The reason why the driver has to talk to the service is for cleaning up shuffle files reliably\n * after the application exits. Mesos does not provide a great alternative to do this, so Spark\n * has to detect this itself.\n */",
		"name": "org.apache.spark.network.shuffle.mesos.MesosExternalBlockStoreClient",
		"extends": "org.apache.spark.network.shuffle.ExternalBlockStoreClient",
		"Methods": [
			{
				"signature": "public MesosExternalBlockStoreClient(\n      TransportConf conf,\n      SecretKeyHolder secretKeyHolder,\n      boolean authEnabled,\n      long registrationTimeoutMs)",
				"documentation": "/**\n   * Creates an Mesos external shuffle client that wraps the {@link ExternalBlockStoreClient}.\n   * Please refer to docs on {@link ExternalBlockStoreClient} for more information.\n   */"
			},
			{
				"signature": "public void registerDriverWithShuffleService(\n      String host,\n      int port,\n      long heartbeatTimeoutMs,\n      long heartbeatIntervalMs) throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver",
			"org.apache.spark.network.shuffle.protocol.mesos.ShuffleServiceHeartbeat",
			"org.apache.spark.network.shuffle.mesos.RegisterDriverCallback",
			"org.apache.spark.network.shuffle.mesos.Heartbeater"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.network.shuffle.mesos.RegisterDriverCallback",
			"org.apache.spark.network.shuffle.mesos.Heartbeater"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.mesos.RegisterDriverCallback",
		"extends": "",
		"Methods": [
			{
				"signature": "private RegisterDriverCallback(TransportClient client, long heartbeatIntervalMs)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onSuccess(ByteBuffer response)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onFailure(Throwable e)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.network.client.RpcResponseCallback"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.network.shuffle.mesos.MesosExternalBlockStoreClient"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.network.shuffle.mesos.Heartbeater",
		"extends": "",
		"Methods": [
			{
				"signature": "private Heartbeater(TransportClient client)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void run()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Runnable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.shuffle.protocol.mesos.ShuffleServiceHeartbeat"
		],
		"usedBy": [
			"org.apache.spark.network.shuffle.mesos.MesosExternalBlockStoreClient"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Copied from\n * hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/ServerSocketUtil.java\n * for Hadoop-3.x testing\n */",
		"name": "org.apache.hadoop.net.ServerSocketUtil",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int getPort(int port, int retries) throws IOException",
				"documentation": "/**\n   * Port scan \u0026 allocate is how most other apps find ports\n   *\n   * @param port given port\n   * @param retries number of retries\n   * @return\n   * @throws IOException\n   */"
			},
			{
				"signature": "private static boolean isPortAvailable(int port)",
				"documentation": "/**\n   * Check whether port is available or not.\n   *\n   * @param port given port\n   * @return\n   */"
			},
			{
				"signature": "public static int waitForPort(int port, int retries)\n          throws InterruptedException, IOException",
				"documentation": "/**\n   * Wait till the port available.\n   *\n   * @param port given port\n   * @param retries number of retries for given port\n   * @return\n   * @throws InterruptedException\n   * @throws IOException\n   */"
			},
			{
				"signature": "public static int[] getPorts(int numPorts) throws IOException",
				"documentation": "/**\n   * Find the specified number of unique ports available.\n   * The ports are all closed afterwards,\n   * so other network services started may grab those same ports.\n   *\n   * @param numPorts number of required port numbers\n   * @return array of available port numbers\n   * @throws IOException\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Adapted from https://github.com/eclipse/jetty.project/blob/jetty-9.3.25.v20180904/\n *   jetty-server/src/main/java/org/eclipse/jetty/server/SessionManager.java\n */",
		"name": "org.eclipse.jetty.server.SessionManager",
		"extends": "org.eclipse.jetty.util.component.LifeCycle",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Adapted from https://github.com/eclipse/jetty.project/blob/jetty-9.3.25.v20180904/\n *   jetty-server/src/main/java/org/eclipse/jetty/server/session/SessionHandler.java\n */",
		"name": "org.eclipse.jetty.server.session.SessionHandler",
		"extends": "org.eclipse.jetty.server.handler.ScopedHandler",
		"Methods": [
			{
				"signature": "public SessionHandler()",
				"documentation": "/**\n * Adapted from https://github.com/eclipse/jetty.project/blob/jetty-9.3.25.v20180904/\n *   jetty-server/src/main/java/org/eclipse/jetty/server/session/SessionHandler.java\n */"
			},
			{
				"signature": "public SessionHandler(SessionManager manager)",
				"documentation": "/**\n   * @param manager\n   *            The session manager\n   */"
			},
			{
				"signature": "public SessionManager getSessionManager()",
				"documentation": "/**\n   * @return Returns the sessionManager.\n   */"
			},
			{
				"signature": "public void setSessionManager(SessionManager sessionManager)",
				"documentation": "/**\n   * @param sessionManager\n   *            The sessionManager to set.\n   */"
			},
			{
				"signature": "@Override\n  public void doHandle(String target, Request baseRequest, HttpServletRequest request,\n      HttpServletResponse response) throws IOException, ServletException",
				"documentation": ""
			},
			{
				"signature": "public void clearEventListeners()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A factory class used to construct {@link Row} objects.\n *\n * @since 1.3.0\n */",
		"name": "org.apache.spark.sql.RowFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "public static Row create(Object ... values)",
				"documentation": "/**\n   * Create a {@link Row} from the given arguments. Position i in the argument list becomes\n   * position i in the created {@link Row} object.\n   *\n   * @since 1.3.0\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [
			"org.apache.spark.examples.ml.JavaChiSquareTestExample"
		],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [
			"org.apache.spark.examples.ml.JavaAFTSurvivalRegressionExample",
			"org.apache.spark.examples.ml.JavaBinarizerExample",
			"org.apache.spark.examples.ml.JavaBucketedRandomProjectionLSHExample",
			"org.apache.spark.examples.ml.JavaBucketizerExample",
			"org.apache.spark.examples.ml.JavaChiSqSelectorExample",
			"org.apache.spark.examples.ml.JavaCorrelationExample",
			"org.apache.spark.examples.ml.JavaCountVectorizerExample",
			"org.apache.spark.examples.ml.JavaDCTExample",
			"org.apache.spark.examples.ml.JavaElementwiseProductExample",
			"org.apache.spark.examples.ml.JavaEstimatorTransformerParamExample",
			"org.apache.spark.examples.ml.JavaFPGrowthExample",
			"org.apache.spark.examples.ml.JavaFeatureHasherExample",
			"org.apache.spark.examples.ml.JavaImputerExample",
			"org.apache.spark.examples.ml.JavaIndexToStringExample",
			"org.apache.spark.examples.ml.JavaMaxAbsScalerExample",
			"org.apache.spark.examples.ml.JavaMinHashLSHExample",
			"org.apache.spark.examples.ml.JavaMinMaxScalerExample",
			"org.apache.spark.examples.ml.JavaNGramExample",
			"org.apache.spark.examples.ml.JavaNormalizerExample",
			"org.apache.spark.examples.ml.JavaOneHotEncoderExample",
			"org.apache.spark.examples.ml.JavaPCAExample",
			"org.apache.spark.examples.ml.JavaPolynomialExpansionExample",
			"org.apache.spark.examples.ml.JavaPowerIterationClusteringExample",
			"org.apache.spark.examples.ml.JavaPrefixSpanExample",
			"org.apache.spark.examples.ml.JavaQuantileDiscretizerExample",
			"org.apache.spark.examples.ml.JavaRFormulaExample",
			"org.apache.spark.examples.ml.JavaSQLTransformerExample",
			"org.apache.spark.examples.ml.JavaStopWordsRemoverExample",
			"org.apache.spark.examples.ml.JavaStringIndexerExample",
			"org.apache.spark.examples.ml.JavaTfIdfExample",
			"org.apache.spark.examples.ml.JavaTokenizerExample",
			"org.apache.spark.examples.ml.JavaUnivariateFeatureSelectorExample",
			"org.apache.spark.examples.ml.JavaVarianceThresholdSelectorExample",
			"org.apache.spark.examples.ml.JavaVectorAssemblerExample",
			"org.apache.spark.examples.ml.JavaVectorSizeHintExample",
			"org.apache.spark.examples.ml.JavaVectorSlicerExample",
			"org.apache.spark.examples.ml.JavaWord2VecExample",
			"org.apache.spark.examples.sql.JavaSparkSQLExample",
			"org.apache.spark.ml.classification.JavaNaiveBayesSuite",
			"org.apache.spark.ml.feature.JavaBucketizerSuite",
			"org.apache.spark.ml.feature.JavaHashingTFSuite",
			"org.apache.spark.ml.feature.JavaPolynomialExpansionSuite",
			"org.apache.spark.ml.feature.JavaStopWordsRemoverSuite",
			"org.apache.spark.ml.feature.JavaVectorAssemblerSuite",
			"org.apache.spark.ml.feature.JavaVectorSlicerSuite",
			"org.apache.spark.ml.feature.JavaWord2VecSuite",
			"org.apache.spark.mllib.util.JavaMLUtilsSuite",
			"test.org.apache.spark.sql.JavaApplySchemaSuite",
			"test.org.apache.spark.sql.JavaColumnExpressionSuite",
			"test.org.apache.spark.sql.JavaRowSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ::DeveloperApi::\n *\n * A function description type which can be recognized by FunctionRegistry, and will be used to\n * show the usage of the function in human language.\n *\n * `usage()` will be used for the function usage in brief way.\n *\n * These below are concatenated and used for the function usage in verbose way, suppose arguments,\n * examples, note, group, source, since and deprecated will be provided.\n *\n * `arguments()` describes arguments for the expression.\n *\n * `examples()` describes examples for the expression.\n *\n * `note()` contains some notes for the expression optionally.\n *\n * `group()` describes the category that the expression belongs to. The valid value is\n * \"agg_funcs\", \"array_funcs\", \"datetime_funcs\", \"json_funcs\", \"map_funcs\" and \"window_funcs\".\n *\n * `source()` describe the source of the function. The valid value is \"built-in\", \"hive\",\n * \"python_udf\", \"scala_udf\", \"java_udf\".\n *\n * `since()` contains version information for the expression. Version is specified by,\n * for example, \"2.2.0\".\n *\n * `deprecated()` contains deprecation information for the expression optionally, for example,\n * \"Deprecated since 2.2.0. Use something else instead\".\n *\n * The format, in particular for `arguments()`, `examples()`,`note()`, `group()`, `source()`,\n * `since()` and `deprecated()`,  should strictly be as follows.\n *\n * \u003cpre\u003e\n * \u003ccode\u003e@ExpressionDescription(\n *   ...\n *   arguments = \"\"\"\n *     Arguments:\n *       * arg0 - ...\n *           ....\n *       * arg1 - ...\n *           ....\n *   \"\"\",\n *   examples = \"\"\"\n *     Examples:\n *       \u003e SELECT ...;\n *        ...\n *       \u003e SELECT ...;\n *        ...\n *   \"\"\",\n *   note = \"\"\"\n *     ...\n *   \"\"\",\n *   group = \"agg_funcs\",\n *   source = \"built-in\",\n *   since = \"3.0.0\",\n *   deprecated = \"\"\"\n *     ...\n *   \"\"\")\n * \u003c/code\u003e\n * \u003c/pre\u003e\n *\n *  We can refer the function name by `_FUNC_`, in `usage()`, `arguments()`, `examples()` and\n *  `note()` as it is registered in `FunctionRegistry`.\n *\n *  Note that, if `extended()` is defined, `arguments()`, `examples()`, `note()`, `group()`,\n *  `since()` and `deprecated()` should be not defined together. `extended()` exists\n *  for backward compatibility.\n *\n *  Note this contents are used in the SparkSQL documentation for built-in functions. The contents\n *  here are considered as a Markdown text and then rendered.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.ExpressionDescription",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An utility class for constructing expressions.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.ExpressionImplUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static byte[] aesEncrypt(byte[] input, byte[] key, UTF8String mode, UTF8String padding)",
				"documentation": "/**\n * An utility class for constructing expressions.\n */"
			},
			{
				"signature": "public static byte[] aesDecrypt(byte[] input, byte[] key, UTF8String mode, UTF8String padding)",
				"documentation": ""
			},
			{
				"signature": "private static byte[] aesInternal(\n      byte[] input,\n      byte[] key,\n      String mode,\n      String padding,\n      int opmode)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Expression information, will be used to describe a expression.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.ExpressionInfo",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getClassName()",
				"documentation": "/**\n * Expression information, will be used to describe a expression.\n */"
			},
			{
				"signature": "public String getUsage()",
				"documentation": ""
			},
			{
				"signature": "public String getName()",
				"documentation": ""
			},
			{
				"signature": "public String getExtended()",
				"documentation": ""
			},
			{
				"signature": "public String getSince()",
				"documentation": ""
			},
			{
				"signature": "public String getArguments()",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n    public String getOriginalExamples()",
				"documentation": ""
			},
			{
				"signature": "public String getExamples()",
				"documentation": ""
			},
			{
				"signature": "public String getNote()",
				"documentation": ""
			},
			{
				"signature": "public String getDeprecated()",
				"documentation": ""
			},
			{
				"signature": "public String getGroup()",
				"documentation": ""
			},
			{
				"signature": "public String getDb()",
				"documentation": ""
			},
			{
				"signature": "public String getSource()",
				"documentation": ""
			},
			{
				"signature": "public ExpressionInfo(\n            String className,\n            String db,\n            String name,\n            String usage,\n            String arguments,\n            String examples,\n            String note,\n            String group,\n            String since,\n            String deprecated,\n            String source)",
				"documentation": ""
			},
			{
				"signature": "public ExpressionInfo(String className, String name)",
				"documentation": ""
			},
			{
				"signature": "public ExpressionInfo(String className, String db, String name)",
				"documentation": ""
			},
			{
				"signature": "@Deprecated\n    public ExpressionInfo(String className, String db, String name, String usage, String extended)",
				"documentation": "/**\n     * @deprecated This constructor is deprecated as of Spark 3.0. Use other constructors to fully\n     *   specify each argument for extended usage.\n     */"
			},
			{
				"signature": "private String replaceFunctionName(String usage)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An implementation of `RowBasedKeyValueBatch` in which all key-value records have same length.\n *\n * The format for each record looks like this:\n * [UnsafeRow for key of length klen] [UnsafeRow for Value of length vlen]\n * [8 bytes pointer to next]\n * Thus, record length = klen + vlen + 8\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch",
		"extends": "org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch",
		"Methods": [
			{
				"signature": "private long getKeyOffsetForFixedLengthRecords(int rowId)",
				"documentation": "/**\n * An implementation of `RowBasedKeyValueBatch` in which all key-value records have same length.\n *\n * The format for each record looks like this:\n * [UnsafeRow for key of length klen] [UnsafeRow for Value of length vlen]\n * [8 bytes pointer to next]\n * Thus, record length = klen + vlen + 8\n */"
			},
			{
				"signature": "@Override\n  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen)",
				"documentation": "/**\n   * Append a key value pair.\n   * It copies data into the backing MemoryBlock.\n   * Returns an UnsafeRow pointing to the value if succeeds, otherwise returns null.\n   */"
			},
			{
				"signature": "@Override\n  public UnsafeRow getKeyRow(int rowId)",
				"documentation": "/**\n   * Returns the key row in this batch at `rowId`. Returned key row is reused across calls.\n   */"
			},
			{
				"signature": "@Override\n  protected UnsafeRow getValueFromKey(int rowId)",
				"documentation": "/**\n   * Returns the value row by two steps:\n   * 1) looking up the key row with the same id (skipped if the key row is cached)\n   * 2) retrieve the value row by reusing the metadata from step 1)\n   * In most times, 1) is skipped because `getKeyRow(id)` is often called before `getValueRow(id)`.\n   */"
			},
			{
				"signature": "@Override\n  public org.apache.spark.unsafe.KVIterator\u003cUnsafeRow, UnsafeRow\u003e rowIterator()",
				"documentation": "/**\n   * Returns an iterator to go through all rows\n   */"
			},
			{
				"signature": "private void init()",
				"documentation": "/**\n   * Returns an iterator to go through all rows\n   */"
			},
			{
				"signature": "@Override\n      public boolean next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public UnsafeRow getKey()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public UnsafeRow getValue()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void close()",
				"documentation": ""
			},
			{
				"signature": "private void freeCurrentPage()",
				"documentation": ""
			},
			{
				"signature": "protected FixedLengthRowBasedKeyValueBatch(StructType keySchema, StructType valueSchema,\n                                             int maxRows, TaskMemoryManager manager)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * RowBasedKeyValueBatch stores key value pairs in contiguous memory region.\n *\n * Each key or value is stored as a single UnsafeRow. Each record contains one key and one value\n * and some auxiliary data, which differs based on implementation:\n * i.e., `FixedLengthRowBasedKeyValueBatch` and `VariableLengthRowBasedKeyValueBatch`.\n *\n * We use `FixedLengthRowBasedKeyValueBatch` if all fields in the key and the value are fixed-length\n * data types. Otherwise we use `VariableLengthRowBasedKeyValueBatch`.\n *\n * RowBasedKeyValueBatch is backed by a single page / MemoryBlock (ranges from 1 to 64MB depending\n * on the system configuration). If the page is full, the aggregate logic should fallback to a\n * second level, larger hash map. We intentionally use the single-page design because it simplifies\n * memory address encoding \u0026 decoding for each key-value pair. Because the maximum capacity for\n * RowBasedKeyValueBatch is only 2^16, it is unlikely we need a second page anyway. Filling the\n * page requires an average size for key value pairs to be larger than 1024 bytes.\n *\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch",
		"extends": "org.apache.spark.memory.MemoryConsumer",
		"Methods": [
			{
				"signature": "public static RowBasedKeyValueBatch allocate(StructType keySchema, StructType valueSchema,\n                                               TaskMemoryManager manager)",
				"documentation": "/**\n * RowBasedKeyValueBatch stores key value pairs in contiguous memory region.\n *\n * Each key or value is stored as a single UnsafeRow. Each record contains one key and one value\n * and some auxiliary data, which differs based on implementation:\n * i.e., `FixedLengthRowBasedKeyValueBatch` and `VariableLengthRowBasedKeyValueBatch`.\n *\n * We use `FixedLengthRowBasedKeyValueBatch` if all fields in the key and the value are fixed-length\n * data types. Otherwise we use `VariableLengthRowBasedKeyValueBatch`.\n *\n * RowBasedKeyValueBatch is backed by a single page / MemoryBlock (ranges from 1 to 64MB depending\n * on the system configuration). If the page is full, the aggregate logic should fallback to a\n * second level, larger hash map. We intentionally use the single-page design because it simplifies\n * memory address encoding \u0026 decoding for each key-value pair. Because the maximum capacity for\n * RowBasedKeyValueBatch is only 2^16, it is unlikely we need a second page anyway. Filling the\n * page requires an average size for key value pairs to be larger than 1024 bytes.\n *\n */"
			},
			{
				"signature": "public static RowBasedKeyValueBatch allocate(StructType keySchema, StructType valueSchema,\n                                               TaskMemoryManager manager, int maxRows)",
				"documentation": ""
			},
			{
				"signature": "protected RowBasedKeyValueBatch(StructType keySchema, StructType valueSchema, int maxRows,\n                                TaskMemoryManager manager)",
				"documentation": ""
			},
			{
				"signature": "public final int numRows()",
				"documentation": ""
			},
			{
				"signature": "public final void close()",
				"documentation": ""
			},
			{
				"signature": "private boolean acquirePage(long requiredSize)",
				"documentation": ""
			},
			{
				"signature": "public final UnsafeRow getValueRow(int rowId)",
				"documentation": "/**\n   * Returns the value row in this batch at `rowId`. Returned value row is reused across calls.\n   * Because `getValueRow(id)` is always called after `getKeyRow(id)` with the same id, we use\n   * `getValueFromKey(id) to retrieve value row, which reuses metadata from the cached key.\n   */"
			},
			{
				"signature": "public final long spill(long size, MemoryConsumer trigger) throws IOException",
				"documentation": "/**\n   * Sometimes the TaskMemoryManager may call spill() on its associated MemoryConsumers to make\n   * space for new consumers. For RowBasedKeyValueBatch, we do not actually spill and return 0.\n   * We should not throw OutOfMemory exception here because other associated consumers might spill\n   */"
			}
		],
		"interfaces": [
			"java.io.Closeable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.catalyst.expressions.FixedLengthRowBasedKeyValueBatch",
			"org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.catalyst.expressions.SpecializedGetters",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.catalyst.expressions.SpecializedGettersReader",
		"extends": "",
		"Methods": [
			{
				"signature": "private SpecializedGettersReader()",
				"documentation": ""
			},
			{
				"signature": "public static Object read(\n      SpecializedGetters obj,\n      int ordinal,\n      DataType dataType,\n      boolean handleNull,\n      boolean handleUserDefinedType)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.vectorized.ColumnarArray"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An Unsafe implementation of Array which is backed by raw memory instead of Java objects.\n *\n * Each array has four parts:\n *   [numElements][null bits][values or offset\u0026length][variable length portion]\n *\n * The `numElements` is 8 bytes storing the number of elements of this array.\n *\n * In the `null bits` region, we store 1 bit per element, represents whether an element is null\n * Its total size is ceil(numElements / 8) bytes, and it is aligned to 8-byte boundaries.\n *\n * In the `values or offset\u0026length` region, we store the content of elements. For fields that hold\n * fixed-length primitive types, such as long, double, or int, we store the value directly\n * in the field. The whole fixed-length portion (even for byte) is aligned to 8-byte boundaries.\n * For fields with non-primitive or variable-length values, we store a relative offset\n * (w.r.t. the base address of the array) that points to the beginning of the variable-length field\n * and length (they are combined into a long). For variable length portion, each is aligned\n * to 8-byte boundaries.\n *\n * Instances of `UnsafeArrayData` act as pointers to row data stored in this format.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.UnsafeArrayData",
		"extends": "org.apache.spark.sql.catalyst.util.ArrayData",
		"Methods": [
			{
				"signature": "public static int calculateHeaderPortionInBytes(int numFields)",
				"documentation": "/**\n * An Unsafe implementation of Array which is backed by raw memory instead of Java objects.\n *\n * Each array has four parts:\n *   [numElements][null bits][values or offset\u0026length][variable length portion]\n *\n * The `numElements` is 8 bytes storing the number of elements of this array.\n *\n * In the `null bits` region, we store 1 bit per element, represents whether an element is null\n * Its total size is ceil(numElements / 8) bytes, and it is aligned to 8-byte boundaries.\n *\n * In the `values or offset\u0026length` region, we store the content of elements. For fields that hold\n * fixed-length primitive types, such as long, double, or int, we store the value directly\n * in the field. The whole fixed-length portion (even for byte) is aligned to 8-byte boundaries.\n * For fields with non-primitive or variable-length values, we store a relative offset\n * (w.r.t. the base address of the array) that points to the beginning of the variable-length field\n * and length (they are combined into a long). For variable length portion, each is aligned\n * to 8-byte boundaries.\n *\n * Instances of `UnsafeArrayData` act as pointers to row data stored in this format.\n */"
			},
			{
				"signature": "public static long calculateHeaderPortionInBytes(long numFields)",
				"documentation": ""
			},
			{
				"signature": "private long getElementOffset(int ordinal, int elementSize)",
				"documentation": "/** The position to start storing array elements, */"
			},
			{
				"signature": "public Object getBaseObject()",
				"documentation": ""
			},
			{
				"signature": "public long getBaseOffset()",
				"documentation": ""
			},
			{
				"signature": "public int getSizeInBytes()",
				"documentation": ""
			},
			{
				"signature": "private void assertIndexIsValid(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "public Object[] array()",
				"documentation": ""
			},
			{
				"signature": "public UnsafeArrayData()",
				"documentation": "/**\n   * Construct a new UnsafeArrayData. The resulting UnsafeArrayData won't be usable until\n   * `pointTo()` has been called, since the value returned by this constructor is equivalent\n   * to a null pointer.\n   */"
			},
			{
				"signature": "@Override\n  public int numElements()",
				"documentation": ""
			},
			{
				"signature": "public void pointTo(Object baseObject, long baseOffset, int sizeInBytes)",
				"documentation": "/**\n   * Update this UnsafeArrayData to point to different backing data.\n   *\n   * @param baseObject the base object\n   * @param baseOffset the offset within the base object\n   * @param sizeInBytes the size of this array's backing data, in bytes\n   */"
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object get(int ordinal, DataType dataType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int ordinal, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public CalendarInterval getInterval(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeRow getStruct(int ordinal, int numFields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeArrayData getArray(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeMapData getMap(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void update(int ordinal, Object value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setBoolean(int ordinal, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setByte(int ordinal, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setShort(int ordinal, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setInt(int ordinal, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setLong(int ordinal, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setFloat(int ordinal, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setDouble(int ordinal, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "public void writeToMemory(Object target, long targetOffset)",
				"documentation": ""
			},
			{
				"signature": "public void writeTo(ByteBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeArrayData copy()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean[] toBooleanArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] toByteArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short[] toShortArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int[] toIntArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long[] toLongArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float[] toFloatArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double[] toDoubleArray()",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData fromPrimitiveArray(\n       Object arr, int offset, int length, int elementSize)",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData createFreshArray(int length, int elementSize)",
				"documentation": ""
			},
			{
				"signature": "public static boolean shouldUseGenericArrayData(int elementSize, long length)",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData fromPrimitiveArray(boolean[] arr)",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData fromPrimitiveArray(byte[] arr)",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData fromPrimitiveArray(short[] arr)",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData fromPrimitiveArray(int[] arr)",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData fromPrimitiveArray(long[] arr)",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData fromPrimitiveArray(float[] arr)",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeArrayData fromPrimitiveArray(double[] arr)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void writeExternal(ObjectOutput out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(Kryo kryo, Output output)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void read(Kryo kryo, Input input)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Externalizable",
			"com.esotericsoftware.kryo.KryoSerializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.array.ByteArrayMethods",
			"org.apache.spark.unsafe.bitset.BitSetMethods",
			"org.apache.spark.unsafe.hash.Murmur3_x86_32",
			"org.apache.spark.unsafe.types.CalendarInterval",
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [
			"org.apache.spark.sql.vectorized.ColumnarArray"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * General utilities available for unsafe data\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.UnsafeDataUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "private UnsafeDataUtils()",
				"documentation": "/**\n * General utilities available for unsafe data\n */"
			},
			{
				"signature": "public static byte[] getBytes(Object baseObject, long baseOffset, int sizeInBytes)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An Unsafe implementation of Map which is backed by raw memory instead of Java objects.\n *\n * Currently we just use 2 UnsafeArrayData to represent UnsafeMapData, with extra 8 bytes at head\n * to indicate the number of bytes of the unsafe key array.\n * [unsafe key array numBytes] [unsafe key array] [unsafe value array]\n *\n * Note that, user is responsible to guarantee that the key array does not have duplicated\n * elements, otherwise the behavior is undefined.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.UnsafeMapData",
		"extends": "org.apache.spark.sql.catalyst.util.MapData",
		"Methods": [
			{
				"signature": "public Object getBaseObject()",
				"documentation": "/**\n * An Unsafe implementation of Map which is backed by raw memory instead of Java objects.\n *\n * Currently we just use 2 UnsafeArrayData to represent UnsafeMapData, with extra 8 bytes at head\n * to indicate the number of bytes of the unsafe key array.\n * [unsafe key array numBytes] [unsafe key array] [unsafe value array]\n *\n * Note that, user is responsible to guarantee that the key array does not have duplicated\n * elements, otherwise the behavior is undefined.\n */"
			},
			{
				"signature": "public long getBaseOffset()",
				"documentation": ""
			},
			{
				"signature": "public int getSizeInBytes()",
				"documentation": ""
			},
			{
				"signature": "public UnsafeMapData()",
				"documentation": "/**\n   * Construct a new UnsafeMapData. The resulting UnsafeMapData won't be usable until\n   * `pointTo()` has been called, since the value returned by this constructor is equivalent\n   * to a null pointer.\n   */"
			},
			{
				"signature": "public void pointTo(Object baseObject, long baseOffset, int sizeInBytes)",
				"documentation": "/**\n   * Update this UnsafeMapData to point to different backing data.\n   *\n   * @param baseObject the base object\n   * @param baseOffset the offset within the base object\n   * @param sizeInBytes the size of this map's backing data, in bytes\n   */"
			},
			{
				"signature": "@Override\n  public int numElements()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeArrayData keyArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeArrayData valueArray()",
				"documentation": ""
			},
			{
				"signature": "public void writeToMemory(Object target, long targetOffset)",
				"documentation": ""
			},
			{
				"signature": "public void writeTo(ByteBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeMapData copy()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void writeExternal(ObjectOutput out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readExternal(ObjectInput in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(Kryo kryo, Output output)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void read(Kryo kryo, Input input)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Externalizable",
			"com.esotericsoftware.kryo.KryoSerializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An Unsafe implementation of Row which is backed by raw memory instead of Java objects.\n *\n * Each tuple has three parts: [null bit set] [values] [variable length portion]\n *\n * The bit set is used for null tracking and is aligned to 8-byte word boundaries.  It stores\n * one bit per field.\n *\n * In the `values` region, we store one 8-byte word per field. For fields that hold fixed-length\n * primitive types, such as long, double, or int, we store the value directly in the word. For\n * fields with non-primitive or variable-length values, we store a relative offset (w.r.t. the\n * base address of the row) that points to the beginning of the variable-length field, and length\n * (they are combined into a long).\n *\n * Instances of `UnsafeRow` act as pointers to row data stored in this format.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.UnsafeRow",
		"extends": "org.apache.spark.sql.catalyst.InternalRow",
		"Methods": [
			{
				"signature": "public static int calculateBitSetWidthInBytes(int numFields)",
				"documentation": "/**\n * An Unsafe implementation of Row which is backed by raw memory instead of Java objects.\n *\n * Each tuple has three parts: [null bit set] [values] [variable length portion]\n *\n * The bit set is used for null tracking and is aligned to 8-byte word boundaries.  It stores\n * one bit per field.\n *\n * In the `values` region, we store one 8-byte word per field. For fields that hold fixed-length\n * primitive types, such as long, double, or int, we store the value directly in the word. For\n * fields with non-primitive or variable-length values, we store a relative offset (w.r.t. the\n * base address of the row) that points to the beginning of the variable-length field, and length\n * (they are combined into a long).\n *\n * Instances of `UnsafeRow` act as pointers to row data stored in this format.\n */"
			},
			{
				"signature": "static",
				"documentation": "/**\n   * Field types that can be updated in place in UnsafeRows (e.g. we support set() for these types)\n   */"
			},
			{
				"signature": "public static boolean isFixedLength(DataType dt)",
				"documentation": ""
			},
			{
				"signature": "public static boolean isMutable(DataType dt)",
				"documentation": ""
			},
			{
				"signature": "private long getFieldOffset(int ordinal)",
				"documentation": "/** The width of the null tracking bit set, in bytes */"
			},
			{
				"signature": "private void assertIndexIsValid(int index)",
				"documentation": ""
			},
			{
				"signature": "public UnsafeRow(int numFields)",
				"documentation": "/**\n   * Construct a new UnsafeRow. The resulting row won't be usable until `pointTo()` has been called,\n   * since the value returned by this constructor is equivalent to a null pointer.\n   *\n   * @param numFields the number of fields in this row\n   */"
			},
			{
				"signature": "public UnsafeRow()",
				"documentation": ""
			},
			{
				"signature": "public Object getBaseObject()",
				"documentation": ""
			},
			{
				"signature": "public long getBaseOffset()",
				"documentation": ""
			},
			{
				"signature": "public int getSizeInBytes()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numFields()",
				"documentation": ""
			},
			{
				"signature": "public void pointTo(Object baseObject, long baseOffset, int sizeInBytes)",
				"documentation": "/**\n   * Update this UnsafeRow to point to different backing data.\n   *\n   * @param baseObject the base object\n   * @param baseOffset the offset within the base object\n   * @param sizeInBytes the size of this row's backing data, in bytes\n   */"
			},
			{
				"signature": "public void pointTo(byte[] buf, int sizeInBytes)",
				"documentation": "/**\n   * Update this UnsafeRow to point to the underlying byte array.\n   *\n   * @param buf byte array to point to\n   * @param sizeInBytes the number of bytes valid in the byte array\n   */"
			},
			{
				"signature": "public void setTotalSize(int sizeInBytes)",
				"documentation": ""
			},
			{
				"signature": "public void setNotNullAt(int i)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNullAt(int i)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void update(int ordinal, Object value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setInt(int ordinal, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setLong(int ordinal, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setDouble(int ordinal, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setBoolean(int ordinal, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setShort(int ordinal, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setByte(int ordinal, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setFloat(int ordinal, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setDecimal(int ordinal, Decimal value, int precision)",
				"documentation": "/**\n   * Updates the decimal column.\n   *\n   * Note: In order to support update a decimal with precision \u003e 18, CAN NOT call\n   * setNullAt() for this column.\n   */"
			},
			{
				"signature": "@Override\n  public void setInterval(int ordinal, CalendarInterval value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object get(int ordinal, DataType dataType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int ordinal, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public CalendarInterval getInterval(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeRow getStruct(int ordinal, int numFields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeArrayData getArray(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeMapData getMap(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnsafeRow copy()",
				"documentation": "/**\n   * Copies this row, returning a self-contained UnsafeRow that stores its data in an internal\n   * byte array rather than referencing data stored in a data page.\n   */"
			},
			{
				"signature": "public static UnsafeRow createFromByteArray(int numBytes, int numFields)",
				"documentation": "/**\n   * Creates an empty UnsafeRow from a byte array with specified numBytes and numFields.\n   * The returned row is invalid until we call copyFrom on it.\n   */"
			},
			{
				"signature": "public void copyFrom(UnsafeRow row)",
				"documentation": "/**\n   * Copies the input UnsafeRow to this UnsafeRow, and resize the underlying byte[] when the\n   * input row is larger than this row.\n   */"
			},
			{
				"signature": "public void writeToStream(OutputStream out, byte[] writeBuffer) throws IOException",
				"documentation": "/**\n   * Write this UnsafeRow's underlying bytes to the given OutputStream.\n   *\n   * @param out the stream to write to.\n   * @param writeBuffer a byte array for buffering chunks of off-heap data while writing to the\n   *                    output stream. If this row is backed by an on-heap byte array, then this\n   *                    buffer will not be used and may be null.\n   */"
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "public byte[] getBytes()",
				"documentation": "/**\n   * Returns the underlying bytes for this UnsafeRow.\n   */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean anyNull()",
				"documentation": ""
			},
			{
				"signature": "public void writeToMemory(Object target, long targetOffset)",
				"documentation": "/**\n   * Writes the content of this row into a memory address, identified by an object and an offset.\n   * The target memory address must already been allocated, and have enough space to hold all the\n   * bytes in this string.\n   */"
			},
			{
				"signature": "public void writeTo(ByteBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "public void writeFieldTo(int ordinal, ByteBuffer buffer)",
				"documentation": "/**\n   * Write the bytes of var-length field into ByteBuffer\n   *\n   * Note: only work with HeapByteBuffer\n   */"
			},
			{
				"signature": "@Override\n  public void writeExternal(ObjectOutput out) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(Kryo kryo, Output out)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void read(Kryo kryo, Input in)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Externalizable",
			"com.esotericsoftware.kryo.KryoSerializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.array.ByteArrayMethods",
			"org.apache.spark.unsafe.bitset.BitSetMethods",
			"org.apache.spark.unsafe.hash.Murmur3_x86_32",
			"org.apache.spark.unsafe.types.CalendarInterval",
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [
			"org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder",
			"org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter",
			"org.apache.spark.sql.execution.BufferedRowIterator",
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter",
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter.RowComparator",
			"org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap",
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter",
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter.KVComparator",
			"org.apache.spark.sql.execution.KVSorterIterator"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An implementation of `RowBasedKeyValueBatch` in which key-value records have variable lengths.\n *\n *  The format for each record looks like this (in case of uaoSize = 4):\n * [4 bytes total size = (klen + vlen + 4)] [4 bytes key size = klen]\n * [UnsafeRow for key of length klen] [UnsafeRow for Value of length vlen]\n * [8 bytes pointer to next]\n * Thus, record length = 4 + 4 + klen + vlen + 8\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.VariableLengthRowBasedKeyValueBatch",
		"extends": "org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch",
		"Methods": [
			{
				"signature": "@Override\n  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen)",
				"documentation": "/**\n   * Append a key value pair.\n   * It copies data into the backing MemoryBlock.\n   * Returns an UnsafeRow pointing to the value if succeeds, otherwise returns null.\n   */"
			},
			{
				"signature": "@Override\n  public UnsafeRow getKeyRow(int rowId)",
				"documentation": "/**\n   * Returns the key row in this batch at `rowId`. Returned key row is reused across calls.\n   */"
			},
			{
				"signature": "@Override\n  public UnsafeRow getValueFromKey(int rowId)",
				"documentation": "/**\n   * Returns the value row by two steps:\n   * 1) looking up the key row with the same id (skipped if the key row is cached)\n   * 2) retrieve the value row by reusing the metadata from step 1)\n   * In most times, 1) is skipped because `getKeyRow(id)` is often called before `getValueRow(id)`.\n   */"
			},
			{
				"signature": "@Override\n  public org.apache.spark.unsafe.KVIterator\u003cUnsafeRow, UnsafeRow\u003e rowIterator()",
				"documentation": "/**\n   * Returns an iterator to go through all rows\n   */"
			},
			{
				"signature": "private void init()",
				"documentation": "/**\n   * Returns an iterator to go through all rows\n   */"
			},
			{
				"signature": "@Override\n      public boolean next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public UnsafeRow getKey()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public UnsafeRow getValue()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void close()",
				"documentation": ""
			},
			{
				"signature": "private void freeCurrentPage()",
				"documentation": ""
			},
			{
				"signature": "protected VariableLengthRowBasedKeyValueBatch(StructType keySchema, StructType valueSchema,\n                                              int maxRows, TaskMemoryManager manager)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UnsafeAlignedOffset"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * xxHash64. A high quality and fast 64 bit hash code by Yann Colet and Mathias Westerdahl. The\n * class below is modelled like its Murmur3_x86_32 cousin.\n * \u003cp/\u003e\n * This was largely based on the following (original) C and Java implementations:\n * https://github.com/Cyan4973/xxHash/blob/master/xxhash.c\n * https://github.com/OpenHFT/Zero-Allocation-Hashing/blob/master/src/main/java/net/openhft/hashing/XxHash_r39.java\n * https://github.com/airlift/slice/blob/master/src/main/java/io/airlift/slice/XxHash64.java\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.XXH64",
		"extends": "",
		"Methods": [
			{
				"signature": "public XXH64(long seed)",
				"documentation": "/**\n * xxHash64. A high quality and fast 64 bit hash code by Yann Colet and Mathias Westerdahl. The\n * class below is modelled like its Murmur3_x86_32 cousin.\n * \u003cp/\u003e\n * This was largely based on the following (original) C and Java implementations:\n * https://github.com/Cyan4973/xxHash/blob/master/xxhash.c\n * https://github.com/OpenHFT/Zero-Allocation-Hashing/blob/master/src/main/java/net/openhft/hashing/XxHash_r39.java\n * https://github.com/airlift/slice/blob/master/src/main/java/io/airlift/slice/XxHash64.java\n */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "public long hashInt(int input)",
				"documentation": ""
			},
			{
				"signature": "public static long hashInt(int input, long seed)",
				"documentation": ""
			},
			{
				"signature": "public long hashLong(long input)",
				"documentation": ""
			},
			{
				"signature": "public static long hashLong(long input, long seed)",
				"documentation": ""
			},
			{
				"signature": "public long hashUnsafeWords(Object base, long offset, int length)",
				"documentation": ""
			},
			{
				"signature": "public static long hashUnsafeWords(Object base, long offset, int length, long seed)",
				"documentation": ""
			},
			{
				"signature": "public long hashUnsafeBytes(Object base, long offset, int length)",
				"documentation": ""
			},
			{
				"signature": "public static long hashUnsafeBytes(Object base, long offset, int length, long seed)",
				"documentation": ""
			},
			{
				"signature": "public static long hashUTF8String(UTF8String str, long seed)",
				"documentation": ""
			},
			{
				"signature": "private static long fmix(long hash)",
				"documentation": ""
			},
			{
				"signature": "private static long hashBytesByWords(Object base, long offset, int length, long seed)",
				"documentation": ""
			},
			{
				"signature": "do",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A helper class to manage the data buffer for an unsafe row.  The data buffer can grow and\n * automatically re-point the unsafe row to it.\n *\n * This class can be used to build a one-pass unsafe row writing program, i.e. data will be written\n * to the data buffer directly and no extra copy is needed.  There should be only one instance of\n * this class per writing program, so that the memory segment/data buffer can be reused.  Note that\n * for each incoming record, we should call `reset` of BufferHolder instance before write the record\n * and reuse the data buffer.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder",
		"extends": "",
		"Methods": [
			{
				"signature": "BufferHolder(UnsafeRow row)",
				"documentation": "/**\n * A helper class to manage the data buffer for an unsafe row.  The data buffer can grow and\n * automatically re-point the unsafe row to it.\n *\n * This class can be used to build a one-pass unsafe row writing program, i.e. data will be written\n * to the data buffer directly and no extra copy is needed.  There should be only one instance of\n * this class per writing program, so that the memory segment/data buffer can be reused.  Note that\n * for each incoming record, we should call `reset` of BufferHolder instance before write the record\n * and reuse the data buffer.\n */"
			},
			{
				"signature": "BufferHolder(UnsafeRow row, int initialSize)",
				"documentation": ""
			},
			{
				"signature": "void grow(int neededSize)",
				"documentation": "/**\n   * Grows the buffer by at least neededSize and points the row to the buffer.\n   */"
			},
			{
				"signature": "byte[] getBuffer()",
				"documentation": ""
			},
			{
				"signature": "int getCursor()",
				"documentation": ""
			},
			{
				"signature": "void increaseCursor(int val)",
				"documentation": ""
			},
			{
				"signature": "void reset()",
				"documentation": ""
			},
			{
				"signature": "int totalSize()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A helper class to write data into global row buffer using `UnsafeArrayData` format,\n * used by {@link org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection}.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter",
		"extends": "org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter",
		"Methods": [
			{
				"signature": "private void assertIndexIsValid(int index)",
				"documentation": "/**\n * A helper class to write data into global row buffer using `UnsafeArrayData` format,\n * used by {@link org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection}.\n */"
			},
			{
				"signature": "public UnsafeArrayWriter(UnsafeWriter writer, int elementSize)",
				"documentation": ""
			},
			{
				"signature": "public void initialize(int numElements)",
				"documentation": ""
			},
			{
				"signature": "private long getElementOffset(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "private void setNullBit(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNull1Bytes(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNull2Bytes(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNull4Bytes(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNull8Bytes(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "public void setNull(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, Decimal input, int precision, int scale)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A helper class to write data into global row buffer using `UnsafeRow` format.\n *\n * It will remember the offset of row buffer which it starts to write, and move the cursor of row\n * buffer while writing.  If new data(can be the input record if this is the outermost writer, or\n * nested struct if this is an inner writer) comes, the starting cursor of row buffer may be\n * changed, so we need to call `UnsafeRowWriter.resetRowWriter` before writing, to update the\n * `startingOffset` and clear out null bits.\n *\n * Note that if this is the outermost writer, which means we will always write from the very\n * beginning of the global row buffer, we don't need to update `startingOffset` and can just call\n * `zeroOutNullBytes` before writing new data.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter",
		"extends": "org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter",
		"Methods": [
			{
				"signature": "public UnsafeRowWriter(int numFields)",
				"documentation": "/**\n * A helper class to write data into global row buffer using `UnsafeRow` format.\n *\n * It will remember the offset of row buffer which it starts to write, and move the cursor of row\n * buffer while writing.  If new data(can be the input record if this is the outermost writer, or\n * nested struct if this is an inner writer) comes, the starting cursor of row buffer may be\n * changed, so we need to call `UnsafeRowWriter.resetRowWriter` before writing, to update the\n * `startingOffset` and clear out null bits.\n *\n * Note that if this is the outermost writer, which means we will always write from the very\n * beginning of the global row buffer, we don't need to update `startingOffset` and can just call\n * `zeroOutNullBytes` before writing new data.\n */"
			},
			{
				"signature": "public UnsafeRowWriter(int numFields, int initialBufferSize)",
				"documentation": ""
			},
			{
				"signature": "public UnsafeRowWriter(UnsafeWriter writer, int numFields)",
				"documentation": ""
			},
			{
				"signature": "private UnsafeRowWriter(UnsafeRow row)",
				"documentation": ""
			},
			{
				"signature": "private UnsafeRowWriter(UnsafeRow row, int initialBufferSize)",
				"documentation": ""
			},
			{
				"signature": "private UnsafeRowWriter(UnsafeRow row, BufferHolder holder, int numFields)",
				"documentation": ""
			},
			{
				"signature": "public UnsafeRow getRow()",
				"documentation": "/**\n   * Updates total size of the UnsafeRow using the size collected by BufferHolder, and returns\n   * the UnsafeRow created at a constructor\n   */"
			},
			{
				"signature": "public void resetRowWriter()",
				"documentation": "/**\n   * Resets the `startingOffset` according to the current cursor of row buffer, and clear out null\n   * bits.  This should be called before we write a new nested struct to the row buffer.\n   */"
			},
			{
				"signature": "public void zeroOutNullBytes()",
				"documentation": "/**\n   * Clears out null bits.  This should be called before we write a new row to row buffer.\n   */"
			},
			{
				"signature": "public boolean isNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "public void setNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNull1Bytes(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNull2Bytes(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNull4Bytes(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNull8Bytes(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "public long getFieldOffset(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void write(int ordinal, Decimal input, int precision, int scale)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.bitset.BitSetMethods",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base class for writing Unsafe* structures.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter",
		"extends": "",
		"Methods": [
			{
				"signature": "protected UnsafeWriter(BufferHolder holder)",
				"documentation": "/**\n * Base class for writing Unsafe* structures.\n */"
			},
			{
				"signature": "public final BufferHolder getBufferHolder()",
				"documentation": "/**\n   * Accessor methods are delegated from BufferHolder class\n   */"
			},
			{
				"signature": "public final byte[] getBuffer()",
				"documentation": ""
			},
			{
				"signature": "public final void reset()",
				"documentation": ""
			},
			{
				"signature": "public final int totalSize()",
				"documentation": ""
			},
			{
				"signature": "public final void grow(int neededSize)",
				"documentation": ""
			},
			{
				"signature": "public final int cursor()",
				"documentation": ""
			},
			{
				"signature": "public final void increaseCursor(int val)",
				"documentation": ""
			},
			{
				"signature": "public final void setOffsetAndSizeFromPreviousCursor(int ordinal, int previousCursor)",
				"documentation": ""
			},
			{
				"signature": "protected void setOffsetAndSize(int ordinal, int size)",
				"documentation": ""
			},
			{
				"signature": "protected void setOffsetAndSize(int ordinal, int currentCursor, int size)",
				"documentation": ""
			},
			{
				"signature": "protected final void zeroOutPaddingBytes(int numBytes)",
				"documentation": ""
			},
			{
				"signature": "public final void write(int ordinal, UTF8String input)",
				"documentation": ""
			},
			{
				"signature": "public final void write(int ordinal, byte[] input)",
				"documentation": ""
			},
			{
				"signature": "public final void write(int ordinal, byte[] input, int offset, int numBytes)",
				"documentation": ""
			},
			{
				"signature": "private void writeUnalignedBytes(\n      int ordinal,\n      Object baseObject,\n      long baseOffset,\n      int numBytes)",
				"documentation": ""
			},
			{
				"signature": "public final void write(int ordinal, CalendarInterval input)",
				"documentation": ""
			},
			{
				"signature": "public final void write(int ordinal, UnsafeRow row)",
				"documentation": ""
			},
			{
				"signature": "public final void write(int ordinal, UnsafeMapData map)",
				"documentation": ""
			},
			{
				"signature": "public final void write(UnsafeArrayData array)",
				"documentation": ""
			},
			{
				"signature": "private void writeAlignedBytes(\n      int ordinal,\n      Object baseObject,\n      long baseOffset,\n      int numBytes)",
				"documentation": ""
			},
			{
				"signature": "protected final void writeBoolean(long offset, boolean value)",
				"documentation": ""
			},
			{
				"signature": "protected final void writeByte(long offset, byte value)",
				"documentation": ""
			},
			{
				"signature": "protected final void writeShort(long offset, short value)",
				"documentation": ""
			},
			{
				"signature": "protected final void writeInt(long offset, int value)",
				"documentation": ""
			},
			{
				"signature": "protected final void writeLong(long offset, long value)",
				"documentation": ""
			},
			{
				"signature": "protected final void writeFloat(long offset, float value)",
				"documentation": ""
			},
			{
				"signature": "protected final void writeDouble(long offset, double value)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter",
			"org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Utility class for all XPath UDFs. Each UDF instance should keep an instance of this class.\n *\n * This is based on Hive's UDFXPathUtil implementation.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtil",
		"extends": "",
		"Methods": [
			{
				"signature": "public Object eval(String xml, String path, QName qname) throws XPathExpressionException",
				"documentation": "/**\n * Utility class for all XPath UDFs. Each UDF instance should keep an instance of this class.\n *\n * This is based on Hive's UDFXPathUtil implementation.\n */"
			},
			{
				"signature": "private void initializeDocumentBuilderFactory() throws ParserConfigurationException",
				"documentation": ""
			},
			{
				"signature": "public Boolean evalBoolean(String xml, String path) throws XPathExpressionException",
				"documentation": ""
			},
			{
				"signature": "public String evalString(String xml, String path) throws XPathExpressionException",
				"documentation": ""
			},
			{
				"signature": "public Double evalNumber(String xml, String path) throws XPathExpressionException",
				"documentation": ""
			},
			{
				"signature": "public Node evalNode(String xml, String path) throws XPathExpressionException",
				"documentation": ""
			},
			{
				"signature": "public NodeList evalNodeList(String xml, String path) throws XPathExpressionException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtil.ReusableStringReader"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtil.ReusableStringReader"
		]
	},
	{
		"documentation": "/**\n   * Reusable, non-threadsafe version of {@link java.io.StringReader}.\n   */",
		"name": "org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtil.ReusableStringReader",
		"extends": "java.io.Reader",
		"Methods": [
			{
				"signature": "public ReusableStringReader()",
				"documentation": "/**\n   * Reusable, non-threadsafe version of {@link java.io.StringReader}.\n   */"
			},
			{
				"signature": "public void set(String s)",
				"documentation": ""
			},
			{
				"signature": "private void ensureOpen() throws IOException",
				"documentation": "/** Check to make sure that the stream has not been closed */"
			},
			{
				"signature": "@Override\n    public int read() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int read(char[] cbuf, int off, int len) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public long skip(long ns) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean ready() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean markSupported()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void mark(int readAheadLimit) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void reset() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtil"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "private static UTF8String trimTrailingSpaces(\n      UTF8String inputStr, int numChars, int limit)",
				"documentation": ""
			},
			{
				"signature": "public static UTF8String charTypeWriteSideCheck(UTF8String inputStr, int limit)",
				"documentation": ""
			},
			{
				"signature": "public static UTF8String varcharTypeWriteSideCheck(UTF8String inputStr, int limit)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface to execute an arbitrary string command inside an external execution engine rather\n * than Spark. This could be useful when user wants to execute some commands out of Spark. For\n * example, executing custom DDL/DML command for JDBC, creating index for ElasticSearch, creating\n * cores for Solr and so on.\n * \u003cp\u003e\n * This interface will be instantiated when end users call `SparkSession#executeCommand`.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.ExternalCommandRunner",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Unstable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An API to extend the Spark built-in session catalog. Implementation can get the built-in session\n * catalog from {@link #setDelegateCatalog(CatalogPlugin)}, implement catalog functions with\n * some custom logic and call the built-in session catalog at the end. For example, they can\n * implement {@code createTable}, do something else before calling {@code createTable} of the\n * built-in session catalog.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.CatalogExtension",
		"extends": "TableCatalog,",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A marker interface to provide a catalog implementation for Spark.\n * \u003cp\u003e\n * Implementations can provide catalog functions by implementing additional interfaces for tables,\n * views, and functions.\n * \u003cp\u003e\n * Catalog implementations must implement this marker interface to be loaded by\n * {@link Catalogs#load(String, SQLConf)}. The loader will instantiate catalog classes using the\n * required public no-arg constructor. After creating an instance, it will be configured by calling\n * {@link #initialize(String, CaseInsensitiveStringMap)}.\n * \u003cp\u003e\n * Catalog implementations are registered to a name by adding a configuration option to Spark:\n * {@code spark.sql.catalog.catalog-name=com.example.YourCatalogClass}. All configuration properties\n * in the Spark configuration that share the catalog name prefix,\n * {@code spark.sql.catalog.catalog-name.(key)=(value)} will be passed in the case insensitive\n * string map of options in initialization with the prefix removed.\n * {@code name}, is also passed and is the catalog's name; in this case, \"catalog-name\".\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.CatalogPlugin",
		"extends": "",
		"Methods": [
			{
				"signature": "default String[] defaultNamespace()",
				"documentation": "/**\n   * Return a default namespace for the catalog.\n   * \u003cp\u003e\n   * When this catalog is set as the current catalog, the namespace returned by this method will be\n   * set as the current namespace.\n   * \u003cp\u003e\n   * The namespace returned by this method is not required to exist.\n   *\n   * @return a multi-part namespace\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.FunctionCatalog",
			"org.apache.spark.sql.connector.catalog.SupportsNamespaces",
			"org.apache.spark.sql.connector.catalog.TableCatalog"
		],
		"implementedBy": [
			"org.apache.spark.sql.connector.catalog.TestCatalogPlugin",
			"org.apache.spark.sql.connector.catalog.ConstructorFailureCatalogPlugin",
			"org.apache.spark.sql.connector.catalog.AccessErrorCatalogPlugin"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A simple implementation of {@link CatalogExtension}, which implements all the catalog functions\n * by calling the built-in session catalog directly. This is created for convenience, so that users\n * only need to override some methods where they want to apply custom logic. For example, they can\n * override {@code createTable}, do something else before calling {@code super.createTable}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension",
		"extends": "",
		"Methods": [
			{
				"signature": "public final void setDelegateCatalog(CatalogPlugin delegate)",
				"documentation": "/**\n * A simple implementation of {@link CatalogExtension}, which implements all the catalog functions\n * by calling the built-in session catalog directly. This is created for convenience, so that users\n * only need to override some methods where they want to apply custom logic. For example, they can\n * override {@code createTable}, do something else before calling {@code super.createTable}.\n *\n * @since 3.0.0\n */"
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void initialize(String name, CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String[] defaultNamespace()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Identifier[] listTables(String[] namespace) throws NoSuchNamespaceException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Table loadTable(Identifier ident) throws NoSuchTableException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Table loadTable(Identifier ident, long timestamp) throws NoSuchTableException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Table loadTable(Identifier ident, String version) throws NoSuchTableException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void invalidateTable(Identifier ident)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean tableExists(Identifier ident)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Table createTable(\n      Identifier ident,\n      StructType schema,\n      Transform[] partitions,\n      Map\u003cString, String\u003e properties) throws TableAlreadyExistsException, NoSuchNamespaceException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Table alterTable(\n      Identifier ident,\n      TableChange... changes) throws NoSuchTableException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean dropTable(Identifier ident)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean purgeTable(Identifier ident)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void renameTable(\n      Identifier oldIdent,\n      Identifier newIdent) throws NoSuchTableException, TableAlreadyExistsException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String[][] listNamespaces() throws NoSuchNamespaceException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String[][] listNamespaces(String[] namespace) throws NoSuchNamespaceException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean namespaceExists(String[] namespace)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Map\u003cString, String\u003e loadNamespaceMetadata(\n      String[] namespace) throws NoSuchNamespaceException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void createNamespace(\n      String[] namespace,\n      Map\u003cString, String\u003e metadata) throws NamespaceAlreadyExistsException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void alterNamespace(\n      String[] namespace,\n      NamespaceChange... changes) throws NoSuchNamespaceException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean dropNamespace(\n      String[] namespace,\n      boolean cascade) throws NoSuchNamespaceException, NonEmptyNamespaceException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UnboundFunction loadFunction(Identifier ident) throws NoSuchFunctionException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Identifier[] listFunctions(String[] namespace) throws NoSuchNamespaceException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean functionExists(Identifier ident)",
				"documentation": ""
			},
			{
				"signature": "private TableCatalog asTableCatalog()",
				"documentation": ""
			},
			{
				"signature": "private SupportsNamespaces asNamespaceCatalog()",
				"documentation": ""
			},
			{
				"signature": "private FunctionCatalog asFunctionCatalog()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.CatalogExtension"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Catalog methods for working with Functions.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.FunctionCatalog",
		"extends": "org.apache.spark.sql.connector.catalog.CatalogPlugin",
		"Methods": [
			{
				"signature": "default boolean functionExists(Identifier ident)",
				"documentation": "/**\n   * Returns true if the function exists, false otherwise.\n   *\n   * @since 3.3.0\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Identifies an object in a catalog.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.Identifier",
		"extends": "",
		"Methods": [
			{
				"signature": "static Identifier of(String[] namespace, String name)",
				"documentation": "/**\n * Identifies an object in a catalog.\n *\n * @since 3.0.0\n */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.catalog.IdentifierImpl"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n *  An {@link Identifier} implementation.\n */",
		"name": "org.apache.spark.sql.connector.catalog.IdentifierImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "IdentifierImpl(String[] namespace, String name)",
				"documentation": "/**\n *  An {@link Identifier} implementation.\n */"
			},
			{
				"signature": "@Override\n  public String[] namespace()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.Identifier"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface for a metadata column.\n * \u003cp\u003e\n * A metadata column can expose additional metadata about a row. For example, rows from Kafka can\n * use metadata columns to expose a message's topic, partition number, and offset.\n * \u003cp\u003e\n * A metadata column could also be the result of a transform applied to a value in the row. For\n * example, a partition value produced by bucket(id, 16) could be exposed by a metadata column. In\n * this case, {@link #transform()} should return a non-null {@link Transform} that produced the\n * metadata column's values.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.MetadataColumn",
		"extends": "",
		"Methods": [
			{
				"signature": "default boolean isNullable()",
				"documentation": "/**\n   * @return whether values produced by this metadata column may be null\n   */"
			},
			{
				"signature": "default String comment()",
				"documentation": "/**\n   * Documentation for this metadata column, or null.\n   *\n   * @return a documentation String\n   */"
			},
			{
				"signature": "default Transform transform()",
				"documentation": "/**\n   * The {@link Transform} used to produce this metadata column from data rows, or null.\n   *\n   * @return a {@link Transform} used to produce the column's values, or null if there isn't one\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * NamespaceChange subclasses represent requested changes to a namespace. These are passed to\n * {@link SupportsNamespaces#alterNamespace}. For example,\n * \u003cpre\u003e\n *   import NamespaceChange._\n *   val catalog = Catalogs.load(name)\n *   catalog.alterNamespace(ident,\n *       setProperty(\"prop\", \"value\"),\n *       removeProperty(\"other_prop\")\n *     )\n * \u003c/pre\u003e\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.NamespaceChange",
		"extends": "",
		"Methods": [
			{
				"signature": "static NamespaceChange setProperty(String property, String value)",
				"documentation": "/**\n   * Create a NamespaceChange for setting a namespace property.\n   * \u003cp\u003e\n   * If the property already exists, it will be replaced with the new value.\n   *\n   * @param property the property name\n   * @param value the new property value\n   * @return a NamespaceChange for the addition\n   */"
			},
			{
				"signature": "static NamespaceChange removeProperty(String property)",
				"documentation": "/**\n   * Create a NamespaceChange for removing a namespace property.\n   * \u003cp\u003e\n   * If the property does not exist, the change will succeed.\n   *\n   * @param property the property name\n   * @return a NamespaceChange for the addition\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.catalog.SetProperty",
			"org.apache.spark.sql.connector.catalog.RemoveProperty"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.sql.connector.catalog.SetProperty",
			"org.apache.spark.sql.connector.catalog.RemoveProperty"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.connector.catalog.SetProperty",
			"org.apache.spark.sql.connector.catalog.RemoveProperty"
		]
	},
	{
		"documentation": "/**\n   * A NamespaceChange to set a namespace property.\n   * \u003cp\u003e\n   * If the property already exists, it must be replaced with the new value.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.SetProperty",
		"extends": "",
		"Methods": [
			{
				"signature": "private SetProperty(String property, String value)",
				"documentation": "/**\n   * A NamespaceChange to set a namespace property.\n   * \u003cp\u003e\n   * If the property already exists, it must be replaced with the new value.\n   */"
			},
			{
				"signature": "public String property()",
				"documentation": ""
			},
			{
				"signature": "public String value()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.NamespaceChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.NamespaceChange",
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A NamespaceChange to remove a namespace property.\n   * \u003cp\u003e\n   * If the property does not exist, the change should succeed.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.RemoveProperty",
		"extends": "",
		"Methods": [
			{
				"signature": "private RemoveProperty(String property)",
				"documentation": "/**\n   * A NamespaceChange to remove a namespace property.\n   * \u003cp\u003e\n   * If the property does not exist, the change should succeed.\n   */"
			},
			{
				"signature": "public String property()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.NamespaceChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.NamespaceChange",
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link TableProvider}. Data sources can implement this interface to\n * propagate session configs with the specified key-prefix to all data source operations in this\n * session.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SessionConfigSupport",
		"extends": "org.apache.spark.sql.connector.catalog.TableProvider",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a table which is staged for being committed to the metastore.\n * \u003cp\u003e\n * This is used to implement atomic CREATE TABLE AS SELECT and REPLACE TABLE AS SELECT queries. The\n * planner will create one of these via\n * {@link StagingTableCatalog#stageCreate(Identifier, StructType, Transform[], Map)} or\n * {@link StagingTableCatalog#stageReplace(Identifier, StructType, Transform[], Map)} to prepare the\n * table for being written to. This table should usually implement {@link SupportsWrite}. A new\n * writer will be constructed via {@link SupportsWrite#newWriteBuilder(LogicalWriteInfo)}, and the\n * write will be committed. The job concludes with a call to {@link #commitStagedChanges()}, at\n * which point implementations are expected to commit the table's metadata into the metastore along\n * with the data that was written by the writes from the write builder this table created.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.StagedTable",
		"extends": "org.apache.spark.sql.connector.catalog.Table",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An optional mix-in for implementations of {@link TableCatalog} that support staging creation of\n * the a table before committing the table's metadata along with its contents in CREATE TABLE AS\n * SELECT or REPLACE TABLE AS SELECT operations.\n * \u003cp\u003e\n * It is highly recommended to implement this trait whenever possible so that CREATE TABLE AS\n * SELECT and REPLACE TABLE AS SELECT operations are atomic. For example, when one runs a REPLACE\n * TABLE AS SELECT operation, if the catalog does not implement this trait, the planner will first\n * drop the table via {@link TableCatalog#dropTable(Identifier)}, then create the table via\n * {@link TableCatalog#createTable(Identifier, StructType, Transform[], Map)}, and then perform\n * the write via {@link SupportsWrite#newWriteBuilder(LogicalWriteInfo)}.\n * However, if the write operation fails, the catalog will have already dropped the table, and the\n * planner cannot roll back the dropping of the table.\n * \u003cp\u003e\n * If the catalog implements this plugin, the catalog can implement the methods to \"stage\" the\n * creation and the replacement of a table. After the table's\n * {@link BatchWrite#commit(WriterCommitMessage[])} is called,\n * {@link StagedTable#commitStagedChanges()} is called, at which point the staged table can\n * complete both the data write and the metadata swap operation atomically.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.StagingTableCatalog",
		"extends": "org.apache.spark.sql.connector.catalog.TableCatalog",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An atomic partition interface of {@link Table} to operate multiple partitions atomically.\n * \u003cp\u003e\n * These APIs are used to modify table partition or partition metadata,\n * they will change the table data as well.\n * \u003cul\u003e\n *   \u003cli\u003e{@link #createPartitions}: add an array of partitions and any data they contain to the\n *   table\u003c/li\u003e\n *   \u003cli\u003e{@link #dropPartitions}: remove an array of partitions and any data they contain from\n *   the table\u003c/li\u003e\n *   \u003cli\u003e{@link #purgePartitions}: remove an array of partitions and any data they contain from\n *   the table by skipping a trash even if it is supported\u003c/li\u003e\n *   \u003cli\u003e{@link #truncatePartitions}: truncate an array of partitions by removing partitions\n *   data\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsAtomicPartitionManagement",
		"extends": "org.apache.spark.sql.connector.catalog.SupportsPartitionManagement",
		"Methods": [
			{
				"signature": "@Override\n  default void createPartition(\n      InternalRow ident,\n      Map\u003cString, String\u003e properties)\n      throws PartitionAlreadyExistsException, UnsupportedOperationException",
				"documentation": "/**\n * An atomic partition interface of {@link Table} to operate multiple partitions atomically.\n * \u003cp\u003e\n * These APIs are used to modify table partition or partition metadata,\n * they will change the table data as well.\n * \u003cul\u003e\n *   \u003cli\u003e{@link #createPartitions}: add an array of partitions and any data they contain to the\n *   table\u003c/li\u003e\n *   \u003cli\u003e{@link #dropPartitions}: remove an array of partitions and any data they contain from\n *   the table\u003c/li\u003e\n *   \u003cli\u003e{@link #purgePartitions}: remove an array of partitions and any data they contain from\n *   the table by skipping a trash even if it is supported\u003c/li\u003e\n *   \u003cli\u003e{@link #truncatePartitions}: truncate an array of partitions by removing partitions\n *   data\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * @since 3.1.0\n */"
			},
			{
				"signature": "@Override\n  default boolean dropPartition(InternalRow ident)",
				"documentation": ""
			},
			{
				"signature": "default boolean purgePartitions(InternalRow[] idents)\n    throws NoSuchPartitionException, UnsupportedOperationException",
				"documentation": "/**\n   * Drop an array of partitions atomically from table, and completely remove partitions data\n   * by skipping a trash even if it is supported.\n   * \u003cp\u003e\n   * If any partition doesn't exists,\n   * the operation of purgePartitions need to be safely rolled back.\n   *\n   * @param idents an array of partition identifiers\n   * @return true if partitions were deleted, false if any partition not exists\n   * @throws NoSuchPartitionException If any partition identifier to alter doesn't exist\n   * @throws UnsupportedOperationException If partition purging is not supported\n   *\n   * @since 3.2.0\n   */"
			},
			{
				"signature": "default boolean truncatePartitions(InternalRow[] idents)\n      throws NoSuchPartitionException, UnsupportedOperationException",
				"documentation": "/**\n   * Truncate an array of partitions atomically from table, and completely remove partitions data.\n   * \u003cp\u003e\n   * If any partition doesn't exists,\n   * the operation of truncatePartitions need to be safely rolled back.\n   *\n   * @param idents an array of partition identifiers\n   * @return true if partitions were truncated successfully otherwise false\n   * @throws NoSuchPartitionException If any partition identifier to truncate doesn't exist\n   * @throws UnsupportedOperationException If partition truncate is not supported\n   *\n   * @since 3.2.0\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface, which TableProviders can implement, to support table existence checks and creation\n * through a catalog, without having to use table identifiers. For example, when file based data\n * sources use the `DataFrameWriter.save(path)` method, the option `path` can translate to a\n * PathIdentifier. A catalog can then use this PathIdentifier to check the existence of a table, or\n * whether a table can be created at a given directory.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsCatalogOptions",
		"extends": "org.apache.spark.sql.connector.catalog.TableProvider",
		"Methods": [
			{
				"signature": "default String extractCatalog(CaseInsensitiveStringMap options)",
				"documentation": "/**\n   * Return the name of a catalog that can be used to check the existence of, load, and create\n   * a table for this DataSource given the identifier that will be extracted by\n   * {@link #extractIdentifier(CaseInsensitiveStringMap) extractIdentifier}. A `null` value can\n   * be used to defer to the V2SessionCatalog.\n   *\n   * @param options the user-specified options that can identify a table, e.g. file path, Kafka\n   *                topic name, etc. It's an immutable case-insensitive string-to-string map.\n   */"
			},
			{
				"signature": "default Optional\u003cString\u003e extractTimeTravelTimestamp(CaseInsensitiveStringMap options)",
				"documentation": "/**\n   * Extracts the timestamp string for time travel from the given options.\n   */"
			},
			{
				"signature": "default Optional\u003cString\u003e extractTimeTravelVersion(CaseInsensitiveStringMap options)",
				"documentation": "/**\n   * Extracts the version string for time travel from the given options.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link Table} delete support. Data sources can implement this\n * interface to provide the ability to delete data from tables that matches filter expressions.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsDelete",
		"extends": "org.apache.spark.sql.connector.catalog.TruncatableTable",
		"Methods": [
			{
				"signature": "default boolean canDeleteWhere(Filter[] filters)",
				"documentation": "/**\n   * Checks whether it is possible to delete data from a data source table that matches filter\n   * expressions.\n   * \u003cp\u003e\n   * Rows should be deleted from the data source iff all of the filter expressions match.\n   * That is, the expressions must be interpreted as a set of filters that are ANDed together.\n   * \u003cp\u003e\n   * Spark will call this method at planning time to check whether {@link #deleteWhere(Filter[])}\n   * would reject the delete operation because it requires significant effort. If this method\n   * returns false, Spark will not call {@link #deleteWhere(Filter[])} and will try to rewrite\n   * the delete operation and produce row-level changes if the data source table supports deleting\n   * individual records.\n   *\n   * @param filters filter expressions, used to select rows to delete when all expressions match\n   * @return true if the delete operation can be performed\n   *\n   * @since 3.1.0\n   */"
			},
			{
				"signature": "@Override\n  default boolean truncateTable()",
				"documentation": "/**\n   * Delete data from a data source table that matches filter expressions. Note that this method\n   * will be invoked only if {@link #canDeleteWhere(Filter[])} returns true.\n   * \u003cp\u003e\n   * Rows are deleted from the data source iff all of the filter expressions match. That is, the\n   * expressions must be interpreted as a set of filters that are ANDed together.\n   * \u003cp\u003e\n   * Implementations may reject a delete operation if the delete isn't possible without significant\n   * effort. For example, partitioned data sources may reject deletes that do not filter by\n   * partition columns because the filter may require rewriting files without deleted records.\n   * To reject a delete implementations should throw {@link IllegalArgumentException} with a clear\n   * error message that identifies which expression was rejected.\n   *\n   * @param filters filter expressions, used to select rows to delete when all expressions match\n   * @throws IllegalArgumentException If the delete is rejected due to required effort\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface for exposing data columns for a table that are not in the table schema. For example,\n * a file source could expose a \"file\" column that contains the path of the file that contained each\n * row.\n * \u003cp\u003e\n * The columns returned by {@link #metadataColumns()} may be passed as {@link StructField} in\n * requested projections. Sources that implement this interface and column projection using\n * {@link SupportsPushDownRequiredColumns} must accept metadata fields passed to\n * {@link SupportsPushDownRequiredColumns#pruneColumns(StructType)}.\n * \u003cp\u003e\n * If a table column and a metadata column have the same name, the metadata column will never be\n * requested. It is recommended that Table implementations reject data column name that conflict\n * with metadata column names.\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsMetadataColumns",
		"extends": "org.apache.spark.sql.connector.catalog.Table",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Catalog methods for working with namespaces.\n * \u003cp\u003e\n * If an object such as a table, view, or function exists, its parent namespaces must also exist\n * and must be returned by the discovery methods {@link #listNamespaces()} and\n * {@link #listNamespaces(String[])}.\n * \u003cp\u003e\n * Catalog implementations are not required to maintain the existence of namespaces independent of\n * objects in a namespace. For example, a function catalog that loads functions using reflection\n * and uses Java packages as namespaces is not required to support the methods to create, alter, or\n * drop a namespace. Implementations are allowed to discover the existence of objects or namespaces\n * without throwing {@link NoSuchNamespaceException} when no namespace is found.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsNamespaces",
		"extends": "org.apache.spark.sql.connector.catalog.CatalogPlugin",
		"Methods": [
			{
				"signature": "default boolean namespaceExists(String[] namespace)",
				"documentation": "/**\n   * Test whether a namespace exists.\n   * \u003cp\u003e\n   * If an object such as a table, view, or function exists, its parent namespaces must also exist.\n   * For example, if table a.b.t exists, this method invoked as namespaceExists([\"a\"]) or\n   * namespaceExists([\"a\", \"b\"]) must return true.\n   *\n   * @param namespace a multi-part namespace\n   * @return true if the namespace exists, false otherwise\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A partition interface of {@link Table}.\n * A partition is composed of identifier and properties,\n * and properties contains metadata information of the partition.\n * \u003cp\u003e\n * These APIs are used to modify table partition identifier or partition metadata.\n * In some cases, they will change the table data as well.\n * \u003cul\u003e\n *   \u003cli\u003e{@link #createPartition}: add a partition and any data it contains to the table\u003c/li\u003e\n *   \u003cli\u003e{@link #dropPartition}: remove a partition and any data it contains from the table\u003c/li\u003e\n *   \u003cli\u003e{@link #purgePartition}: remove a partition and any data it contains from the table by\n *   skipping a trash even if it is supported.\u003c/li\u003e\n *   \u003cli\u003e{@link #replacePartitionMetadata}: point a partition to a new location, which will swap\n *   one location's data for the other\u003c/li\u003e\n *   \u003cli\u003e{@link #truncatePartition}: remove partition data from the table\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * @since 3.1.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsPartitionManagement",
		"extends": "org.apache.spark.sql.connector.catalog.Table",
		"Methods": [
			{
				"signature": "default boolean purgePartition(InternalRow ident)\n      throws NoSuchPartitionException, UnsupportedOperationException",
				"documentation": "/**\n     * Drop a partition from the table and completely remove partition data by skipping a trash\n     * even if it is supported.\n     *\n     * @param ident a partition identifier\n     * @return true if a partition was deleted, false if no partition exists for the identifier\n     * @throws NoSuchPartitionException If the partition identifier to alter doesn't exist\n     * @throws UnsupportedOperationException If partition purging is not supported\n     *\n     * @since 3.2.0\n     */"
			},
			{
				"signature": "default boolean partitionExists(InternalRow ident)",
				"documentation": "/**\n     * Test whether a partition exists using an {@link InternalRow ident} from the table.\n     *\n     * @param ident a partition identifier which must contain all partition fields in order\n     * @return true if the partition exists, false otherwise\n     */"
			},
			{
				"signature": "default boolean renamePartition(InternalRow from, InternalRow to)\n        throws UnsupportedOperationException,\n               PartitionAlreadyExistsException,\n               NoSuchPartitionException",
				"documentation": "/**\n     * Rename an existing partition of the table.\n     *\n     * @param from an existing partition identifier to rename\n     * @param to new partition identifier\n     * @return true if renaming completes successfully otherwise false\n     * @throws UnsupportedOperationException If partition renaming is not supported\n     * @throws PartitionAlreadyExistsException If the `to` partition exists already\n     * @throws NoSuchPartitionException If the `from` partition does not exist\n     *\n     * @since 3.2.0\n     */"
			},
			{
				"signature": "default boolean truncatePartition(InternalRow ident)\n        throws NoSuchPartitionException, UnsupportedOperationException",
				"documentation": "/**\n     * Truncate a partition in the table by completely removing partition data.\n     *\n     * @param ident a partition identifier\n     * @return true if the partition was truncated successfully otherwise false\n     * @throws NoSuchPartitionException If the partition identifier to alter doesn't exist\n     * @throws UnsupportedOperationException If partition truncation is not supported\n     *\n     * @since 3.2.0\n     */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.SupportsAtomicPartitionManagement"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface of {@link Table}, to indicate that it's readable. This adds\n * {@link #newScanBuilder(CaseInsensitiveStringMap)} that is used to create a scan for batch,\n * micro-batch, or continuous processing.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsRead",
		"extends": "org.apache.spark.sql.connector.catalog.Table",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link Table} row-level operations support. Data sources can implement\n * this interface to indicate they support rewriting data for DELETE, UPDATE, MERGE operations.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsRowLevelOperations",
		"extends": "org.apache.spark.sql.connector.catalog.Table",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface of {@link Table}, to indicate that it's writable. This adds\n * {@link #newWriteBuilder(LogicalWriteInfo)} that is used to create a\n * write for batch or streaming.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.SupportsWrite",
		"extends": "org.apache.spark.sql.connector.catalog.Table",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface representing a logical structured data set of a data source. For example, the\n * implementation can be a directory on the file system, a topic of Kafka, or a table in the\n * catalog, etc.\n * \u003cp\u003e\n * This interface can mixin {@code SupportsRead} and {@code SupportsWrite} to provide data reading\n * and writing ability.\n * \u003cp\u003e\n * The default implementation of {@link #partitioning()} returns an empty array of partitions, and\n * the default implementation of {@link #properties()} returns an empty map. These should be\n * overridden by implementations that support partitioning and table properties.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.Table",
		"extends": "",
		"Methods": [
			{
				"signature": "default Transform[] partitioning()",
				"documentation": "/**\n   * Returns the physical partitioning of this table.\n   */"
			},
			{
				"signature": "default Map\u003cString, String\u003e properties()",
				"documentation": "/**\n   * Returns the string map of table properties.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.StagedTable",
			"org.apache.spark.sql.connector.catalog.SupportsMetadataColumns",
			"org.apache.spark.sql.connector.catalog.SupportsPartitionManagement",
			"org.apache.spark.sql.connector.catalog.SupportsRead",
			"org.apache.spark.sql.connector.catalog.SupportsRowLevelOperations",
			"org.apache.spark.sql.connector.catalog.SupportsWrite",
			"org.apache.spark.sql.connector.catalog.TruncatableTable"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Capabilities that can be provided by a {@link Table} implementation.\n * \u003cp\u003e\n * Tables use {@link Table#capabilities()} to return a set of capabilities. Each capability signals\n * to Spark that the table supports a feature identified by the capability. For example, returning\n * {@link #BATCH_READ} allows Spark to read from the table using a batch scan.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.TableCapability",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Catalog methods for working with Tables.\n * \u003cp\u003e\n * TableCatalog implementations may be case sensitive or case insensitive. Spark will pass\n * {@link Identifier table identifiers} without modification. Field names passed to\n * {@link #alterTable(Identifier, TableChange...)} will be normalized to match the case used in the\n * table schema when updating, renaming, or dropping existing columns when catalyst analysis is case\n * insensitive.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.TableCatalog",
		"extends": "org.apache.spark.sql.connector.catalog.CatalogPlugin",
		"Methods": [
			{
				"signature": "default Table loadTable(Identifier ident, String version) throws NoSuchTableException",
				"documentation": "/**\n   * Load table metadata of a specific version by {@link Identifier identifier} from the catalog.\n   * \u003cp\u003e\n   * If the catalog supports views and contains a view for the identifier and not a table, this\n   * must throw {@link NoSuchTableException}.\n   *\n   * @param ident a table identifier\n   * @param version version of the table\n   * @return the table's metadata\n   * @throws NoSuchTableException If the table doesn't exist or is a view\n   */"
			},
			{
				"signature": "default Table loadTable(Identifier ident, long timestamp) throws NoSuchTableException",
				"documentation": "/**\n   * Load table metadata at a specific time by {@link Identifier identifier} from the catalog.\n   * \u003cp\u003e\n   * If the catalog supports views and contains a view for the identifier and not a table, this\n   * must throw {@link NoSuchTableException}.\n   *\n   * @param ident a table identifier\n   * @param timestamp timestamp of the table, which is microseconds since 1970-01-01 00:00:00 UTC\n   * @return the table's metadata\n   * @throws NoSuchTableException If the table doesn't exist or is a view\n   */"
			},
			{
				"signature": "default void invalidateTable(Identifier ident)",
				"documentation": "/**\n   * Invalidate cached table metadata for an {@link Identifier identifier}.\n   * \u003cp\u003e\n   * If the table is already loaded or cached, drop cached data. If the table does not exist or is\n   * not cached, do nothing. Calling this method should not query remote services.\n   *\n   * @param ident a table identifier\n   */"
			},
			{
				"signature": "default boolean tableExists(Identifier ident)",
				"documentation": "/**\n   * Test whether a table exists using an {@link Identifier identifier} from the catalog.\n   * \u003cp\u003e\n   * If the catalog supports views and contains a view for the identifier and not a table, this\n   * must return false.\n   *\n   * @param ident a table identifier\n   * @return true if the table exists, false otherwise\n   */"
			},
			{
				"signature": "default boolean purgeTable(Identifier ident) throws UnsupportedOperationException",
				"documentation": "/**\n   * Drop a table in the catalog and completely remove its data by skipping a trash even if it is\n   * supported.\n   * \u003cp\u003e\n   * If the catalog supports views and contains a view for the identifier and not a table, this\n   * must not drop the view and must return false.\n   * \u003cp\u003e\n   * If the catalog supports to purge a table, this method should be overridden.\n   * The default implementation throws {@link UnsupportedOperationException}.\n   *\n   * @param ident a table identifier\n   * @return true if a table was deleted, false if no table exists for the identifier\n   * @throws UnsupportedOperationException If table purging is not supported\n   *\n   * @since 3.1.0\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.StagingTableCatalog"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * TableChange subclasses represent requested changes to a table. These are passed to\n * {@link TableCatalog#alterTable}. For example,\n * \u003cpre\u003e\n *   import TableChange._\n *   val catalog = Catalogs.load(name)\n *   catalog.asTableCatalog.alterTable(ident,\n *       addColumn(\"x\", IntegerType),\n *       renameColumn(\"a\", \"b\"),\n *       deleteColumn(\"c\")\n *     )\n * \u003c/pre\u003e\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.TableChange",
		"extends": "",
		"Methods": [
			{
				"signature": "static TableChange setProperty(String property, String value)",
				"documentation": "/**\n   * Create a TableChange for setting a table property.\n   * \u003cp\u003e\n   * If the property already exists, it will be replaced with the new value.\n   *\n   * @param property the property name\n   * @param value the new property value\n   * @return a TableChange for the addition\n   */"
			},
			{
				"signature": "static TableChange removeProperty(String property)",
				"documentation": "/**\n   * Create a TableChange for removing a table property.\n   * \u003cp\u003e\n   * If the property does not exist, the change will succeed.\n   *\n   * @param property the property name\n   * @return a TableChange for the addition\n   */"
			},
			{
				"signature": "static TableChange addColumn(String[] fieldNames, DataType dataType)",
				"documentation": "/**\n   * Create a TableChange for adding an optional column.\n   * \u003cp\u003e\n   * If the field already exists, the change will result in an {@link IllegalArgumentException}.\n   * If the new field is nested and its parent does not exist or is not a struct, the change will\n   * result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the new column\n   * @param dataType the new column's data type\n   * @return a TableChange for the addition\n   */"
			},
			{
				"signature": "static TableChange addColumn(String[] fieldNames, DataType dataType, boolean isNullable)",
				"documentation": "/**\n   * Create a TableChange for adding a column.\n   * \u003cp\u003e\n   * If the field already exists, the change will result in an {@link IllegalArgumentException}.\n   * If the new field is nested and its parent does not exist or is not a struct, the change will\n   * result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the new column\n   * @param dataType the new column's data type\n   * @param isNullable whether the new column can contain null\n   * @return a TableChange for the addition\n   */"
			},
			{
				"signature": "static TableChange addColumn(\n      String[] fieldNames,\n      DataType dataType,\n      boolean isNullable,\n      String comment)",
				"documentation": "/**\n   * Create a TableChange for adding a column.\n   * \u003cp\u003e\n   * If the field already exists, the change will result in an {@link IllegalArgumentException}.\n   * If the new field is nested and its parent does not exist or is not a struct, the change will\n   * result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the new column\n   * @param dataType the new column's data type\n   * @param isNullable whether the new column can contain null\n   * @param comment the new field's comment string\n   * @return a TableChange for the addition\n   */"
			},
			{
				"signature": "static TableChange addColumn(\n      String[] fieldNames,\n      DataType dataType,\n      boolean isNullable,\n      String comment,\n      ColumnPosition position)",
				"documentation": "/**\n   * Create a TableChange for adding a column.\n   * \u003cp\u003e\n   * If the field already exists, the change will result in an {@link IllegalArgumentException}.\n   * If the new field is nested and its parent does not exist or is not a struct, the change will\n   * result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the new column\n   * @param dataType the new column's data type\n   * @param isNullable whether the new column can contain null\n   * @param comment the new field's comment string\n   * @param position the new columns's position\n   * @return a TableChange for the addition\n   */"
			},
			{
				"signature": "static TableChange renameColumn(String[] fieldNames, String newName)",
				"documentation": "/**\n   * Create a TableChange for renaming a field.\n   * \u003cp\u003e\n   * The name is used to find the field to rename. The new name will replace the leaf field name.\n   * For example, renameColumn([\"a\", \"b\", \"c\"], \"x\") should produce column a.b.x.\n   * \u003cp\u003e\n   * If the field does not exist, the change will result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames the current field names\n   * @param newName the new name\n   * @return a TableChange for the rename\n   */"
			},
			{
				"signature": "static TableChange updateColumnType(String[] fieldNames, DataType newDataType)",
				"documentation": "/**\n   * Create a TableChange for updating the type of a field that is nullable.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change will result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the column to update\n   * @param newDataType the new data type\n   * @return a TableChange for the update\n   */"
			},
			{
				"signature": "static TableChange updateColumnNullability(String[] fieldNames, boolean nullable)",
				"documentation": "/**\n   * Create a TableChange for updating the nullability of a field.\n   * \u003cp\u003e\n   * The name is used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change will result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the column to update\n   * @param nullable the nullability\n   * @return a TableChange for the update\n   */"
			},
			{
				"signature": "static TableChange updateColumnComment(String[] fieldNames, String newComment)",
				"documentation": "/**\n   * Create a TableChange for updating the comment of a field.\n   * \u003cp\u003e\n   * The name is used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change will result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the column to update\n   * @param newComment the new comment\n   * @return a TableChange for the update\n   */"
			},
			{
				"signature": "static TableChange updateColumnPosition(String[] fieldNames, ColumnPosition newPosition)",
				"documentation": "/**\n   * Create a TableChange for updating the position of a field.\n   * \u003cp\u003e\n   * The name is used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change will result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the column to update\n   * @param newPosition the new position\n   * @return a TableChange for the update\n   */"
			},
			{
				"signature": "static TableChange deleteColumn(String[] fieldNames, Boolean ifExists)",
				"documentation": "/**\n   * Create a TableChange for deleting a field.\n   * \u003cp\u003e\n   * If the field does not exist, the change will result in an {@link IllegalArgumentException}.\n   *\n   * @param fieldNames field names of the column to delete\n   * @param ifExists   silence the error if column doesn't exist during drop\n   * @return a TableChange for the delete\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.ColumnChange"
		],
		"implementedBy": [
			"org.apache.spark.sql.connector.catalog.SetProperty",
			"org.apache.spark.sql.connector.catalog.RemoveProperty"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.sql.connector.catalog.SetProperty",
			"org.apache.spark.sql.connector.catalog.RemoveProperty",
			"org.apache.spark.sql.connector.catalog.After",
			"org.apache.spark.sql.connector.catalog.AddColumn",
			"org.apache.spark.sql.connector.catalog.RenameColumn",
			"org.apache.spark.sql.connector.catalog.UpdateColumnType",
			"org.apache.spark.sql.connector.catalog.UpdateColumnNullability",
			"org.apache.spark.sql.connector.catalog.UpdateColumnComment",
			"org.apache.spark.sql.connector.catalog.UpdateColumnPosition",
			"org.apache.spark.sql.connector.catalog.DeleteColumn"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.connector.catalog.SetProperty",
			"org.apache.spark.sql.connector.catalog.RemoveProperty",
			"org.apache.spark.sql.connector.catalog.ColumnPosition",
			"org.apache.spark.sql.connector.catalog.First",
			"org.apache.spark.sql.connector.catalog.After",
			"org.apache.spark.sql.connector.catalog.ColumnChange",
			"org.apache.spark.sql.connector.catalog.AddColumn",
			"org.apache.spark.sql.connector.catalog.RenameColumn",
			"org.apache.spark.sql.connector.catalog.UpdateColumnType",
			"org.apache.spark.sql.connector.catalog.UpdateColumnNullability",
			"org.apache.spark.sql.connector.catalog.UpdateColumnComment",
			"org.apache.spark.sql.connector.catalog.UpdateColumnPosition",
			"org.apache.spark.sql.connector.catalog.DeleteColumn"
		]
	},
	{
		"documentation": "/**\n   * A TableChange to set a table property.\n   * \u003cp\u003e\n   * If the property already exists, it must be replaced with the new value.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.SetProperty",
		"extends": "",
		"Methods": [
			{
				"signature": "private SetProperty(String property, String value)",
				"documentation": "/**\n   * A TableChange to set a table property.\n   * \u003cp\u003e\n   * If the property already exists, it must be replaced with the new value.\n   */"
			},
			{
				"signature": "public String property()",
				"documentation": ""
			},
			{
				"signature": "public String value()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.NamespaceChange",
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A TableChange to remove a table property.\n   * \u003cp\u003e\n   * If the property does not exist, the change should succeed.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.RemoveProperty",
		"extends": "",
		"Methods": [
			{
				"signature": "private RemoveProperty(String property)",
				"documentation": "/**\n   * A TableChange to remove a table property.\n   * \u003cp\u003e\n   * If the property does not exist, the change should succeed.\n   */"
			},
			{
				"signature": "public String property()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.NamespaceChange",
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.connector.catalog.ColumnPosition",
		"extends": "",
		"Methods": [
			{
				"signature": "static ColumnPosition first()",
				"documentation": ""
			},
			{
				"signature": "static ColumnPosition after(String column)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.catalog.First",
			"org.apache.spark.sql.connector.catalog.After"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Column position FIRST means the specified column should be the first column.\n   * Note that, the specified column may be a nested field, and then FIRST means this field should\n   * be the first one within the struct.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.First",
		"extends": "",
		"Methods": [
			{
				"signature": "private First()",
				"documentation": "/**\n   * Column position FIRST means the specified column should be the first column.\n   * Note that, the specified column may be a nested field, and then FIRST means this field should\n   * be the first one within the struct.\n   */"
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnPosition"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Column position AFTER means the specified column should be put after the given `column`.\n   * Note that, the specified column may be a nested field, and then the given `column` refers to\n   * a field in the same struct.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.After",
		"extends": "",
		"Methods": [
			{
				"signature": "private After(String column)",
				"documentation": "/**\n   * Column position AFTER means the specified column should be put after the given `column`.\n   * Note that, the specified column may be a nested field, and then the given `column` refers to\n   * a field in the same struct.\n   */"
			},
			{
				"signature": "public String column()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnPosition"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.connector.catalog.ColumnChange",
		"extends": "org.apache.spark.sql.connector.catalog.TableChange",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.catalog.AddColumn",
			"org.apache.spark.sql.connector.catalog.RenameColumn",
			"org.apache.spark.sql.connector.catalog.UpdateColumnType",
			"org.apache.spark.sql.connector.catalog.UpdateColumnNullability",
			"org.apache.spark.sql.connector.catalog.UpdateColumnComment",
			"org.apache.spark.sql.connector.catalog.UpdateColumnPosition",
			"org.apache.spark.sql.connector.catalog.DeleteColumn"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A TableChange to add a field.\n   * \u003cp\u003e\n   * If the field already exists, the change must result in an {@link IllegalArgumentException}.\n   * If the new field is nested and its parent does not exist or is not a struct, the change must\n   * result in an {@link IllegalArgumentException}.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.AddColumn",
		"extends": "",
		"Methods": [
			{
				"signature": "private AddColumn(\n        String[] fieldNames,\n        DataType dataType,\n        boolean isNullable,\n        String comment,\n        ColumnPosition position)",
				"documentation": "/**\n   * A TableChange to add a field.\n   * \u003cp\u003e\n   * If the field already exists, the change must result in an {@link IllegalArgumentException}.\n   * If the new field is nested and its parent does not exist or is not a struct, the change must\n   * result in an {@link IllegalArgumentException}.\n   */"
			},
			{
				"signature": "@Override\n    public String[] fieldNames()",
				"documentation": ""
			},
			{
				"signature": "public DataType dataType()",
				"documentation": ""
			},
			{
				"signature": "public boolean isNullable()",
				"documentation": ""
			},
			{
				"signature": "@Nullable\n    public String comment()",
				"documentation": ""
			},
			{
				"signature": "@Nullable\n    public ColumnPosition position()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A TableChange to rename a field.\n   * \u003cp\u003e\n   * The name is used to find the field to rename. The new name will replace the leaf field name.\n   * For example, renameColumn(\"a.b.c\", \"x\") should produce column a.b.x.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.RenameColumn",
		"extends": "",
		"Methods": [
			{
				"signature": "private RenameColumn(String[] fieldNames, String newName)",
				"documentation": "/**\n   * A TableChange to rename a field.\n   * \u003cp\u003e\n   * The name is used to find the field to rename. The new name will replace the leaf field name.\n   * For example, renameColumn(\"a.b.c\", \"x\") should produce column a.b.x.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */"
			},
			{
				"signature": "@Override\n    public String[] fieldNames()",
				"documentation": ""
			},
			{
				"signature": "public String newName()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A TableChange to update the type of a field.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.UpdateColumnType",
		"extends": "",
		"Methods": [
			{
				"signature": "private UpdateColumnType(String[] fieldNames, DataType newDataType)",
				"documentation": "/**\n   * A TableChange to update the type of a field.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */"
			},
			{
				"signature": "@Override\n    public String[] fieldNames()",
				"documentation": ""
			},
			{
				"signature": "public DataType newDataType()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A TableChange to update the nullability of a field.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.UpdateColumnNullability",
		"extends": "",
		"Methods": [
			{
				"signature": "private UpdateColumnNullability(String[] fieldNames, boolean nullable)",
				"documentation": "/**\n   * A TableChange to update the nullability of a field.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */"
			},
			{
				"signature": "public String[] fieldNames()",
				"documentation": ""
			},
			{
				"signature": "public boolean nullable()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A TableChange to update the comment of a field.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.UpdateColumnComment",
		"extends": "",
		"Methods": [
			{
				"signature": "private UpdateColumnComment(String[] fieldNames, String newComment)",
				"documentation": "/**\n   * A TableChange to update the comment of a field.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */"
			},
			{
				"signature": "@Override\n    public String[] fieldNames()",
				"documentation": ""
			},
			{
				"signature": "public String newComment()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A TableChange to update the position of a field.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.UpdateColumnPosition",
		"extends": "",
		"Methods": [
			{
				"signature": "private UpdateColumnPosition(String[] fieldNames, ColumnPosition position)",
				"documentation": "/**\n   * A TableChange to update the position of a field.\n   * \u003cp\u003e\n   * The field names are used to find the field to update.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */"
			},
			{
				"signature": "@Override\n    public String[] fieldNames()",
				"documentation": ""
			},
			{
				"signature": "public ColumnPosition position()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * A TableChange to delete a field.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */",
		"name": "org.apache.spark.sql.connector.catalog.DeleteColumn",
		"extends": "",
		"Methods": [
			{
				"signature": "private DeleteColumn(String[] fieldNames, Boolean ifExists)",
				"documentation": "/**\n   * A TableChange to delete a field.\n   * \u003cp\u003e\n   * If the field does not exist, the change must result in an {@link IllegalArgumentException}.\n   */"
			},
			{
				"signature": "@Override\n    public String[] fieldNames()",
				"documentation": ""
			},
			{
				"signature": "public Boolean ifExists()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.ColumnChange"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.catalog.TableChange"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The base interface for v2 data sources which don't have a real catalog. Implementations must\n * have a public, 0-arg constructor.\n * \u003cp\u003e\n * Note that, TableProvider can only apply data operations to existing tables, like read, append,\n * delete, and overwrite. It does not support the operations that require metadata changes, like\n * create/drop tables.\n * \u003cp\u003e\n * The major responsibility of this interface is to return a {@link Table} for read/write.\n * \u003c/p\u003e\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.TableProvider",
		"extends": "",
		"Methods": [
			{
				"signature": "default Transform[] inferPartitioning(CaseInsensitiveStringMap options)",
				"documentation": "/**\n   * Infer the partitioning of the table identified by the given options.\n   * \u003cp\u003e\n   * By default this method returns empty partitioning, please override it if this source support\n   * partitioning.\n   *\n   * @param options an immutable case-insensitive string-to-string map that can identify a table,\n   *                e.g. file path, Kafka topic name, etc.\n   */"
			},
			{
				"signature": "default boolean supportsExternalMetadata()",
				"documentation": "/**\n   * Returns true if the source has the ability of accepting external table metadata when getting\n   * tables. The external table metadata includes:\n   * \u003col\u003e\n   *   \u003cli\u003eFor table reader: user-specified schema from {@code DataFrameReader}/{@code\n   *   DataStreamReader} and schema/partitioning stored in Spark catalog.\u003c/li\u003e\n   *   \u003cli\u003eFor table writer: the schema of the input {@code Dataframe} of\n   *   {@code DataframeWriter}/{@code DataStreamWriter}.\u003c/li\u003e\n   * \u003c/ol\u003e\n   * \u003cp\u003e\n   * By default this method returns false, which means the schema and partitioning passed to\n   * {@link #getTable(StructType, Transform[], Map)} are from the infer methods. Please override it\n   * if this source has expensive schema/partitioning inference and wants external table metadata\n   * to avoid inference.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.SessionConfigSupport",
			"org.apache.spark.sql.connector.catalog.SupportsCatalogOptions"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a table which can be atomically truncated.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.TruncatableTable",
		"extends": "org.apache.spark.sql.connector.catalog.Table",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.SupportsDelete"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface for a function that produces a result value by aggregating over multiple input rows.\n * \u003cp\u003e\n * For each input row, Spark will call the {@link #update} method which should evaluate the row\n * and update the aggregation state. The JVM type of result values produced by\n * {@link #produceResult} must be the type used by Spark's\n * InternalRow API for the {@link DataType SQL data type} returned by {@link #resultType()}.\n * Please refer to class documentation of {@link ScalarFunction} for the mapping between\n * {@link DataType} and the JVM type.\n * \u003cp\u003e\n * All implementations must support partial aggregation by implementing merge so that Spark can\n * partially aggregate and shuffle intermediate results, instead of shuffling all rows for an\n * aggregate. This reduces the impact of data skew and the amount of data shuffled to produce the\n * result.\n * \u003cp\u003e\n * Intermediate aggregation state must be {@link Serializable} so that state produced by parallel\n * tasks can be serialized, shuffled, and then merged to produce a final result.\n *\n * @param \u003cS\u003e the JVM type for the aggregation's intermediate state; must be {@link Serializable}\n * @param \u003cR\u003e the JVM type of result values\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.functions.AggregateFunction",
		"extends": "org.apache.spark.sql.connector.catalog.functions.BoundFunction",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a function that is bound to an input type.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.functions.BoundFunction",
		"extends": "org.apache.spark.sql.connector.catalog.functions.Function",
		"Methods": [
			{
				"signature": "default boolean isResultNullable()",
				"documentation": "/**\n   * Returns whether the values produced by this function may be null.\n   * \u003cp\u003e\n   * For example, a \"plus\" function may return false when it is bound to arguments that are always\n   * non-null, but true when either argument may be null.\n   *\n   * @return true if values produced by this function may be null, false otherwise\n   */"
			},
			{
				"signature": "default boolean isDeterministic()",
				"documentation": "/**\n   * Returns whether this function result is deterministic.\n   * \u003cp\u003e\n   * By default, functions are assumed to be deterministic. Functions that are not deterministic\n   * should override this method so that Spark can ensure the function runs only once for a given\n   * input.\n   *\n   * @return true if this function is deterministic, false otherwise\n   */"
			},
			{
				"signature": "default String canonicalName()",
				"documentation": "/**\n   * Returns the canonical name of this function, used to determine if functions are equivalent.\n   * \u003cp\u003e\n   * The canonical name is used to determine whether two functions are the same when loaded by\n   * different catalogs. For example, the same catalog implementation may be used for by two\n   * environments, \"prod\" and \"test\". Functions produced by the catalogs may be equivalent, but\n   * loaded using different names, like \"test.func_name\" and \"prod.func_name\".\n   * \u003cp\u003e\n   * Names returned by this function should be unique and unlikely to conflict with similar\n   * functions in other catalogs. For example, many catalogs may define a \"bucket\" function with a\n   * different implementation. Adding context, like \"com.mycompany.bucket(string)\", is recommended\n   * to avoid unintentional collisions.\n   *\n   * @return a canonical name for this function\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.functions.AggregateFunction",
			"org.apache.spark.sql.connector.catalog.functions.ScalarFunction"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base class for user-defined functions.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.functions.Function",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.catalog.functions.BoundFunction",
			"org.apache.spark.sql.connector.catalog.functions.UnboundFunction"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface for a function that produces a result value for each input row.\n * \u003cp\u003e\n * To evaluate each input row, Spark will first try to lookup and use a \"magic method\" (described\n * below) through Java reflection. If the method is not found, Spark will call\n * {@link #produceResult(InternalRow)} as a fallback approach.\n * \u003cp\u003e\n * The JVM type of result values produced by this function must be the type used by Spark's\n * InternalRow API for the {@link DataType SQL data type} returned by {@link #resultType()}.\n * The mapping between {@link DataType} and the corresponding JVM type is defined below.\n * \u003cp\u003e\n * \u003ch2\u003e Magic method \u003c/h2\u003e\n * \u003cb\u003eIMPORTANT\u003c/b\u003e: the default implementation of {@link #produceResult} throws\n * {@link UnsupportedOperationException}. Users must choose to either override this method, or\n * implement a magic method with name {@link #MAGIC_METHOD_NAME}, which takes individual parameters\n * instead of a {@link InternalRow}. The magic method approach is generally recommended because it\n * provides better performance over the default {@link #produceResult}, due to optimizations such\n * as whole-stage codegen, elimination of Java boxing, etc.\n * \u003cp\u003e\n * The type parameters for the magic method \u003cb\u003emust match\u003c/b\u003e those returned from\n * {@link BoundFunction#inputTypes()}. Otherwise Spark will not be able to find the magic method.\n * \u003cp\u003e\n * In addition, for stateless Java functions, users can optionally define the\n * {@link #MAGIC_METHOD_NAME} as a static method, which further avoids certain runtime costs such\n * as Java dynamic dispatch.\n * \u003cp\u003e\n * For example, a scalar UDF for adding two integers can be defined as follow with the magic\n * method approach:\n *\n * \u003cpre\u003e\n *   public class IntegerAdd implements{@code ScalarFunction\u003cInteger\u003e} {\n *     public DataType[] inputTypes() {\n *       return new DataType[] { DataTypes.IntegerType, DataTypes.IntegerType };\n *     }\n *     public int invoke(int left, int right) {\n *       return left + right;\n *     }\n *   }\n * \u003c/pre\u003e\n * In the above, since {@link #MAGIC_METHOD_NAME} is defined, and also that it has\n * matching parameter types and return type, Spark will use it to evaluate inputs.\n * \u003cp\u003e\n * As another example, in the following:\n * \u003cpre\u003e\n *   public class IntegerAdd implements{@code ScalarFunction\u003cInteger\u003e} {\n *     public DataType[] inputTypes() {\n *       return new DataType[] { DataTypes.IntegerType, DataTypes.IntegerType };\n *     }\n *     public static int invoke(int left, int right) {\n *       return left + right;\n *     }\n *     public Integer produceResult(InternalRow input) {\n *       return input.getInt(0) + input.getInt(1);\n *     }\n *   }\n * \u003c/pre\u003e\n *\n * the class defines both the magic method and the {@link #produceResult}, and Spark will use\n * {@link #MAGIC_METHOD_NAME} over the {@link #produceResult(InternalRow)} as it takes higher\n * precedence. Also note that the magic method is annotated as a static method in this case.\n * \u003cp\u003e\n * Resolution on magic method is done during query analysis, where Spark looks up the magic\n * method by first converting the actual input SQL data types to their corresponding Java types\n * following the mapping defined below, and then checking if there is a matching method from all the\n * declared methods in the UDF class, using method name and the Java types.\n * \u003cp\u003e\n * \u003ch2\u003e Handling of nullable primitive arguments \u003c/h2\u003e\n * The handling of null primitive arguments is different between the magic method approach and\n * the {@link #produceResult} approach. With the former, whenever any of the method arguments meet\n * the following conditions:\n * \u003col\u003e\n *   \u003cli\u003ethe argument is of primitive type\u003c/li\u003e\n *   \u003cli\u003ethe argument is nullable\u003c/li\u003e\n *   \u003cli\u003ethe value of the argument is null\u003c/li\u003e\n * \u003c/ol\u003e\n * Spark will return null directly instead of calling the magic method. On the other hand, Spark\n * will pass null primitive arguments to {@link #produceResult} and it is user's responsibility to\n * handle them in the function implementation.\n * \u003cp\u003e\n * Because of the difference, if Spark users want to implement special handling of nulls for\n * nullable primitive arguments, they should override the {@link #produceResult} method instead\n * of using the magic method approach.\n * \u003cp\u003e\n * \u003ch2\u003e Spark data type to Java type mapping \u003c/h2\u003e\n * The following are the mapping from {@link DataType SQL data type} to Java type which is used\n * by Spark to infer parameter types for the magic methods as well as return value type for\n * {@link #produceResult}:\n * \u003cul\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.BooleanType}: {@code boolean}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.ByteType}: {@code byte}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.ShortType}: {@code short}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.IntegerType}: {@code int}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.LongType}: {@code long}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.FloatType}: {@code float}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.DoubleType}: {@code double}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.StringType}:\n *       {@link org.apache.spark.unsafe.types.UTF8String}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.DateType}: {@code int}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.TimestampType}: {@code long}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.BinaryType}: {@code byte[]}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.DayTimeIntervalType}: {@code long}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.YearMonthIntervalType}: {@code int}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.DecimalType}:\n *       {@link org.apache.spark.sql.types.Decimal}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.StructType}: {@link InternalRow}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.ArrayType}:\n *       {@link org.apache.spark.sql.catalyst.util.ArrayData}\u003c/li\u003e\n *   \u003cli\u003e{@link org.apache.spark.sql.types.MapType}:\n *       {@link org.apache.spark.sql.catalyst.util.MapData}\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * @param \u003cR\u003e the JVM type of result values, MUST be consistent with the {@link DataType}\n *          returned via {@link #resultType()}, according to the mapping above.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.functions.ScalarFunction",
		"extends": "org.apache.spark.sql.connector.catalog.functions.BoundFunction",
		"Methods": [
			{
				"signature": "default R produceResult(InternalRow input)",
				"documentation": "/**\n   * Applies the function to an input row to produce a value.\n   *\n   * @param input an input row\n   * @return a result value\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a user-defined function that is not bound to input types.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.functions.UnboundFunction",
		"extends": "org.apache.spark.sql.connector.catalog.functions.Function",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Table methods for working with index\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.index.SupportsIndex",
		"extends": "org.apache.spark.sql.connector.catalog.Table",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Index in a table\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.catalog.index.TableIndex",
		"extends": "",
		"Methods": [
			{
				"signature": "public TableIndex(\n      String indexName,\n      String indexType,\n      NamedReference[] columns,\n      Map\u003cNamedReference, Properties\u003e columnProperties,\n      Properties properties)",
				"documentation": "/**\n * Index in a table\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public String indexName()",
				"documentation": "/**\n   * @return the Index name.\n   */"
			},
			{
				"signature": "public String indexType()",
				"documentation": "/**\n   * @return the indexType of this Index.\n   */"
			},
			{
				"signature": "public NamedReference[] columns()",
				"documentation": "/**\n   * @return the column(s) this Index is on. Could be multi columns (a multi-column index).\n   */"
			},
			{
				"signature": "public Map\u003cNamedReference, Properties\u003e columnProperties()",
				"documentation": "/**\n   * @return the map of column and column property map.\n   */"
			},
			{
				"signature": "public Properties properties()",
				"documentation": "/**\n   * Returns the index properties.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A distribution where tuples that share the same values for clustering expressions are co-located\n * in the same partition.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.distributions.ClusteredDistribution",
		"extends": "org.apache.spark.sql.connector.distributions.Distribution",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface that defines how data is distributed across partitions.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.distributions.Distribution",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.distributions.ClusteredDistribution",
			"org.apache.spark.sql.connector.distributions.OrderedDistribution",
			"org.apache.spark.sql.connector.distributions.UnspecifiedDistribution"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Helper methods to create distributions to pass into Spark.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.distributions.Distributions",
		"extends": "",
		"Methods": [
			{
				"signature": "private Distributions()",
				"documentation": "/**\n * Helper methods to create distributions to pass into Spark.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "public static UnspecifiedDistribution unspecified()",
				"documentation": "/**\n   * Creates a distribution where no promises are made about co-location of data.\n   */"
			},
			{
				"signature": "public static ClusteredDistribution clustered(Expression[] clustering)",
				"documentation": "/**\n   * Creates a distribution where tuples that share the same values for clustering expressions are\n   * co-located in the same partition.\n   */"
			},
			{
				"signature": "public static OrderedDistribution ordered(SortOrder[] ordering)",
				"documentation": "/**\n   * Creates a distribution where tuples have been ordered across partitions according\n   * to ordering expressions, but not necessarily within a given partition.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A distribution where tuples have been ordered across partitions according\n * to ordering expressions, but not necessarily within a given partition.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.distributions.OrderedDistribution",
		"extends": "org.apache.spark.sql.connector.distributions.Distribution",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A distribution where no promises are made about co-location of data.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.distributions.UnspecifiedDistribution",
		"extends": "org.apache.spark.sql.connector.distributions.Distribution",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a cast expression in the public logical expression API.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.Cast",
		"extends": "",
		"Methods": [
			{
				"signature": "public Cast(Expression expression, DataType dataType)",
				"documentation": "/**\n * Represents a cast expression in the public logical expression API.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public Expression expression()",
				"documentation": ""
			},
			{
				"signature": "public DataType dataType()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Expression[] children()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.Expression",
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base class of the public logical expression API.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.Expression",
		"extends": "",
		"Methods": [
			{
				"signature": "default String describe()",
				"documentation": "/**\n   * Format the expression as a human readable SQL-like string.\n   */"
			},
			{
				"signature": "default NamedReference[] references()",
				"documentation": "/**\n   * List of fields or columns that are referenced by this expression.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.expressions.Literal",
			"org.apache.spark.sql.connector.expressions.NamedReference",
			"org.apache.spark.sql.connector.expressions.SortOrder",
			"org.apache.spark.sql.connector.expressions.Transform"
		],
		"implementedBy": [
			"org.apache.spark.sql.connector.expressions.Cast",
			"org.apache.spark.sql.connector.expressions.GeneralScalarExpression"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Helper methods to create logical transforms to pass into Spark.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.Expressions",
		"extends": "",
		"Methods": [
			{
				"signature": "private Expressions()",
				"documentation": "/**\n * Helper methods to create logical transforms to pass into Spark.\n *\n * @since 3.0.0\n */"
			},
			{
				"signature": "public static Transform apply(String name, Expression... args)",
				"documentation": "/**\n   * Create a logical transform for applying a named transform.\n   * \u003cp\u003e\n   * This transform can represent applying any named transform.\n   *\n   * @param name the transform name\n   * @param args expression arguments to the transform\n   * @return a logical transform\n   */"
			},
			{
				"signature": "public static NamedReference column(String name)",
				"documentation": "/**\n   * Create a named reference expression for a (nested) column.\n   *\n   * @param name The column name. It refers to nested column if name contains dot.\n   * @return a named reference for the column\n   */"
			},
			{
				"signature": "public static \u003cT\u003e Literal\u003cT\u003e literal(T value)",
				"documentation": "/**\n   * Create a literal from a value.\n   * \u003cp\u003e\n   * The JVM type of the value held by a literal must be the type used by Spark's InternalRow API\n   * for the literal's {@link DataType SQL data type}.\n   *\n   * @param value a value\n   * @param \u003cT\u003e the JVM type of the value\n   * @return a literal expression for the value\n   */"
			},
			{
				"signature": "public static Transform bucket(int numBuckets, String... columns)",
				"documentation": "/**\n   * Create a bucket transform for one or more columns.\n   * \u003cp\u003e\n   * This transform represents a logical mapping from a value to a bucket id in [0, numBuckets)\n   * based on a hash of the value.\n   * \u003cp\u003e\n   * The name reported by transforms created with this method is \"bucket\".\n   *\n   * @param numBuckets the number of output buckets\n   * @param columns input columns for the bucket transform\n   * @return a logical bucket transform with name \"bucket\"\n   */"
			},
			{
				"signature": "public static Transform identity(String column)",
				"documentation": "/**\n   * Create an identity transform for a column.\n   * \u003cp\u003e\n   * This transform represents a logical mapping from a value to itself.\n   * \u003cp\u003e\n   * The name reported by transforms created with this method is \"identity\".\n   *\n   * @param column an input column\n   * @return a logical identity transform with name \"identity\"\n   */"
			},
			{
				"signature": "public static Transform years(String column)",
				"documentation": "/**\n   * Create a yearly transform for a timestamp or date column.\n   * \u003cp\u003e\n   * This transform represents a logical mapping from a timestamp or date to a year, such as 2018.\n   * \u003cp\u003e\n   * The name reported by transforms created with this method is \"years\".\n   *\n   * @param column an input timestamp or date column\n   * @return a logical yearly transform with name \"years\"\n   */"
			},
			{
				"signature": "public static Transform months(String column)",
				"documentation": "/**\n   * Create a monthly transform for a timestamp or date column.\n   * \u003cp\u003e\n   * This transform represents a logical mapping from a timestamp or date to a month, such as\n   * 2018-05.\n   * \u003cp\u003e\n   * The name reported by transforms created with this method is \"months\".\n   *\n   * @param column an input timestamp or date column\n   * @return a logical monthly transform with name \"months\"\n   */"
			},
			{
				"signature": "public static Transform days(String column)",
				"documentation": "/**\n   * Create a daily transform for a timestamp or date column.\n   * \u003cp\u003e\n   * This transform represents a logical mapping from a timestamp or date to a date, such as\n   * 2018-05-13.\n   * \u003cp\u003e\n   * The name reported by transforms created with this method is \"days\".\n   *\n   * @param column an input timestamp or date column\n   * @return a logical daily transform with name \"days\"\n   */"
			},
			{
				"signature": "public static Transform hours(String column)",
				"documentation": "/**\n   * Create an hourly transform for a timestamp column.\n   * \u003cp\u003e\n   * This transform represents a logical mapping from a timestamp to a date and hour, such as\n   * 2018-05-13, hour 19.\n   * \u003cp\u003e\n   * The name reported by transforms created with this method is \"hours\".\n   *\n   * @param column an input timestamp column\n   * @return a logical hourly transform with name \"hours\"\n   */"
			},
			{
				"signature": "public static SortOrder sort(Expression expr, SortDirection direction, NullOrdering nullOrder)",
				"documentation": "/**\n   * Create a sort expression.\n   *\n   * @param expr an expression to produce values to sort\n   * @param direction direction of the sort\n   * @param nullOrder null order of the sort\n   * @return a SortOrder\n   *\n   * @since 3.2.0\n   */"
			},
			{
				"signature": "public static SortOrder sort(Expression expr, SortDirection direction)",
				"documentation": "/**\n   * Create a sort expression.\n   *\n   * @param expr an expression to produce values to sort\n   * @param direction direction of the sort\n   * @return a SortOrder\n   *\n   * @since 3.2.0\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.MyScanBuilder"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The general representation of SQL scalar expressions, which contains the upper-cased\n * expression name and all the children expressions. Please also see {@link Predicate}\n * for the supported predicate expressions.\n * \u003cp\u003e\n * The currently supported SQL scalar expressions:\n * \u003col\u003e\n *  \u003cli\u003eName: \u003ccode\u003e+\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 + expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e-\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 - expr2\u003c/code\u003e or \u003ccode\u003e- expr\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e*\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 * expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e/\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 / expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e%\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 % expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026amp;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026amp; expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e|\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 | expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e^\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 ^ expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e~\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003e~ expr\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eCASE_WHEN\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic:\n *     \u003ccode\u003eCASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END\u003c/code\u003e\n *    \u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eABS\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eABS(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eCOALESCE\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eCOALESCE(expr1, expr2)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eLN\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eLN(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eEXP\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eEXP(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003ePOWER\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003ePOWER(expr, number)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eSQRT\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eSQRT(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eFLOOR\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eFLOOR(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eCEIL\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eCEIL(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eWIDTH_BUCKET\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eWIDTH_BUCKET(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n * \u003c/ol\u003e\n * Note: SQL semantic conforms ANSI standard, so some expressions are not supported when ANSI off,\n * including: add, subtract, multiply, divide, remainder, pmod.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.GeneralScalarExpression",
		"extends": "",
		"Methods": [
			{
				"signature": "public GeneralScalarExpression(String name, Expression[] children)",
				"documentation": "/**\n * The general representation of SQL scalar expressions, which contains the upper-cased\n * expression name and all the children expressions. Please also see {@link Predicate}\n * for the supported predicate expressions.\n * \u003cp\u003e\n * The currently supported SQL scalar expressions:\n * \u003col\u003e\n *  \u003cli\u003eName: \u003ccode\u003e+\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 + expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e-\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 - expr2\u003c/code\u003e or \u003ccode\u003e- expr\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e*\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 * expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e/\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 / expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e%\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 % expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026amp;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026amp; expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e|\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 | expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e^\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 ^ expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e~\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003e~ expr\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eCASE_WHEN\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic:\n *     \u003ccode\u003eCASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END\u003c/code\u003e\n *    \u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eABS\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eABS(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eCOALESCE\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eCOALESCE(expr1, expr2)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eLN\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eLN(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eEXP\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eEXP(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003ePOWER\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003ePOWER(expr, number)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eSQRT\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eSQRT(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eFLOOR\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eFLOOR(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eCEIL\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eCEIL(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eWIDTH_BUCKET\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eWIDTH_BUCKET(expr)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n * \u003c/ol\u003e\n * Note: SQL semantic conforms ANSI standard, so some expressions are not supported when ANSI off,\n * including: add, subtract, multiply, divide, remainder, pmod.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public String name()",
				"documentation": ""
			},
			{
				"signature": "public Expression[] children()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.Expression",
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.sql.connector.util.V2ExpressionSQLBuilder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a constant literal value in the public expression API.\n * \u003cp\u003e\n * The JVM type of the value held by a literal must be the type used by Spark's InternalRow API for\n * the literal's {@link DataType SQL data type}.\n *\n * @param \u003cT\u003e the JVM type of a value held by the literal\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.Literal",
		"extends": "org.apache.spark.sql.connector.expressions.Expression",
		"Methods": [
			{
				"signature": "@Override\n  default Expression[] children()",
				"documentation": "/**\n   * Returns the SQL data type of the literal.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a field or column reference in the public logical expression API.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.NamedReference",
		"extends": "org.apache.spark.sql.connector.expressions.Expression",
		"Methods": [
			{
				"signature": "@Override\n  default Expression[] children()",
				"documentation": "/**\n   * Returns the referenced field name as an array of String parts.\n   * \u003cp\u003e\n   * Each string in the returned array represents a field name.\n   */"
			},
			{
				"signature": "@Override\n  default NamedReference[] references()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A null order used in sorting expressions.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.NullOrdering",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public String toString()",
				"documentation": "/**\n * A null order used in sorting expressions.\n *\n * @since 3.2.0\n */"
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A sort direction used in sorting expressions.\n * \u003cp\u003e\n * Each direction has a default null ordering that is implied if no null ordering is specified\n * explicitly.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.SortDirection",
		"extends": "",
		"Methods": [
			{
				"signature": "SortDirection(NullOrdering defaultNullOrdering)",
				"documentation": "/**\n * A sort direction used in sorting expressions.\n * \u003cp\u003e\n * Each direction has a default null ordering that is implied if no null ordering is specified\n * explicitly.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "public NullOrdering defaultNullOrdering()",
				"documentation": "/**\n   * Returns the default null ordering to use if no null ordering is specified explicitly.\n   */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a sort order in the public expression API.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.SortOrder",
		"extends": "org.apache.spark.sql.connector.expressions.Expression",
		"Methods": [
			{
				"signature": "@Override\n  default Expression[] children()",
				"documentation": "/**\n   * Returns the null ordering.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a transform function in the public logical expression API.\n * \u003cp\u003e\n * For example, the transform date(ts) is used to derive a date value from a timestamp column. The\n * transform name is \"date\" and its argument is a reference to the \"ts\" column.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.Transform",
		"extends": "org.apache.spark.sql.connector.expressions.Expression",
		"Methods": [
			{
				"signature": "@Override\n  default Expression[] children()",
				"documentation": "/**\n   * Returns the arguments passed to the transform function.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base class of the Aggregate Functions.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc",
		"extends": "Expression,",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.expressions.aggregate.Avg",
			"org.apache.spark.sql.connector.expressions.aggregate.Count",
			"org.apache.spark.sql.connector.expressions.aggregate.CountStar",
			"org.apache.spark.sql.connector.expressions.aggregate.GeneralAggregateFunc",
			"org.apache.spark.sql.connector.expressions.aggregate.Max",
			"org.apache.spark.sql.connector.expressions.aggregate.Min",
			"org.apache.spark.sql.connector.expressions.aggregate.Sum"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Aggregation in SQL statement.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.Aggregation",
		"extends": "",
		"Methods": [
			{
				"signature": "public Aggregation(AggregateFunc[] aggregateExpressions, Expression[] groupByExpressions)",
				"documentation": "/**\n * Aggregation in SQL statement.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "public AggregateFunc[] aggregateExpressions()",
				"documentation": ""
			},
			{
				"signature": "public Expression[] groupByExpressions()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An aggregate function that returns the mean of all the values in a group.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.Avg",
		"extends": "",
		"Methods": [
			{
				"signature": "public Avg(Expression column, boolean isDistinct)",
				"documentation": "/**\n * An aggregate function that returns the mean of all the values in a group.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public Expression column()",
				"documentation": ""
			},
			{
				"signature": "public boolean isDistinct()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Expression[] children()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An aggregate function that returns the number of the specific row in a group.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.Count",
		"extends": "",
		"Methods": [
			{
				"signature": "public Count(Expression column, boolean isDistinct)",
				"documentation": "/**\n * An aggregate function that returns the number of the specific row in a group.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "public Expression column()",
				"documentation": ""
			},
			{
				"signature": "public boolean isDistinct()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Expression[] children()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An aggregate function that returns the number of rows in a group.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.CountStar",
		"extends": "",
		"Methods": [
			{
				"signature": "public CountStar()",
				"documentation": "/**\n * An aggregate function that returns the number of rows in a group.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "@Override\n  public Expression[] children()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The general implementation of {@link AggregateFunc}, which contains the upper-cased function\n * name, the `isDistinct` flag and all the inputs. Note that Spark cannot push down partial\n * aggregate with this function to the source, but can only push down the entire aggregate.\n * \u003cp\u003e\n * The currently supported SQL aggregate functions:\n * \u003col\u003e\n *  \u003cli\u003e\u003cpre\u003eVAR_POP(input1)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eVAR_SAMP(input1)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eSTDDEV_POP(input1)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eSTDDEV_SAMP(input1)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eCOVAR_POP(input1, input2)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eCOVAR_SAMP(input1, input2)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eCORR(input1, input2)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n * \u003c/ol\u003e\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.GeneralAggregateFunc",
		"extends": "",
		"Methods": [
			{
				"signature": "public String name()",
				"documentation": "/**\n * The general implementation of {@link AggregateFunc}, which contains the upper-cased function\n * name, the `isDistinct` flag and all the inputs. Note that Spark cannot push down partial\n * aggregate with this function to the source, but can only push down the entire aggregate.\n * \u003cp\u003e\n * The currently supported SQL aggregate functions:\n * \u003col\u003e\n *  \u003cli\u003e\u003cpre\u003eVAR_POP(input1)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eVAR_SAMP(input1)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eSTDDEV_POP(input1)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eSTDDEV_SAMP(input1)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eCOVAR_POP(input1, input2)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eCOVAR_SAMP(input1, input2)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n *  \u003cli\u003e\u003cpre\u003eCORR(input1, input2)\u003c/pre\u003e Since 3.3.0\u003c/li\u003e\n * \u003c/ol\u003e\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public boolean isDistinct()",
				"documentation": ""
			},
			{
				"signature": "public GeneralAggregateFunc(String name, boolean isDistinct, Expression[] children)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Expression[] children()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An aggregate function that returns the maximum value in a group.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.Max",
		"extends": "",
		"Methods": [
			{
				"signature": "public Max(Expression column)",
				"documentation": "/**\n * An aggregate function that returns the maximum value in a group.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "public Expression column()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Expression[] children()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An aggregate function that returns the minimum value in a group.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.Min",
		"extends": "",
		"Methods": [
			{
				"signature": "public Min(Expression column)",
				"documentation": "/**\n * An aggregate function that returns the minimum value in a group.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "public Expression column()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Expression[] children()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An aggregate function that returns the summation of all the values in a group.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.aggregate.Sum",
		"extends": "",
		"Methods": [
			{
				"signature": "public Sum(Expression column, boolean isDistinct)",
				"documentation": "/**\n * An aggregate function that returns the summation of all the values in a group.\n *\n * @since 3.2.0\n */"
			},
			{
				"signature": "public Expression column()",
				"documentation": ""
			},
			{
				"signature": "public boolean isDistinct()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Expression[] children()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.aggregate.AggregateFunc"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A predicate that always evaluates to {@code false}.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.filter.AlwaysFalse",
		"extends": "org.apache.spark.sql.connector.expressions.filter.Predicate",
		"Methods": [
			{
				"signature": "public AlwaysFalse()",
				"documentation": "/**\n * A predicate that always evaluates to {@code false}.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public Boolean value()",
				"documentation": ""
			},
			{
				"signature": "public DataType dataType()",
				"documentation": ""
			},
			{
				"signature": "public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.Literal"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A predicate that always evaluates to {@code true}.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.filter.AlwaysTrue",
		"extends": "org.apache.spark.sql.connector.expressions.filter.Predicate",
		"Methods": [
			{
				"signature": "public AlwaysTrue()",
				"documentation": "/**\n * A predicate that always evaluates to {@code true}.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public Boolean value()",
				"documentation": ""
			},
			{
				"signature": "public DataType dataType()",
				"documentation": ""
			},
			{
				"signature": "public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.expressions.Literal"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A predicate that evaluates to {@code true} iff both {@code left} and {@code right} evaluate to\n * {@code true}.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.filter.And",
		"extends": "org.apache.spark.sql.connector.expressions.filter.Predicate",
		"Methods": [
			{
				"signature": "public And(Predicate left, Predicate right)",
				"documentation": "/**\n * A predicate that evaluates to {@code true} iff both {@code left} and {@code right} evaluate to\n * {@code true}.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public Predicate left()",
				"documentation": ""
			},
			{
				"signature": "public Predicate right()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A predicate that evaluates to {@code true} iff {@code child} is evaluated to {@code false}.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.filter.Not",
		"extends": "org.apache.spark.sql.connector.expressions.filter.Predicate",
		"Methods": [
			{
				"signature": "public Not(Predicate child)",
				"documentation": "/**\n * A predicate that evaluates to {@code true} iff {@code child} is evaluated to {@code false}.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public Predicate child()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A predicate that evaluates to {@code true} iff at least one of {@code left} or {@code right}\n * evaluates to {@code true}.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.filter.Or",
		"extends": "org.apache.spark.sql.connector.expressions.filter.Predicate",
		"Methods": [
			{
				"signature": "public Or(Predicate left, Predicate right)",
				"documentation": "/**\n * A predicate that evaluates to {@code true} iff at least one of {@code left} or {@code right}\n * evaluates to {@code true}.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public Predicate left()",
				"documentation": ""
			},
			{
				"signature": "public Predicate right()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The general representation of predicate expressions, which contains the upper-cased expression\n * name and all the children expressions. You can also use these concrete subclasses for better\n * type safety: {@link And}, {@link Or}, {@link Not}, {@link AlwaysTrue}, {@link AlwaysFalse}.\n * \u003cp\u003e\n * The currently supported predicate expressions:\n * \u003col\u003e\n *  \u003cli\u003eName: \u003ccode\u003eIS_NULL\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr IS NULL\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eIS_NOT_NULL\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr IS NOT NULL\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eSTARTS_WITH\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 LIKE 'expr2%'\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eENDS_WITH\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 LIKE '%expr2'\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eCONTAINS\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 LIKE '%expr2%'\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eIN\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr IN (expr1, expr2, ...)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e=\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 = expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026lt;\u0026gt;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026lt;\u0026gt; expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026lt;=\u0026gt;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: null-safe version of \u003ccode\u003eexpr1 = expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026lt;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026lt; expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026lt;=\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026lt;= expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026gt;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026gt; expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026gt;=\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026gt;= expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eAND\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 AND expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eOR\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 OR expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eNOT\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eNOT expr\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eALWAYS_TRUE\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eTRUE\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eALWAYS_FALSE\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eFALSE\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n * \u003c/ol\u003e\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.expressions.filter.Predicate",
		"extends": "org.apache.spark.sql.connector.expressions.GeneralScalarExpression",
		"Methods": [
			{
				"signature": "public Predicate(String name, Expression[] children)",
				"documentation": "/**\n * The general representation of predicate expressions, which contains the upper-cased expression\n * name and all the children expressions. You can also use these concrete subclasses for better\n * type safety: {@link And}, {@link Or}, {@link Not}, {@link AlwaysTrue}, {@link AlwaysFalse}.\n * \u003cp\u003e\n * The currently supported predicate expressions:\n * \u003col\u003e\n *  \u003cli\u003eName: \u003ccode\u003eIS_NULL\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr IS NULL\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eIS_NOT_NULL\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr IS NOT NULL\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eSTARTS_WITH\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 LIKE 'expr2%'\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eENDS_WITH\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 LIKE '%expr2'\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eCONTAINS\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 LIKE '%expr2%'\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eIN\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr IN (expr1, expr2, ...)\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e=\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 = expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026lt;\u0026gt;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026lt;\u0026gt; expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026lt;=\u0026gt;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: null-safe version of \u003ccode\u003eexpr1 = expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026lt;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026lt; expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026lt;=\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026lt;= expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026gt;\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026gt; expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003e\u0026gt;=\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 \u0026gt;= expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eAND\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 AND expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eOR\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eexpr1 OR expr2\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eNOT\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eNOT expr\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eALWAYS_TRUE\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eTRUE\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n *  \u003cli\u003eName: \u003ccode\u003eALWAYS_FALSE\u003c/code\u003e\n *   \u003cul\u003e\n *    \u003cli\u003eSQL semantic: \u003ccode\u003eFALSE\u003c/code\u003e\u003c/li\u003e\n *    \u003cli\u003eSince version: 3.3.0\u003c/li\u003e\n *   \u003c/ul\u003e\n *  \u003c/li\u003e\n * \u003c/ol\u003e\n *\n * @since 3.3.0\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.expressions.filter.AlwaysFalse",
			"org.apache.spark.sql.connector.expressions.filter.AlwaysTrue",
			"org.apache.spark.sql.connector.expressions.filter.And",
			"org.apache.spark.sql.connector.expressions.filter.Not",
			"org.apache.spark.sql.connector.expressions.filter.Or"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Built-in `CustomMetric` that computes average of metric values. Note that please extend this\n * class and override `name` and `description` to create your custom metric for real usage.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.metric.CustomAvgMetric",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public String aggregateTaskMetrics(long[] taskMetrics)",
				"documentation": "/**\n * Built-in `CustomMetric` that computes average of metric values. Note that please extend this\n * class and override `name` and `description` to create your custom metric for real usage.\n *\n * @since 3.2.0\n */"
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.metric.CustomMetric"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A custom metric. Data source can define supported custom metrics using this interface.\n * During query execution, Spark will collect the task metrics using {@link CustomTaskMetric}\n * and combine the metrics at the driver side. How to combine task metrics is defined by the\n * metric class with the same metric name.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.metric.CustomMetric",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.metric.CustomAvgMetric",
			"org.apache.spark.sql.connector.metric.CustomSumMetric"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Built-in `CustomMetric` that sums up metric values. Note that please extend this class\n * and override `name` and `description` to create your custom metric for real usage.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.metric.CustomSumMetric",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public String aggregateTaskMetrics(long[] taskMetrics)",
				"documentation": "/**\n * Built-in `CustomMetric` that sums up metric values. Note that please extend this class\n * and override `name` and `description` to create your custom metric for real usage.\n *\n * @since 3.2.0\n */"
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.metric.CustomMetric"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A custom task metric. This is a logical representation of a metric reported by data sources\n * at the executor side. During query execution, Spark will collect the task metrics per partition\n * by {@link PartitionReader} and update internal metrics based on collected metric values.\n * For streaming query, Spark will collect and combine metrics for a final result per micro batch.\n * \u003cp\u003e\n * The metrics will be gathered during query execution back to the driver and then combined. How\n * the task metrics are combined is defined by corresponding {@link CustomMetric} with same metric\n * name. The final result will be shown up in the data source scan operator in Spark UI.\n * \u003cp\u003e\n * There are a few special metric names: \"bytesWritten\" and \"recordsWritten\". If the data source\n * defines custom metrics with the same names, the metric values will also be updated to\n * corresponding task metrics.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.metric.CustomTaskMetric",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A physical representation of a data source scan for batch queries. This interface is used to\n * provide physical information, like how many partitions the scanned data has, and how to read\n * records from the partitions.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.Batch",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in for input partitions whose records are clustered on the same set of partition keys\n * (provided via {@link SupportsReportPartitioning}, see below). Data sources can opt-in to\n * implement this interface for the partitions they report to Spark, which will use the\n * information to avoid data shuffling in certain scenarios, such as join, aggregate, etc. Note\n * that Spark requires ALL input partitions to implement this interface, otherwise it can't take\n * advantage of it.\n * \u003cp\u003e\n * This interface should be used in combination with {@link SupportsReportPartitioning}, which\n * allows data sources to report distribution and ordering spec to Spark. In particular, Spark\n * expects data sources to report\n * {@link org.apache.spark.sql.connector.distributions.ClusteredDistribution} whenever its input\n * partitions implement this interface. Spark derives partition keys spec (e.g., column names,\n * transforms) from the distribution, and partition values from the input partitions.\n * \u003cp\u003e\n * It is implementor's responsibility to ensure that when an input partition implements this\n * interface, its records all have the same value for the partition keys. Spark doesn't check\n * this property.\n *\n * @see org.apache.spark.sql.connector.read.SupportsReportPartitioning\n * @see org.apache.spark.sql.connector.read.partitioning.Partitioning\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.HasPartitionKey",
		"extends": "org.apache.spark.sql.connector.read.InputPartition",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A serializable representation of an input partition returned by\n * {@link Batch#planInputPartitions()} and the corresponding ones in streaming .\n * \u003cp\u003e\n * Note that {@link InputPartition} will be serialized and sent to executors, then\n * {@link PartitionReader} will be created by\n * {@link PartitionReaderFactory#createReader(InputPartition)} or\n * {@link PartitionReaderFactory#createColumnarReader(InputPartition)} on executors to do\n * the actual reading. So {@link InputPartition} must be serializable while {@link PartitionReader}\n * doesn't need to be.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.InputPartition",
		"extends": "java.io.Serializable",
		"Methods": [
			{
				"signature": "default String[] preferredLocations()",
				"documentation": "/**\n   * The preferred locations where the input partition reader returned by this partition can run\n   * faster, but Spark does not guarantee to run the input partition reader on these locations.\n   * The implementations should make sure that it can be run on any location.\n   * The location is a string representing the host name.\n   * \u003cp\u003e\n   * Note that if a host name cannot be recognized by Spark, it will be ignored as it was not in\n   * the returned locations. The default return value is empty string array, which means this\n   * input partition's reader has no location preference.\n   * \u003cp\u003e\n   * If this method fails (by throwing an exception), the action will fail and no Spark job will be\n   * submitted.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.read.HasPartitionKey"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A special Scan which will happen on Driver locally instead of Executors.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.read.LocalScan",
		"extends": "org.apache.spark.sql.connector.read.Scan",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A partition reader returned by {@link PartitionReaderFactory#createReader(InputPartition)} or\n * {@link PartitionReaderFactory#createColumnarReader(InputPartition)}. It's responsible for\n * outputting data for a RDD partition.\n * \u003cp\u003e\n * Note that, Currently the type `T` can only be {@link org.apache.spark.sql.catalyst.InternalRow}\n * for normal data sources, or {@link org.apache.spark.sql.vectorized.ColumnarBatch} for columnar\n * data sources(whose {@link PartitionReaderFactory#supportColumnarReads(InputPartition)}\n * returns true).\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.PartitionReader",
		"extends": "java.io.Closeable",
		"Methods": [
			{
				"signature": "default CustomTaskMetric[] currentMetricsValues()",
				"documentation": "/**\n   * Returns an array of custom task metrics. By default it returns empty array. Note that it is\n   * not recommended to put heavy logic in this method as it may affect reading performance.\n   */"
			},
			{
				"signature": "CustomTaskMetric[] NO_METRICS =",
				"documentation": "/**\n   * Returns an array of custom task metrics. By default it returns empty array. Note that it is\n   * not recommended to put heavy logic in this method as it may affect reading performance.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.ColumnarReaderFactory",
			"test.org.apache.spark.sql.connector.JavaSimpleReaderFactory",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVReaderFactory"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A factory used to create {@link PartitionReader} instances.\n * \u003cp\u003e\n * If Spark fails to execute any methods in the implementations of this interface or in the returned\n * {@link PartitionReader} (by throwing an exception), corresponding Spark task would fail and\n * get retried until hitting the maximum retry times.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.PartitionReaderFactory",
		"extends": "java.io.Serializable",
		"Methods": [
			{
				"signature": "default PartitionReader\u003cColumnarBatch\u003e createColumnarReader(InputPartition partition)",
				"documentation": "/**\n   * Returns a columnar partition reader to read data from the given {@link InputPartition}.\n   * \u003cp\u003e\n   * Implementations probably need to cast the input partition to the concrete\n   * {@link InputPartition} class defined for the data source.\n   */"
			},
			{
				"signature": "default boolean supportColumnarReads(InputPartition partition)",
				"documentation": "/**\n   * Returns true if the given {@link InputPartition} should be read by Spark in a columnar way.\n   * This means, implementations must also implement {@link #createColumnarReader(InputPartition)}\n   * for the input partitions that this method returns true.\n   * \u003cp\u003e\n   * As of Spark 2.4, Spark can only read all input partition in a columnar way, or none of them.\n   * Data source can't mix columnar and row-based partitions. This may be relaxed in future\n   * versions.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A logical representation of a data source scan. This interface is used to provide logical\n * information, like what the actual read schema is.\n * \u003cp\u003e\n * This logical representation is shared between batch scan, micro-batch streaming scan and\n * continuous streaming scan. Data sources must implement the corresponding methods in this\n * interface, to match what the table promises to support. For example, {@link #toBatch()} must be\n * implemented, if the {@link Table} that creates this {@link Scan} returns\n * {@link TableCapability#BATCH_READ} support in its {@link Table#capabilities()}.\n * \u003c/p\u003e\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.Scan",
		"extends": "",
		"Methods": [
			{
				"signature": "default String description()",
				"documentation": "/**\n   * A description string of this scan, which may includes information like: what filters are\n   * configured for this scan, what's the value of some important options like path, etc. The\n   * description doesn't need to include {@link #readSchema()}, as Spark already knows it.\n   * \u003cp\u003e\n   * By default this returns the class name of the implementation. Please override it to provide a\n   * meaningful description.\n   * \u003c/p\u003e\n   */"
			},
			{
				"signature": "default Batch toBatch()",
				"documentation": "/**\n   * Returns the physical representation of this scan for batch query. By default this method throws\n   * exception, data sources must overwrite this method to provide an implementation, if the\n   * {@link Table} that creates this scan returns {@link TableCapability#BATCH_READ} support in its\n   * {@link Table#capabilities()}.\n   * \u003cp\u003e\n   * If the scan supports runtime filtering and implements {@link SupportsRuntimeFiltering},\n   * this method may be called multiple times. Therefore, implementations can cache some state\n   * to avoid planning the job twice.\n   *\n   * @throws UnsupportedOperationException\n   */"
			},
			{
				"signature": "default MicroBatchStream toMicroBatchStream(String checkpointLocation)",
				"documentation": "/**\n   * Returns the physical representation of this scan for streaming query with micro-batch mode. By\n   * default this method throws exception, data sources must overwrite this method to provide an\n   * implementation, if the {@link Table} that creates this scan returns\n   * {@link TableCapability#MICRO_BATCH_READ} support in its {@link Table#capabilities()}.\n   *\n   * @param checkpointLocation a path to Hadoop FS scratch space that can be used for failure\n   *                           recovery. Data streams for the same logical source in the same query\n   *                           will be given the same checkpointLocation.\n   *\n   * @throws UnsupportedOperationException\n   */"
			},
			{
				"signature": "default ContinuousStream toContinuousStream(String checkpointLocation)",
				"documentation": "/**\n   * Returns the physical representation of this scan for streaming query with continuous mode. By\n   * default this method throws exception, data sources must overwrite this method to provide an\n   * implementation, if the {@link Table} that creates this scan returns\n   * {@link TableCapability#CONTINUOUS_READ} support in its {@link Table#capabilities()}.\n   *\n   * @param checkpointLocation a path to Hadoop FS scratch space that can be used for failure\n   *                           recovery. Data streams for the same logical source in the same query\n   *                           will be given the same checkpointLocation.\n   *\n   * @throws UnsupportedOperationException\n   */"
			},
			{
				"signature": "default CustomMetric[] supportedCustomMetrics()",
				"documentation": "/**\n   * Returns an array of supported custom metrics with name and description.\n   * By default it returns empty array.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.read.LocalScan",
			"org.apache.spark.sql.connector.read.SupportsReportPartitioning",
			"org.apache.spark.sql.connector.read.SupportsReportStatistics",
			"org.apache.spark.sql.connector.read.SupportsRuntimeFiltering",
			"org.apache.spark.sql.connector.read.V1Scan"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface for building the {@link Scan}. Implementations can mixin SupportsPushDownXYZ\n * interfaces to do operator push down, and keep the operator push down result in the returned\n * {@link Scan}. When pushing down operators, the push down order is:\n * sample -\u0026gt; filter -\u0026gt; aggregate -\u0026gt; limit -\u0026gt; column pruning.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.ScanBuilder",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.read.SupportsPushDownAggregates",
			"org.apache.spark.sql.connector.read.SupportsPushDownFilters",
			"org.apache.spark.sql.connector.read.SupportsPushDownLimit",
			"org.apache.spark.sql.connector.read.SupportsPushDownRequiredColumns",
			"org.apache.spark.sql.connector.read.SupportsPushDownTableSample",
			"org.apache.spark.sql.connector.read.SupportsPushDownTopN",
			"org.apache.spark.sql.connector.read.SupportsPushDownV2Filters"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface to represent statistics for a data source, which is returned by\n * {@link SupportsReportStatistics#estimateStatistics()}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.Statistics",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaReportStatisticsDataSource",
			"test.org.apache.spark.sql.connector.JavaReportStatisticsDataSource.MyScanBuilder"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to\n * push down aggregates.\n * \u003cp\u003e\n * If the data source can't fully complete the grouping work, then\n * {@link #supportCompletePushDown(Aggregation)} should return false, and Spark will group the data\n * source output again. For queries like \"SELECT min(value) AS m FROM t GROUP BY key\", after\n * pushing down the aggregate to the data source, the data source can still output data with\n * duplicated keys, which is OK as Spark will do GROUP BY key again. The final query plan can be\n * something like this:\n * \u003cpre\u003e\n *   Aggregate [key#1], [min(min_value#2) AS m#3]\n *     +- RelationV2[key#1, min_value#2]\n * \u003c/pre\u003e\n * Similarly, if there is no grouping expression, the data source can still output more than one\n * rows.\n * \u003cp\u003e\n * When pushing down operators, Spark pushes down filters to the data source first, then push down\n * aggregates or apply column pruning. Depends on data source implementation, aggregates may or\n * may not be able to be pushed down with filters. If pushed filters still need to be evaluated\n * after scanning, aggregates can't be pushed down.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsPushDownAggregates",
		"extends": "org.apache.spark.sql.connector.read.ScanBuilder",
		"Methods": [
			{
				"signature": "default boolean supportCompletePushDown(Aggregation aggregation)",
				"documentation": "/**\n   * Whether the datasource support complete aggregation push-down. Spark will do grouping again\n   * if this method returns false.\n   *\n   * @param aggregation Aggregation in SQL statement.\n   * @return true if the aggregation can be pushed down to datasource completely, false otherwise.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to\n * push down filters to the data source and reduce the size of the data to be read.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsPushDownFilters",
		"extends": "org.apache.spark.sql.connector.read.ScanBuilder",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to\n * push down LIMIT. Please note that the combination of LIMIT with other operations\n * such as AGGREGATE, GROUP BY, SORT BY, CLUSTER BY, DISTRIBUTE BY, etc. is NOT pushed down.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsPushDownLimit",
		"extends": "org.apache.spark.sql.connector.read.ScanBuilder",
		"Methods": [
			{
				"signature": "default boolean isPartiallyPushed()",
				"documentation": "/**\n   * Whether the LIMIT is partially pushed or not. If it returns true, then Spark will do LIMIT\n   * again. This method will only be called when {@link #pushLimit} returns true.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link ScanBuilder}. Data sources can implement this\n * interface to push down required columns to the data source and only read these columns during\n * scan to reduce the size of the data to be read.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsPushDownRequiredColumns",
		"extends": "org.apache.spark.sql.connector.read.ScanBuilder",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link Scan}. Data sources can implement this interface to\n * push down SAMPLE.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsPushDownTableSample",
		"extends": "org.apache.spark.sql.connector.read.ScanBuilder",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to\n * push down top N(query with ORDER BY ... LIMIT n). Please note that the combination of top N\n * with other operations such as AGGREGATE, GROUP BY, CLUSTER BY, DISTRIBUTE BY, etc.\n * is NOT pushed down.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsPushDownTopN",
		"extends": "org.apache.spark.sql.connector.read.ScanBuilder",
		"Methods": [
			{
				"signature": "default boolean isPartiallyPushed()",
				"documentation": "/**\n     * Whether the top N is partially pushed or not. If it returns true, then Spark will do top N\n     * again. This method will only be called when {@link #pushTopN} returns true.\n     */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link ScanBuilder}. Data sources can implement this interface to\n * push down V2 {@link Predicate} to the data source and reduce the size of the data to be read.\n * Please Note that this interface is preferred over {@link SupportsPushDownFilters}, which uses\n * V1 {@link org.apache.spark.sql.sources.Filter} and is less efficient due to the\n * internal -\u0026gt; external data conversion.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsPushDownV2Filters",
		"extends": "org.apache.spark.sql.connector.read.ScanBuilder",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix in interface for {@link Scan}. Data sources can implement this interface to\n * report data partitioning and try to avoid shuffle at Spark side.\n * \u003cp\u003e\n * Note that, when a {@link Scan} implementation creates exactly one {@link InputPartition},\n * Spark may avoid adding a shuffle even if the reader does not implement this interface.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsReportPartitioning",
		"extends": "org.apache.spark.sql.connector.read.Scan",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix in interface for {@link Scan}. Data sources can implement this interface to\n * report statistics to Spark.\n * \u003cp\u003e\n * As of Spark 3.0, statistics are reported to the optimizer after operators are pushed to the\n * data source. Implementations may return more accurate statistics based on pushed operators\n * which may improve query performance by providing better information to the optimizer.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsReportStatistics",
		"extends": "org.apache.spark.sql.connector.read.Scan",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link Scan}. Data sources can implement this interface if they can\n * filter initially planned {@link InputPartition}s using predicates Spark infers at runtime.\n * \u003cp\u003e\n * Note that Spark will push runtime filters only if they are beneficial.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.read.SupportsRuntimeFiltering",
		"extends": "org.apache.spark.sql.connector.read.Scan",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a partitioning where rows are split across partitions based on the\n * partition transform expressions returned by {@link KeyGroupedPartitioning#keys}.\n * \u003cp\u003e\n * Note: Data source implementations should make sure for a single partition, all of its rows\n * must be evaluated to the same partition value after being applied by\n * {@link KeyGroupedPartitioning#keys} expressions. Different partitions can share the same\n * partition value: Spark will group these into a single logical partition during planning phase.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.partitioning.KeyGroupedPartitioning",
		"extends": "",
		"Methods": [
			{
				"signature": "public KeyGroupedPartitioning(Expression[] keys, int numPartitions)",
				"documentation": "/**\n * Represents a partitioning where rows are split across partitions based on the\n * partition transform expressions returned by {@link KeyGroupedPartitioning#keys}.\n * \u003cp\u003e\n * Note: Data source implementations should make sure for a single partition, all of its rows\n * must be evaluated to the same partition value after being applied by\n * {@link KeyGroupedPartitioning#keys} expressions. Different partitions can share the same\n * partition value: Spark will group these into a single logical partition during planning phase.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "public Expression[] keys()",
				"documentation": "/**\n   * Returns the partition transform expressions for this partitioning.\n   */"
			},
			{
				"signature": "@Override\n  public int numPartitions()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.partitioning.Partitioning"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.MyScanBuilder"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface to represent the output data partitioning for a data source, which is returned by\n * {@link SupportsReportPartitioning#outputPartitioning()}.\n * \u003cp\u003e\n * Note: implementors \u003cb\u003eshould NOT\u003c/b\u003e directly implement this interface. Instead, they should\n * use one of the following subclasses:\n * \u003cul\u003e\n * \u003cli\u003e{@link KeyGroupedPartitioning}\u003c/li\u003e\n * \u003cli\u003e{@link UnknownPartitioning}\u003c/li\u003e\n * \u003c/ul\u003e\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.partitioning.Partitioning",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.read.partitioning.KeyGroupedPartitioning",
			"org.apache.spark.sql.connector.read.partitioning.UnknownPartitioning"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a partitioning where rows are split across partitions in an unknown pattern.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.partitioning.UnknownPartitioning",
		"extends": "",
		"Methods": [
			{
				"signature": "public UnknownPartitioning(int numPartitions)",
				"documentation": "/**\n * Represents a partitioning where rows are split across partitions in an unknown pattern.\n *\n * @since 3.3.0\n */"
			},
			{
				"signature": "@Override\n  public int numPartitions()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.partitioning.Partitioning"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Indicates that the source accepts the latest seen offset, which requires streaming execution\n * to provide the latest seen offset when restarting the streaming query from checkpoint.\n *\n * Note that this interface aims to only support DSv2 streaming sources. Spark may throw error\n * if the interface is implemented along with DSv1 streaming sources.\n *\n * The callback method will be called once per run.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.AcceptsLatestSeenOffset",
		"extends": "org.apache.spark.sql.connector.read.streaming.SparkDataStream",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n /**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} should scan approximately\n * given maximum number of rows with at least the given minimum number of rows.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.CompositeReadLimit",
		"extends": "",
		"Methods": [
			{
				"signature": "CompositeReadLimit(ReadLimit[] readLimits)",
				"documentation": "/**\n /**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} should scan approximately\n * given maximum number of rows with at least the given minimum number of rows.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.2.0\n */"
			},
			{
				"signature": "public ReadLimit[] getReadLimits()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.streaming.ReadLimit"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A variation on {@link PartitionReader} for use with continuous streaming processing.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader",
		"extends": "org.apache.spark.sql.connector.read.PartitionReader",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A variation on {@link PartitionReaderFactory} that returns {@link ContinuousPartitionReader}\n * instead of {@link PartitionReader}. It's used for continuous streaming processing.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory",
		"extends": "org.apache.spark.sql.connector.read.PartitionReaderFactory",
		"Methods": [
			{
				"signature": "@Override\n  default ContinuousPartitionReader\u003cColumnarBatch\u003e createColumnarReader(InputPartition partition)",
				"documentation": "/**\n * A variation on {@link PartitionReaderFactory} that returns {@link ContinuousPartitionReader}\n * instead of {@link PartitionReader}. It's used for continuous streaming processing.\n *\n * @since 3.0.0\n */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A {@link SparkDataStream} for streaming queries with continuous mode.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ContinuousStream",
		"extends": "org.apache.spark.sql.connector.read.streaming.SparkDataStream",
		"Methods": [
			{
				"signature": "default boolean needsReconfiguration()",
				"documentation": "/**\n   * The execution engine will call this method in every epoch to determine if new input\n   * partitions need to be generated, which may be required if for example the underlying\n   * source system has had partitions added or removed.\n   * \u003cp\u003e\n   * If true, the Spark job to scan this continuous data stream will be interrupted and Spark will\n   * launch it again with a new list of {@link InputPartition input partitions}.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A {@link SparkDataStream} for streaming queries with micro-batch mode.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.MicroBatchStream",
		"extends": "org.apache.spark.sql.connector.read.streaming.SparkDataStream",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An abstract representation of progress through a {@link MicroBatchStream} or\n * {@link ContinuousStream}.\n * \u003cp\u003e\n * During execution, offsets provided by the data source implementation will be logged and used as\n * restart checkpoints. Each source should provide an offset implementation which the source can use\n * to reconstruct a position in the stream up to which data has been seen/processed.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.Offset",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public boolean equals(Object obj)",
				"documentation": "/**\n     * Equality based on JSON string representation. We leverage the\n     * JSON representation for normalization between the Offset's\n     * in deserialized and serialized representations.\n     */"
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Used for per-partition offsets in continuous processing. ContinuousReader implementations will\n * provide a method to merge these into a global Offset.\n * \u003cp\u003e\n * These offsets must be serializable.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.PartitionOffset",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} must scan all the data\n * available at the streaming source. This is meant to be a hard specification as being able\n * to return all available data is necessary for {@code Trigger.Once()} to work correctly.\n * If a source is unable to scan all available data, then it must throw an error.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ReadAllAvailable",
		"extends": "",
		"Methods": [
			{
				"signature": "private ReadAllAvailable()",
				"documentation": "/**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} must scan all the data\n * available at the streaming source. This is meant to be a hard specification as being able\n * to return all available data is necessary for {@code Trigger.Once()} to work correctly.\n * If a source is unable to scan all available data, then it must throw an error.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.0.0\n */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.streaming.ReadLimit"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface representing limits on how much to read from a {@link MicroBatchStream} when it\n * implements {@link SupportsAdmissionControl}. There are several child interfaces representing\n * various kinds of limits.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @see ReadAllAvailable\n * @see ReadMaxRows\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ReadLimit",
		"extends": "",
		"Methods": [
			{
				"signature": "static ReadLimit minRows(long rows, long maxTriggerDelayMs)",
				"documentation": "/**\n * Interface representing limits on how much to read from a {@link MicroBatchStream} when it\n * implements {@link SupportsAdmissionControl}. There are several child interfaces representing\n * various kinds of limits.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @see ReadAllAvailable\n * @see ReadMaxRows\n * @since 3.0.0\n */"
			},
			{
				"signature": "static ReadLimit maxRows(long rows)",
				"documentation": ""
			},
			{
				"signature": "static ReadLimit maxFiles(int files)",
				"documentation": ""
			},
			{
				"signature": "static ReadLimit allAvailable()",
				"documentation": ""
			},
			{
				"signature": "static ReadLimit compositeLimit(ReadLimit[] readLimits)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.connector.read.streaming.CompositeReadLimit",
			"org.apache.spark.sql.connector.read.streaming.ReadAllAvailable",
			"org.apache.spark.sql.connector.read.streaming.ReadMaxFiles",
			"org.apache.spark.sql.connector.read.streaming.ReadMaxRows",
			"org.apache.spark.sql.connector.read.streaming.ReadMinRows"
		],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} should scan approximately the\n * given maximum number of files.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ReadMaxFiles",
		"extends": "",
		"Methods": [
			{
				"signature": "ReadMaxFiles(int maxFiles)",
				"documentation": "/**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} should scan approximately the\n * given maximum number of files.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.0.0\n */"
			},
			{
				"signature": "public int maxFiles()",
				"documentation": "/** Approximate maximum rows to scan. */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.streaming.ReadLimit"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} should scan approximately the\n * given maximum number of rows.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ReadMaxRows",
		"extends": "",
		"Methods": [
			{
				"signature": "ReadMaxRows(long rows)",
				"documentation": "/**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} should scan approximately the\n * given maximum number of rows.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.0.0\n */"
			},
			{
				"signature": "public long maxRows()",
				"documentation": "/** Approximate maximum rows to scan. */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.streaming.ReadLimit"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} should scan approximately\n * at least the given minimum number of rows.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ReadMinRows",
		"extends": "",
		"Methods": [
			{
				"signature": "ReadMinRows(long rows, long maxTriggerDelayMs)",
				"documentation": "/**\n * Represents a {@link ReadLimit} where the {@link MicroBatchStream} should scan approximately\n * at least the given minimum number of rows.\n *\n * @see SupportsAdmissionControl#latestOffset(Offset, ReadLimit)\n * @since 3.2.0\n */"
			},
			{
				"signature": "public long minRows()",
				"documentation": "/** Approximate minimum rows to scan. */"
			},
			{
				"signature": "public long maxTriggerDelayMs()",
				"documentation": "/** Approximate maximum trigger delay. */"
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.streaming.ReadLimit"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for streaming sinks to signal that they can report\n * metrics.\n *\n * @since 3.4.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ReportsSinkMetrics",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link SparkDataStream} streaming sources to signal that they can report\n * metrics.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics",
		"extends": "org.apache.spark.sql.connector.read.streaming.SparkDataStream",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The base interface representing a readable data stream in a Spark streaming query. It's\n * responsible to manage the offsets of the streaming source in the streaming query.\n * \u003cp\u003e\n * Data sources should implement concrete data stream interfaces:\n * {@link MicroBatchStream} and {@link ContinuousStream}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.SparkDataStream",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.read.streaming.AcceptsLatestSeenOffset",
			"org.apache.spark.sql.connector.read.streaming.ContinuousStream",
			"org.apache.spark.sql.connector.read.streaming.MicroBatchStream",
			"org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics",
			"org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mix-in interface for {@link SparkDataStream} streaming sources to signal that they can control\n * the rate of data ingested into the system. These rate limits can come implicitly from the\n * contract of triggers, e.g. Trigger.Once() requires that a micro-batch process all data\n * available to the system at the start of the micro-batch. Alternatively, sources can decide to\n * limit ingest through data source options.\n * \u003cp\u003e\n * Through this interface, a MicroBatchStream should be able to return the next offset that it will\n * process until given a {@link ReadLimit}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl",
		"extends": "org.apache.spark.sql.connector.read.streaming.SparkDataStream",
		"Methods": [
			{
				"signature": "default ReadLimit getDefaultReadLimit()",
				"documentation": "/**\n   * Returns the read limits potentially passed to the data source through options when creating\n   * the data source.\n   */"
			},
			{
				"signature": "default Offset reportLatestOffset()",
				"documentation": "/**\n   * Returns the most recent offset available.\n   * \u003cp\u003e\n   * The source can return `null`, if there is no data to process or the source does not support\n   * to this method.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface for streaming sources that supports running in Trigger.AvailableNow mode, which\n * will process all the available data at the beginning of the query in (possibly) multiple batches.\n *\n * This mode will have better scalability comparing to Trigger.Once mode.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow",
		"extends": "org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The builder to generate SQL from V2 expressions.\n */",
		"name": "org.apache.spark.sql.connector.util.V2ExpressionSQLBuilder",
		"extends": "",
		"Methods": [
			{
				"signature": "public String build(Expression expr)",
				"documentation": "/**\n * The builder to generate SQL from V2 expressions.\n */"
			},
			{
				"signature": "case \"IN\":",
				"documentation": ""
			},
			{
				"signature": "case \"CASE_WHEN\":",
				"documentation": ""
			},
			{
				"signature": "protected String visitLiteral(Literal literal)",
				"documentation": ""
			},
			{
				"signature": "protected String visitNamedReference(NamedReference namedRef)",
				"documentation": ""
			},
			{
				"signature": "protected String visitIn(String v, List\u003cString\u003e list)",
				"documentation": ""
			},
			{
				"signature": "protected String visitIsNull(String v)",
				"documentation": ""
			},
			{
				"signature": "protected String visitIsNotNull(String v)",
				"documentation": ""
			},
			{
				"signature": "protected String visitStartsWith(String l, String r)",
				"documentation": ""
			},
			{
				"signature": "protected String visitEndsWith(String l, String r)",
				"documentation": ""
			},
			{
				"signature": "protected String visitContains(String l, String r)",
				"documentation": ""
			},
			{
				"signature": "private String inputToSQL(Expression input)",
				"documentation": ""
			},
			{
				"signature": "protected String visitBinaryComparison(String name, String l, String r)",
				"documentation": ""
			},
			{
				"signature": "protected String visitBinaryArithmetic(String name, String l, String r)",
				"documentation": ""
			},
			{
				"signature": "protected String visitCast(String l, DataType dataType)",
				"documentation": ""
			},
			{
				"signature": "protected String visitAnd(String name, String l, String r)",
				"documentation": ""
			},
			{
				"signature": "protected String visitOr(String name, String l, String r)",
				"documentation": ""
			},
			{
				"signature": "protected String visitNot(String v)",
				"documentation": ""
			},
			{
				"signature": "protected String visitUnaryArithmetic(String name, String v)",
				"documentation": ""
			},
			{
				"signature": "protected String visitCaseWhen(String[] children)",
				"documentation": ""
			},
			{
				"signature": "protected String visitSQLFunction(String funcName, String[] inputs)",
				"documentation": ""
			},
			{
				"signature": "protected String visitUnexpectedExpr(Expression expr) throws IllegalArgumentException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.connector.expressions.GeneralScalarExpression"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface that defines how to write the data to data source for batch processing.\n * \u003cp\u003e\n * The writing procedure is:\n * \u003col\u003e\n *   \u003cli\u003eCreate a writer factory by {@link #createBatchWriterFactory(PhysicalWriteInfo)}, serialize\n *     and send it to all the partitions of the input data(RDD).\u003c/li\u003e\n *   \u003cli\u003eFor each partition, create the data writer, and write the data of the partition with this\n *     writer. If all the data are written successfully, call {@link DataWriter#commit()}. If\n *     exception happens during the writing, call {@link DataWriter#abort()}.\u003c/li\u003e\n *   \u003cli\u003eIf all writers are successfully committed, call {@link #commit(WriterCommitMessage[])}. If\n *     some writers are aborted, or the job failed with an unknown reason, call\n *     {@link #abort(WriterCommitMessage[])}.\u003c/li\u003e\n * \u003c/ol\u003e\n * \u003cp\u003e\n * While Spark will retry failed writing tasks, Spark won't retry failed writing jobs. Users should\n * do it manually in their Spark applications if they want to retry.\n * \u003cp\u003e\n * Please refer to the documentation of commit/abort methods for detailed specifications.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.BatchWrite",
		"extends": "",
		"Methods": [
			{
				"signature": "default boolean useCommitCoordinator()",
				"documentation": "/**\n   * Returns whether Spark should use the commit coordinator to ensure that at most one task for\n   * each partition commits.\n   *\n   * @return true if commit coordinator should be used, false otherwise.\n   */"
			},
			{
				"signature": "default void onDataWriterCommit(WriterCommitMessage message)",
				"documentation": "/**\n   * Handles a commit message on receiving from a successful data writer.\n   *\n   * If this method fails (by throwing an exception), this writing job is considered to to have been\n   * failed, and {@link #abort(WriterCommitMessage[])} would be called.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A data writer returned by {@link DataWriterFactory#createWriter(int, long)} and is\n * responsible for writing data for an input RDD partition.\n * \u003cp\u003e\n * One Spark task has one exclusive data writer, so there is no thread-safe concern.\n * \u003cp\u003e\n * {@link #write(Object)} is called for each record in the input RDD partition. If one record fails\n * the {@link #write(Object)}, {@link #abort()} is called afterwards and the remaining records will\n * not be processed. If all records are successfully written, {@link #commit()} is called.\n * \u003cp\u003e\n * Once a data writer returns successfully from {@link #commit()} or {@link #abort()}, Spark will\n * call {@link #close()} to let DataWriter doing resource cleanup. After calling {@link #close()},\n * its lifecycle is over and Spark will not use it again.\n * \u003cp\u003e\n * If this data writer succeeds(all records are successfully written and {@link #commit()}\n * succeeds), a {@link WriterCommitMessage} will be sent to the driver side and pass to\n * {@link BatchWrite#commit(WriterCommitMessage[])} with commit messages from other data\n * writers. If this data writer fails(one record fails to write or {@link #commit()} fails), an\n * exception will be sent to the driver side, and Spark may retry this writing task a few times.\n * In each retry, {@link DataWriterFactory#createWriter(int, long)} will receive a\n * different `taskId`. Spark will call {@link BatchWrite#abort(WriterCommitMessage[])}\n * when the configured number of retries is exhausted.\n * \u003cp\u003e\n * Besides the retry mechanism, Spark may launch speculative tasks if the existing writing task\n * takes too long to finish. Different from retried tasks, which are launched one by one after the\n * previous one fails, speculative tasks are running simultaneously. It's possible that one input\n * RDD partition has multiple data writers with different `taskId` running at the same time,\n * and data sources should guarantee that these data writers don't conflict and can work together.\n * Implementations can coordinate with driver during {@link #commit()} to make sure only one of\n * these data writers can commit successfully. Or implementations can allow all of them to commit\n * successfully, and have a way to revert committed data writers without the commit message, because\n * Spark only accepts the commit message that arrives first and ignore others.\n * \u003cp\u003e\n * Note that, Currently the type {@code T} can only be\n * {@link org.apache.spark.sql.catalyst.InternalRow}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.DataWriter",
		"extends": "java.io.Closeable",
		"Methods": [
			{
				"signature": "default CustomTaskMetric[] currentMetricsValues()",
				"documentation": "/**\n   * Returns an array of custom task metrics. By default it returns empty array. Note that it is\n   * not recommended to put heavy logic in this method as it may affect writing performance.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A factory of {@link DataWriter} returned by\n * {@link BatchWrite#createBatchWriterFactory(PhysicalWriteInfo)}, which is responsible for\n * creating and initializing the actual data writer at executor side.\n * \u003cp\u003e\n * Note that, the writer factory will be serialized and sent to executors, then the data writer\n * will be created on executors and do the actual writing. So this interface must be\n * serializable and {@link DataWriter} doesn't need to be.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.DataWriterFactory",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This interface contains logical write information that data sources can use when generating a\n * {@link WriteBuilder}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.LogicalWriteInfo",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This interface contains physical write information that data sources can use when\n * generating a {@link DataWriterFactory} or a {@link StreamingDataWriterFactory}.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.PhysicalWriteInfo",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A write that requires a specific distribution and ordering of data.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.write.RequiresDistributionAndOrdering",
		"extends": "org.apache.spark.sql.connector.write.Write",
		"Methods": [
			{
				"signature": "default int requiredNumPartitions()",
				"documentation": "/**\n   * Returns the number of partitions required by this write.\n   * \u003cp\u003e\n   * Implementations may override this to require a specific number of input partitions.\n   * \u003cp\u003e\n   * Note that Spark doesn't support the number of partitions on {@link UnspecifiedDistribution},\n   * the query will fail if the number of partitions are provided but the distribution is\n   * unspecified.\n   *\n   * @return the required number of partitions, any value less than 1 mean no requirement.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A logical representation of a data source DELETE, UPDATE, or MERGE operation that requires\n * rewriting data.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.write.RowLevelOperation",
		"extends": "",
		"Methods": [
			{
				"signature": "default String description()",
				"documentation": "/**\n   * Returns the description associated with this row-level operation.\n   */"
			},
			{
				"signature": "default NamedReference[] requiredMetadataAttributes()",
				"documentation": "/**\n   * Returns metadata attributes that are required to perform this row-level operation.\n   * \u003cp\u003e\n   * Data sources that can use this method to project metadata columns needed for writing\n   * the data back (e.g. metadata columns for grouping data).\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.connector.write.Command"
		]
	},
	{
		"documentation": "/**\n   * A row-level SQL command.\n   */",
		"name": "org.apache.spark.sql.connector.write.Command",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface for building a {@link RowLevelOperation}.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.write.RowLevelOperationBuilder",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface with logical information for a row-level operation such as DELETE, UPDATE, MERGE.\n *\n * @since 3.3.0\n */",
		"name": "org.apache.spark.sql.connector.write.RowLevelOperationInfo",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Write builder trait for tables that support dynamic partition overwrite.\n * \u003cp\u003e\n * A write that dynamically overwrites partitions removes all existing data in each logical\n * partition for which the write will commit new data. Any existing logical partition for which the\n * write does not contain data will remain unchanged.\n * \u003cp\u003e\n * This is provided to implement SQL compatible with Hive table operations but is not recommended.\n * Instead, use the {@link SupportsOverwrite overwrite by filter API} to explicitly replace data.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.SupportsDynamicOverwrite",
		"extends": "org.apache.spark.sql.connector.write.WriteBuilder",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Write builder trait for tables that support overwrite by filter.\n * \u003cp\u003e\n * Overwriting data by filter will delete any data that matches the filter and replace it with data\n * that is committed in the write.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.SupportsOverwrite",
		"extends": "WriteBuilder,",
		"Methods": [
			{
				"signature": "@Override\n  default WriteBuilder truncate()",
				"documentation": "/**\n   * Configures a write to replace data matching the filters with data committed in the write.\n   * \u003cp\u003e\n   * Rows must be deleted from the data source if and only if all of the filters match. That is,\n   * filters must be interpreted as ANDed together.\n   *\n   * @param filters filters used to match data to overwrite\n   * @return this write builder for method chaining\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Write builder trait for tables that support truncation.\n * \u003cp\u003e\n * Truncation removes all data in a table and replaces it with data that is committed in the write.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.SupportsTruncate",
		"extends": "org.apache.spark.sql.connector.write.WriteBuilder",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A logical representation of a data source write.\n * \u003cp\u003e\n * This logical representation is shared between batch and streaming write. Data sources must\n * implement the corresponding methods in this interface to match what the table promises\n * to support. For example, {@link #toBatch()} must be implemented if the {@link Table} that\n * creates this {@link Write} returns {@link TableCapability#BATCH_WRITE} support in its\n * {@link Table#capabilities()}.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.write.Write",
		"extends": "",
		"Methods": [
			{
				"signature": "default String description()",
				"documentation": "/**\n   * Returns the description associated with this write.\n   */"
			},
			{
				"signature": "default BatchWrite toBatch()",
				"documentation": "/**\n   * Returns a {@link BatchWrite} to write data to batch source. By default this method throws\n   * exception, data sources must overwrite this method to provide an implementation, if the\n   * {@link Table} that creates this write returns {@link TableCapability#BATCH_WRITE} support in\n   * its {@link Table#capabilities()}.\n   */"
			},
			{
				"signature": "default StreamingWrite toStreaming()",
				"documentation": "/**\n   * Returns a {@link StreamingWrite} to write data to streaming source. By default this method\n   * throws exception, data sources must overwrite this method to provide an implementation, if the\n   * {@link Table} that creates this write returns {@link TableCapability#STREAMING_WRITE} support\n   * in its {@link Table#capabilities()}.\n   */"
			},
			{
				"signature": "default CustomMetric[] supportedCustomMetrics()",
				"documentation": "/**\n   * Returns an array of supported custom metrics with name and description.\n   * By default it returns empty array.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.write.RequiresDistributionAndOrdering",
			"org.apache.spark.sql.connector.write.V1Write"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface for building the {@link Write}. Implementations can mix in some interfaces to\n * support different ways to write data to data sources.\n * \u003cp\u003e\n * Unless modified by a mixin interface, the {@link Write} configured by this builder is to\n * append data without affecting existing data.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.WriteBuilder",
		"extends": "",
		"Methods": [
			{
				"signature": "default Write build()",
				"documentation": "/**\n   * Returns a logical {@link Write} shared between batch and streaming.\n   *\n   * @since 3.2.0\n   */"
			},
			{
				"signature": "@Override\n      public BatchWrite toBatch()",
				"documentation": "/**\n   * Returns a logical {@link Write} shared between batch and streaming.\n   *\n   * @since 3.2.0\n   */"
			},
			{
				"signature": "@Override\n      public StreamingWrite toStreaming()",
				"documentation": ""
			},
			{
				"signature": "@Deprecated\n  default BatchWrite buildForBatch()",
				"documentation": "/**\n   * Returns a {@link BatchWrite} to write data to batch source.\n   *\n   * @deprecated use {@link #build()} instead.\n   */"
			},
			{
				"signature": "@Deprecated\n  default StreamingWrite buildForStreaming()",
				"documentation": "/**\n   * Returns a {@link StreamingWrite} to write data to streaming source.\n   *\n   * @deprecated use {@link #build()} instead.\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.connector.write.SupportsDynamicOverwrite",
			"org.apache.spark.sql.connector.write.SupportsTruncate"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A commit message returned by {@link DataWriter#commit()} and will be sent back to the driver side\n * as the input parameter of {@link BatchWrite#commit(WriterCommitMessage[])} or\n * {@link StreamingWrite#commit(long, WriterCommitMessage[])}.\n * \u003cp\u003e\n * This is an empty interface, data sources should define their own message class and use it when\n * generating messages at executor side and handling the messages at driver side.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.WriterCommitMessage",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A factory of {@link DataWriter} returned by\n * {@link StreamingWrite#createStreamingWriterFactory(PhysicalWriteInfo)}, which is responsible for\n * creating and initializing the actual data writer at executor side.\n * \u003cp\u003e\n * Note that, the writer factory will be serialized and sent to executors, then the data writer\n * will be created on executors and do the actual writing. So this interface must be\n * serializable and {@link DataWriter} doesn't need to be.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.streaming.StreamingDataWriterFactory",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface that defines how to write the data to data source in streaming queries.\n *\n * The writing procedure is:\n * \u003col\u003e\n *   \u003cli\u003eCreate a writer factory by {@link #createStreamingWriterFactory(PhysicalWriteInfo)},\n *   serialize and send it to all the partitions of the input data(RDD).\u003c/li\u003e\n *   \u003cli\u003eFor each epoch in each partition, create the data writer, and write the data of the\n *   epoch in the partition with this writer. If all the data are written successfully, call\n *   {@link DataWriter#commit()}. If exception happens during the writing, call\n *   {@link DataWriter#abort()}.\u003c/li\u003e\n *   \u003cli\u003eIf writers in all partitions of one epoch are successfully committed, call\n *   {@link #commit(long, WriterCommitMessage[])}. If some writers are aborted, or the job failed\n *   with an unknown reason, call {@link #abort(long, WriterCommitMessage[])}.\u003c/li\u003e\n * \u003c/ol\u003e\n * \u003cp\u003e\n * While Spark will retry failed writing tasks, Spark won't retry failed writing jobs. Users should\n * do it manually in their Spark applications if they want to retry.\n * \u003cp\u003e\n * Please refer to the documentation of commit/abort methods for detailed specifications.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.write.streaming.StreamingWrite",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Represents the type of timeouts possible for the Dataset operations\n * {@code mapGroupsWithState} and {@code flatMapGroupsWithState}.\n * \u003cp\u003e\n * See documentation on {@code GroupState} for more details.\n *\n * @since 2.2.0\n */",
		"name": "org.apache.spark.sql.streaming.GroupStateTimeout",
		"extends": "",
		"Methods": [
			{
				"signature": "public static GroupStateTimeout ProcessingTimeTimeout()",
				"documentation": "/**\n   * Timeout based on processing time.\n   * \u003cp\u003e\n   * The duration of timeout can be set for each group in\n   * {@code map/flatMapGroupsWithState} by calling {@code GroupState.setTimeoutDuration()}.\n   * \u003cp\u003e\n   * See documentation on {@code GroupState} for more details.\n   */"
			},
			{
				"signature": "public static GroupStateTimeout EventTimeTimeout()",
				"documentation": "/**\n   * Timeout based on event-time.\n   * \u003cp\u003e\n   * The event-time timestamp for timeout can be set for each\n   * group in {@code map/flatMapGroupsWithState} by calling\n   * {@code GroupState.setTimeoutTimestamp()}.\n   * In addition, you have to define the watermark in the query using\n   * {@code Dataset.withWatermark}.\n   * When the watermark advances beyond the set timestamp of a group and the group has not\n   * received any data, then the group times out.\n   * \u003cp\u003e\n   * See documentation on {@code GroupState} for more details.\n   */"
			},
			{
				"signature": "public static GroupStateTimeout NoTimeout()",
				"documentation": "/** No timeout. */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization",
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * OutputMode describes what data will be written to a streaming sink when there is\n * new data available in a streaming DataFrame/Dataset.\n *\n * @since 2.0.0\n */",
		"name": "org.apache.spark.sql.streaming.OutputMode",
		"extends": "",
		"Methods": [
			{
				"signature": "public static OutputMode Append()",
				"documentation": "/**\n   * OutputMode in which only the new rows in the streaming DataFrame/Dataset will be\n   * written to the sink. This output mode can be only be used in queries that do not\n   * contain any aggregation.\n   *\n   * @since 2.0.0\n   */"
			},
			{
				"signature": "public static OutputMode Complete()",
				"documentation": "/**\n   * OutputMode in which all the rows in the streaming DataFrame/Dataset will be written\n   * to the sink every time there are some updates. This output mode can only be used in queries\n   * that contain aggregations.\n   *\n   * @since 2.0.0\n   */"
			},
			{
				"signature": "public static OutputMode Update()",
				"documentation": "/**\n   * OutputMode in which only the rows that were updated in the streaming DataFrame/Dataset will\n   * be written to the sink every time there are some updates. If the query doesn't contain\n   * aggregations, it will be equivalent to `Append` mode.\n   *\n   * @since 2.1.1\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization",
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * To get/create specific data type, users should use singleton objects and factory methods\n * provided by this class.\n *\n * @since 1.3.0\n */",
		"name": "org.apache.spark.sql.types.DataTypes",
		"extends": "",
		"Methods": [
			{
				"signature": "public static ArrayType createArrayType(DataType elementType)",
				"documentation": "/**\n   * Creates an ArrayType by specifying the data type of elements ({@code elementType}).\n   * The field of {@code containsNull} is set to {@code true}.\n   */"
			},
			{
				"signature": "public static ArrayType createArrayType(DataType elementType, boolean containsNull)",
				"documentation": "/**\n   * Creates an ArrayType by specifying the data type of elements ({@code elementType}) and\n   * whether the array contains null values ({@code containsNull}).\n   */"
			},
			{
				"signature": "public static DecimalType createDecimalType(int precision, int scale)",
				"documentation": "/**\n   * Creates a DecimalType by specifying the precision and scale.\n   */"
			},
			{
				"signature": "public static DecimalType createDecimalType()",
				"documentation": "/**\n   * Creates a DecimalType with default precision and scale, which are 10 and 0.\n   */"
			},
			{
				"signature": "public static DayTimeIntervalType createDayTimeIntervalType(byte startField, byte endField)",
				"documentation": "/**\n   * Creates a DayTimeIntervalType by specifying the start and end fields.\n   */"
			},
			{
				"signature": "public static DayTimeIntervalType createDayTimeIntervalType()",
				"documentation": "/**\n   * Creates a DayTimeIntervalType with default start and end fields: interval day to second.\n   */"
			},
			{
				"signature": "public static YearMonthIntervalType createYearMonthIntervalType(byte startField, byte endField)",
				"documentation": "/**\n   * Creates a YearMonthIntervalType by specifying the start and end fields.\n   */"
			},
			{
				"signature": "public static YearMonthIntervalType createYearMonthIntervalType()",
				"documentation": "/**\n   * Creates a YearMonthIntervalType with default start and end fields: interval year to month.\n   */"
			},
			{
				"signature": "public static MapType createMapType(DataType keyType, DataType valueType)",
				"documentation": "/**\n   * Creates a MapType by specifying the data type of keys ({@code keyType}) and values\n   * ({@code keyType}). The field of {@code valueContainsNull} is set to {@code true}.\n   */"
			},
			{
				"signature": "public static MapType createMapType(\n      DataType keyType,\n      DataType valueType,\n      boolean valueContainsNull)",
				"documentation": "/**\n   * Creates a MapType by specifying the data type of keys ({@code keyType}), the data type of\n   * values ({@code keyType}), and whether values contain any null value\n   * ({@code valueContainsNull}).\n   */"
			},
			{
				"signature": "public static StructField createStructField(\n      String name,\n      DataType dataType,\n      boolean nullable,\n      Metadata metadata)",
				"documentation": "/**\n   * Creates a StructField by specifying the name ({@code name}), data type ({@code dataType}) and\n   * whether values of this field can be null values ({@code nullable}).\n   */"
			},
			{
				"signature": "public static StructField createStructField(String name, DataType dataType, boolean nullable)",
				"documentation": "/**\n   * Creates a StructField with empty metadata.\n   *\n   * @see #createStructField(String, DataType, boolean, Metadata)\n   */"
			},
			{
				"signature": "public static StructType createStructType(List\u003cStructField\u003e fields)",
				"documentation": "/**\n   * Creates a StructType with the given list of StructFields ({@code fields}).\n   */"
			},
			{
				"signature": "public static StructType createStructType(StructField[] fields)",
				"documentation": "/**\n   * Creates a StructType with the given StructField array ({@code fields}).\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [
			"org.apache.spark.examples.ml.JavaStopWordsRemoverExample",
			"org.apache.spark.examples.sql.JavaSparkSQLExample",
			"org.apache.spark.ml.feature.JavaStopWordsRemoverSuite",
			"test.org.apache.spark.sql.JavaApplySchemaSuite",
			"test.org.apache.spark.sql.MyDoubleAvg"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ::DeveloperApi::\n * A user-defined type which can be automatically recognized by a SQLContext and registered.\n * WARNING: UDTs are currently only supported from Scala.\n */",
		"name": "org.apache.spark.sql.types.SQLUserDefinedType",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "annotation",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Case-insensitive map of string keys to string values.\n * \u003cp\u003e\n * This is used to pass options to v2 implementations to ensure consistent case insensitivity.\n * \u003cp\u003e\n * Methods that return keys in this map, like {@link #entrySet()} and {@link #keySet()}, return\n * keys converted to lower case. This map doesn't allow null key.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.util.CaseInsensitiveStringMap",
		"extends": "",
		"Methods": [
			{
				"signature": "public static CaseInsensitiveStringMap empty()",
				"documentation": "/**\n * Case-insensitive map of string keys to string values.\n * \u003cp\u003e\n * This is used to pass options to v2 implementations to ensure consistent case insensitivity.\n * \u003cp\u003e\n * Methods that return keys in this map, like {@link #entrySet()} and {@link #keySet()}, return\n * keys converted to lower case. This map doesn't allow null key.\n *\n * @since 3.0.0\n */"
			},
			{
				"signature": "public CaseInsensitiveStringMap(Map\u003cString, String\u003e originalMap)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int size()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isEmpty()",
				"documentation": ""
			},
			{
				"signature": "private String toLowerCase(Object key)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean containsKey(Object key)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean containsValue(Object value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String get(Object key)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String put(String key, String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String remove(Object key)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putAll(Map\u003c? extends String, ? extends String\u003e m)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void clear()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Set\u003cString\u003e keySet()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Collection\u003cString\u003e values()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Set\u003cMap.Entry\u003cString, String\u003e\u003e entrySet()",
				"documentation": ""
			},
			{
				"signature": "public boolean getBoolean(String key, boolean defaultValue)",
				"documentation": "/**\n   * Returns the boolean value to which the specified key is mapped,\n   * or defaultValue if there is no mapping for the key. The key match is case-insensitive.\n   */"
			},
			{
				"signature": "public int getInt(String key, int defaultValue)",
				"documentation": "/**\n   * Returns the integer value to which the specified key is mapped,\n   * or defaultValue if there is no mapping for the key. The key match is case-insensitive.\n   */"
			},
			{
				"signature": "public long getLong(String key, long defaultValue)",
				"documentation": "/**\n   * Returns the long value to which the specified key is mapped,\n   * or defaultValue if there is no mapping for the key. The key match is case-insensitive.\n   */"
			},
			{
				"signature": "public double getDouble(String key, double defaultValue)",
				"documentation": "/**\n   * Returns the double value to which the specified key is mapped,\n   * or defaultValue if there is no mapping for the key. The key match is case-insensitive.\n   */"
			},
			{
				"signature": "public Map\u003cString, String\u003e asCaseSensitiveMap()",
				"documentation": "/**\n   * Returns the original case-sensitive map.\n   */"
			},
			{
				"signature": "@Override\n  public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.Map"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A generic, re-usable histogram class that supports partial aggregations.\n * The algorithm is a heuristic adapted from the following paper:\n * Yael Ben-Haim and Elad Tom-Tov, \"A streaming parallel decision tree algorithm\",\n * J. Machine Learning Research 11 (2010), pp. 849--872. Although there are no approximation\n * guarantees, it appears to work well with adequate data and a large (e.g., 20-80) number\n * of histogram bins.\n *\n * Adapted from Hive's NumericHistogram. Can refer to\n * https://github.com/apache/hive/blob/master/ql/src/\n * java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java\n *\n * Differences:\n *   1. Declaring [[Coord]] and it's variables as public types for\n *      easy access in the HistogramNumeric class.\n *   2. Add method [[getNumBins()]] for serialize [[NumericHistogram]]\n *      in [[NumericHistogramSerializer]].\n *   3. Add method [[addBin()]] for deserialize [[NumericHistogram]]\n *      in [[NumericHistogramSerializer]].\n *   4. In Hive's code, the method [[merge()] pass a serialized histogram,\n *      in Spark, this method pass a deserialized histogram.\n *      Here we change the code about merge bins.\n */",
		"name": "org.apache.spark.sql.util.NumericHistogram",
		"extends": "",
		"Methods": [
			{
				"signature": "public NumericHistogram()",
				"documentation": "/**\n   * Creates a new histogram object. Note that the allocate() or merge()\n   * method must be called before the histogram can be used.\n   */"
			},
			{
				"signature": "public void reset()",
				"documentation": "/**\n   * Resets a histogram object to its initial state. allocate() or merge() must be\n   * called again before use.\n   */"
			},
			{
				"signature": "public int getNumBins()",
				"documentation": "/**\n   * Returns the number of bins.\n   */"
			},
			{
				"signature": "public int getUsedBins()",
				"documentation": "/**\n   * Returns the number of bins currently being used by the histogram.\n   */"
			},
			{
				"signature": "public void setUsedBins(int nusedBins)",
				"documentation": "/**\n   * Set the number of bins currently being used by the histogram.\n   */"
			},
			{
				"signature": "public boolean isReady()",
				"documentation": "/**\n   * Returns true if this histogram object has been initialized by calling merge()\n   * or allocate().\n   */"
			},
			{
				"signature": "public Coord getBin(int b)",
				"documentation": "/**\n   * Returns a particular histogram bin.\n   */"
			},
			{
				"signature": "public void addBin(double x, double y, int b)",
				"documentation": "/**\n   * Set a particular histogram bin with index.\n   */"
			},
			{
				"signature": "public void allocate(int num_bins)",
				"documentation": "/**\n   * Sets the number of histogram bins to use for approximating data.\n   *\n   * @param num_bins Number of non-uniform-width histogram bins to use\n   */"
			},
			{
				"signature": "public void merge(NumericHistogram other)",
				"documentation": "/**\n   * Takes a histogram and merges it with the current histogram object.\n   */"
			},
			{
				"signature": "public void add(double v)",
				"documentation": "/**\n   * Adds a new data point to the histogram approximation. Make sure you have\n   * called either allocate() or merge() first. This method implements Algorithm #1\n   * from Ben-Haim and Tom-Tov, \"A Streaming Parallel Decision Tree Algorithm\", JMLR 2010.\n   *\n   * @param v The data point to add to the histogram approximation.\n   */"
			},
			{
				"signature": "private void trim()",
				"documentation": "/**\n   * Trims a histogram down to 'nbins' bins by iteratively merging the closest bins.\n   * If two pairs of bins are equally close to each other, decide uniformly at random which\n   * pair to merge, based on a PRNG.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.util.NumericHistogram.Coord"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.util.NumericHistogram.Coord"
		]
	},
	{
		"documentation": "/**\n   * The Coord class defines a histogram bin, which is just an (x,y) pair.\n   */",
		"name": "org.apache.spark.sql.util.NumericHistogram.Coord",
		"extends": "",
		"Methods": [
			{
				"signature": "public int compareTo(Object other)",
				"documentation": "/**\n   * The Coord class defines a histogram bin, which is just an (x,y) pair.\n   */"
			}
		],
		"interfaces": [
			"Comparable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.util.NumericHistogram"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A column vector backed by Apache Arrow.\n */",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector",
		"extends": "org.apache.spark.sql.vectorized.ColumnVector",
		"Methods": [
			{
				"signature": "public ValueVector getValueVector()",
				"documentation": "/**\n * A column vector backed by Apache Arrow.\n */"
			},
			{
				"signature": "@Override\n  public boolean hasNull()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numNulls()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ArrowColumnVector getChild(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "ArrowColumnVector(DataType type)",
				"documentation": ""
			},
			{
				"signature": "public ArrowColumnVector(ValueVector vector)",
				"documentation": ""
			},
			{
				"signature": "void initAccessor(ValueVector vector)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi",
			"org.apache.spark.unsafe.types.UTF8String",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ArrayAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.StructAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.MapAccessor"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.BooleanAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ByteAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ShortAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.IntAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.LongAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.FloatAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.DoubleAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.DecimalAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.StringAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.BinaryAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.DateAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.TimestampAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.TimestampNTZAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ArrayAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.StructAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.MapAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.NullAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.IntervalYearAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.DurationAccessor"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"extends": "",
		"Methods": [
			{
				"signature": "ArrowVectorAccessor(ValueVector vector)",
				"documentation": ""
			},
			{
				"signature": "boolean isNullAt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "final int getNullCount()",
				"documentation": ""
			},
			{
				"signature": "final void close()",
				"documentation": ""
			},
			{
				"signature": "boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "UTF8String getUTF8String(int rowId)",
				"documentation": ""
			},
			{
				"signature": "byte[] getBinary(int rowId)",
				"documentation": ""
			},
			{
				"signature": "ColumnarArray getArray(int rowId)",
				"documentation": ""
			},
			{
				"signature": "ColumnarMap getMap(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.vectorized.ArrowColumnVector.BooleanAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ByteAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ShortAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.IntAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.LongAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.FloatAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.DoubleAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.DecimalAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.StringAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.BinaryAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.DateAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.TimestampAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.TimestampNTZAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.ArrayAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.StructAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.MapAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.NullAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.IntervalYearAccessor",
			"org.apache.spark.sql.vectorized.ArrowColumnVector.DurationAccessor"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.vectorized.ArrowColumnVector"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.BooleanAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "BooleanAccessor(BitVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final boolean getBoolean(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.ByteAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "ByteAccessor(TinyIntVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final byte getByte(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.ShortAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "ShortAccessor(SmallIntVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final short getShort(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.IntAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "IntAccessor(IntVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final int getInt(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.LongAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "LongAccessor(BigIntVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final long getLong(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.FloatAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "FloatAccessor(Float4Vector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final float getFloat(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.DoubleAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "DoubleAccessor(Float8Vector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final double getDouble(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.DecimalAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "DecimalAccessor(DecimalVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.StringAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "StringAccessor(VarCharVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final UTF8String getUTF8String(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.BinaryAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "BinaryAccessor(VarBinaryVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final byte[] getBinary(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.DateAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "DateAccessor(DateDayVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final int getInt(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.TimestampAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "TimestampAccessor(TimeStampMicroTZVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final long getLong(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.TimestampNTZAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "TimestampNTZAccessor(TimeStampMicroVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final long getLong(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrayAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "ArrayAccessor(ListVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final boolean isNullAt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final ColumnarArray getArray(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.vectorized.ArrowColumnVector"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * Any call to \"get\" method will throw UnsupportedOperationException.\n   *\n   * Access struct values in a ArrowColumnVector doesn't use this accessor. Instead, it uses\n   * getStruct() method defined in the parent class. Any call to \"get\" method in this class is a\n   * bug in the code.\n   *\n   */",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.StructAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "StructAccessor(StructVector vector)",
				"documentation": "/**\n   * Any call to \"get\" method will throw UnsupportedOperationException.\n   *\n   * Access struct values in a ArrowColumnVector doesn't use this accessor. Instead, it uses\n   * getStruct() method defined in the parent class. Any call to \"get\" method in this class is a\n   * bug in the code.\n   *\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.vectorized.ArrowColumnVector"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.MapAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "MapAccessor(MapVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final ColumnarMap getMap(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.vectorized.ArrowColumnVector"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.NullAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "NullAccessor(NullVector vector)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.IntervalYearAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "IntervalYearAccessor(IntervalYearVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    int getInt(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.vectorized.ArrowColumnVector.DurationAccessor",
		"extends": "org.apache.spark.sql.vectorized.ArrowColumnVector.ArrowVectorAccessor",
		"Methods": [
			{
				"signature": "DurationAccessor(DurationVector vector)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    final long getLong(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An interface representing in-memory columnar data in Spark. This interface defines the main APIs\n * to access the data, as well as their batched versions. The batched versions are considered to be\n * faster and preferable whenever possible.\n * \u003cp\u003e\n * Most of the APIs take the rowId as a parameter. This is the batch local 0-based row id for values\n * in this ColumnVector.\n * \u003cp\u003e\n * Spark only calls specific {@code get} method according to the data type of this\n * {@link ColumnVector},\n * e.g. if it's int type, Spark is guaranteed to only call {@link #getInt(int)} or\n * {@link #getInts(int, int)}.\n * \u003cp\u003e\n * ColumnVector supports all the data types including nested types. To handle nested types,\n * ColumnVector can have children and is a tree structure. Please refer to {@link #getStruct(int)},\n * {@link #getArray(int)} and {@link #getMap(int)} for the details about how to implement nested\n * types.\n * \u003cp\u003e\n * ColumnVector is expected to be reused during the entire data loading process, to avoid allocating\n * memory again and again.\n * \u003cp\u003e\n * ColumnVector is meant to maximize CPU efficiency but not to minimize storage footprint.\n * Implementations should prefer computing efficiency over storage efficiency when design the\n * format. Since it is expected to reuse the ColumnVector instance while loading data, the storage\n * footprint is negligible.\n */",
		"name": "org.apache.spark.sql.vectorized.ColumnVector",
		"extends": "",
		"Methods": [
			{
				"signature": "public final DataType dataType()",
				"documentation": "/**\n   * Returns the data type of this column vector.\n   */"
			},
			{
				"signature": "public boolean[] getBooleans(int rowId, int count)",
				"documentation": "/**\n   * Gets boolean type values from {@code [rowId, rowId + count)}. The return values for the null\n   * slots are undefined and can be anything.\n   */"
			},
			{
				"signature": "public byte[] getBytes(int rowId, int count)",
				"documentation": "/**\n   * Gets byte type values from {@code [rowId, rowId + count)}. The return values for the null slots\n   * are undefined and can be anything.\n   */"
			},
			{
				"signature": "public short[] getShorts(int rowId, int count)",
				"documentation": "/**\n   * Gets short type values from {@code [rowId, rowId + count)}. The return values for the null\n   * slots are undefined and can be anything.\n   */"
			},
			{
				"signature": "public int[] getInts(int rowId, int count)",
				"documentation": "/**\n   * Gets int type values from {@code [rowId, rowId + count)}. The return values for the null slots\n   * are undefined and can be anything.\n   */"
			},
			{
				"signature": "public long[] getLongs(int rowId, int count)",
				"documentation": "/**\n   * Gets long type values from {@code [rowId, rowId + count)}. The return values for the null slots\n   * are undefined and can be anything.\n   */"
			},
			{
				"signature": "public float[] getFloats(int rowId, int count)",
				"documentation": "/**\n   * Gets float type values from {@code [rowId, rowId + count)}. The return values for the null\n   * slots are undefined and can be anything.\n   */"
			},
			{
				"signature": "public double[] getDoubles(int rowId, int count)",
				"documentation": "/**\n   * Gets double type values from {@code [rowId, rowId + count)}. The return values for the null\n   * slots are undefined and can be anything.\n   */"
			},
			{
				"signature": "public final ColumnarRow getStruct(int rowId)",
				"documentation": "/**\n   * Returns the struct type value for {@code rowId}. If the slot for {@code rowId} is null, it\n   * should return null.\n   * \u003cp\u003e\n   * To support struct type, implementations must implement {@link #getChild(int)} and make this\n   * vector a tree structure. The number of child vectors must be same as the number of fields of\n   * the struct type, and each child vector is responsible to store the data for its corresponding\n   * struct field.\n   */"
			},
			{
				"signature": "public final CalendarInterval getInterval(int rowId)",
				"documentation": "/**\n   * Returns the calendar interval type value for {@code rowId}. If the slot for\n   * {@code rowId} is null, it should return null.\n   * \u003cp\u003e\n   * In Spark, calendar interval type value is basically two integer values representing the number\n   * of months and days in this interval, and a long value representing the number of microseconds\n   * in this interval. An interval type vector is the same as a struct type vector with 3 fields:\n   * {@code months}, {@code days} and {@code microseconds}.\n   * \u003cp\u003e\n   * To support interval type, implementations must implement {@link #getChild(int)} and define 3\n   * child vectors: the first child vector is an int type vector, containing all the month values of\n   * all the interval values in this vector. The second child vector is an int type vector,\n   * containing all the day values of all the interval values in this vector. The third child vector\n   * is a long type vector, containing all the microsecond values of all the interval values in this\n   * vector.\n   */"
			},
			{
				"signature": "protected ColumnVector(DataType type)",
				"documentation": "/**\n   * Sets up the data type of this column vector.\n   */"
			}
		],
		"interfaces": [
			"AutoCloseable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.vectorized.ArrowColumnVector"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.unsafe.types.CalendarInterval"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Array abstraction in {@link ColumnVector}.\n */",
		"name": "org.apache.spark.sql.vectorized.ColumnarArray",
		"extends": "org.apache.spark.sql.catalyst.util.ArrayData",
		"Methods": [
			{
				"signature": "public ColumnarArray(ColumnVector data, int offset, int length)",
				"documentation": "/**\n * Array abstraction in {@link ColumnVector}.\n */"
			},
			{
				"signature": "@Override\n  public int numElements()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ArrayData copy()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean[] toBooleanArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] toByteArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short[] toShortArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int[] toIntArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long[] toLongArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float[] toFloatArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double[] toDoubleArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object[] array()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int ordinal, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public CalendarInterval getInterval(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarRow getStruct(int ordinal, int numFields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object get(int ordinal, DataType dataType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void update(int ordinal, Object value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNullAt(int ordinal)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.sql.catalyst.expressions.SpecializedGettersReader",
			"org.apache.spark.sql.catalyst.expressions.UnsafeArrayData"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.orc.OrcArrayColumnVector",
			"org.apache.spark.sql.execution.vectorized.WritableColumnVector"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class wraps multiple ColumnVectors as a row-wise table. It provides a row view of this\n * batch so that Spark can access the data row by row. Instance of it is meant to be reused during\n * the entire data loading process. A data source may extend this class with customized logic.\n */",
		"name": "org.apache.spark.sql.vectorized.ColumnarBatch",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public void close()",
				"documentation": "/**\n   * Called to close all the columns in this batch. It is not valid to access the data after\n   * calling this. This must be called at the end to clean up memory allocations.\n   */"
			},
			{
				"signature": "public Iterator\u003cInternalRow\u003e rowIterator()",
				"documentation": "/**\n   * Returns an iterator over the rows in this batch.\n   */"
			},
			{
				"signature": "@Override\n      public boolean hasNext()",
				"documentation": "/**\n   * Returns an iterator over the rows in this batch.\n   */"
			},
			{
				"signature": "@Override\n      public InternalRow next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void remove()",
				"documentation": ""
			},
			{
				"signature": "public void setNumRows(int numRows)",
				"documentation": "/**\n   * Sets the number of rows in this batch.\n   */"
			},
			{
				"signature": "public int numCols()",
				"documentation": "/**\n   * Returns the number of columns that make up this batch.\n   */"
			},
			{
				"signature": "public int numRows()",
				"documentation": "/**\n   * Returns the number of rows for read, including filtered rows.\n   */"
			},
			{
				"signature": "public ColumnVector column(int ordinal)",
				"documentation": "/**\n   * Returns the column at `ordinal`.\n   */"
			},
			{
				"signature": "public InternalRow getRow(int rowId)",
				"documentation": "/**\n   * Returns the row in this batch at `rowId`. Returned row is reused across calls.\n   */"
			},
			{
				"signature": "public ColumnarBatch(ColumnVector[] columns)",
				"documentation": ""
			},
			{
				"signature": "public ColumnarBatch(ColumnVector[] columns, int numRows)",
				"documentation": "/**\n   * Create a new batch from existing column vectors.\n   * @param columns The columns of this batch\n   * @param numRows The number of rows in this batch\n   */"
			}
		],
		"interfaces": [
			"AutoCloseable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader",
			"org.apache.spark.sql.execution.vectorized.ColumnVectorUtils",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.ColumnarReaderFactory"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class wraps an array of {@link ColumnVector} and provides a row view.\n */",
		"name": "org.apache.spark.sql.vectorized.ColumnarBatchRow",
		"extends": "org.apache.spark.sql.catalyst.InternalRow",
		"Methods": [
			{
				"signature": "public ColumnarBatchRow(ColumnVector[] columns)",
				"documentation": "/**\n * This class wraps an array of {@link ColumnVector} and provides a row view.\n */"
			},
			{
				"signature": "@Override\n  public int numFields()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public InternalRow copy()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean anyNull()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int ordinal, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public CalendarInterval getInterval(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarRow getStruct(int ordinal, int numFields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object get(int ordinal, DataType dataType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void update(int ordinal, Object value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNullAt(int ordinal)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Map abstraction in {@link ColumnVector}.\n */",
		"name": "org.apache.spark.sql.vectorized.ColumnarMap",
		"extends": "org.apache.spark.sql.catalyst.util.MapData",
		"Methods": [
			{
				"signature": "public ColumnarMap(ColumnVector keys, ColumnVector values, int offset, int length)",
				"documentation": "/**\n * Map abstraction in {@link ColumnVector}.\n */"
			},
			{
				"signature": "@Override\n  public int numElements()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray keyArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray valueArray()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public MapData copy()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.orc.OrcMapColumnVector",
			"org.apache.spark.sql.execution.vectorized.WritableColumnVector"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Row abstraction in {@link ColumnVector}.\n */",
		"name": "org.apache.spark.sql.vectorized.ColumnarRow",
		"extends": "org.apache.spark.sql.catalyst.InternalRow",
		"Methods": [
			{
				"signature": "public ColumnarRow(ColumnVector data, int rowId)",
				"documentation": "/**\n * Row abstraction in {@link ColumnVector}.\n */"
			},
			{
				"signature": "@Override\n  public int numFields()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public InternalRow copy()",
				"documentation": "/**\n   * Revisit this. This is expensive. This is currently only used in test paths.\n   */"
			},
			{
				"signature": "@Override\n  public boolean anyNull()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int ordinal, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public CalendarInterval getInterval(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarRow getStruct(int ordinal, int numFields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object get(int ordinal, DataType dataType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void update(int ordinal, Object value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNullAt(int ordinal)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.catalyst.expressions.HiveHasherSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testKnownIntegerInputs()",
				"documentation": ""
			},
			{
				"signature": "int[] inputs =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKnownLongInputs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKnownStringAndIntInputs()",
				"documentation": ""
			},
			{
				"signature": "int[] inputs =",
				"documentation": ""
			},
			{
				"signature": "int[] expected =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTestBytes()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTestPaddedStrings()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private String getRandomString(int length)",
				"documentation": ""
			},
			{
				"signature": "private UnsafeRow makeKeyRow(long k1, String k2)",
				"documentation": ""
			},
			{
				"signature": "private UnsafeRow makeKeyRow(long k1, long k2)",
				"documentation": ""
			},
			{
				"signature": "private UnsafeRow makeValueRow(long v1, long v2)",
				"documentation": ""
			},
			{
				"signature": "private UnsafeRow appendRow(RowBasedKeyValueBatch batch, UnsafeRow key, UnsafeRow value)",
				"documentation": ""
			},
			{
				"signature": "private void updateValueRow(UnsafeRow row, long v1, long v2)",
				"documentation": ""
			},
			{
				"signature": "private boolean checkKey(UnsafeRow row, long k1, String k2)",
				"documentation": ""
			},
			{
				"signature": "private boolean checkKey(UnsafeRow row, long k1, long k2)",
				"documentation": ""
			},
			{
				"signature": "private boolean checkValue(UnsafeRow row, long v1, long v2)",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void setup()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void emptyBatch() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void batchType()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void setAndRetrieve()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void setUpdateAndRetrieve()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void iteratorTest() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void fixedLengthTest() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void appendRowUntilExceedingCapacity() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void appendRowUntilExceedingPageSize() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void failureToAllocateFirstPage() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedTest()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test the XXH64 function.\n * \u003cp/\u003e\n * Test constants were taken from the original implementation and the airlift/slice implementation.\n */",
		"name": "org.apache.spark.sql.catalyst.expressions.XXH64Suite",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * Test the XXH64 function.\n * \u003cp/\u003e\n * Test constants were taken from the original implementation and the airlift/slice implementation.\n */"
			},
			{
				"signature": "@Test\n  public void testKnownIntegerInputs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKnownLongInputs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKnownByteArrayInputs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKnownWordArrayInputs()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTest()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTestBytes()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void randomizedStressTestPaddedStrings()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.connector.catalog.CatalogLoadingSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testLoad() throws SparkException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInitializationOptions() throws SparkException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLoadWithoutConfig()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLoadMissingClass()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLoadNonCatalogPlugin()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLoadConstructorFailureCatalogPlugin()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLoadAccessErrorCatalogPlugin()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.connector.catalog.TestCatalogPlugin",
		"extends": "",
		"Methods": [
			{
				"signature": "TestCatalogPlugin()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initialize(String name, CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.CatalogPlugin"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.connector.catalog.ConstructorFailureCatalogPlugin",
		"extends": "",
		"Methods": [
			{
				"signature": "ConstructorFailureCatalogPlugin()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initialize(String name, CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.CatalogPlugin"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.connector.catalog.AccessErrorCatalogPlugin",
		"extends": "",
		"Methods": [
			{
				"signature": "private AccessErrorCatalogPlugin()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initialize(String name, CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.CatalogPlugin"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.connector.catalog.InvalidCatalogPlugin",
		"extends": "",
		"Methods": [
			{
				"signature": "public void initialize(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.streaming.JavaGroupStateTimeoutSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testTimeouts()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.streaming.JavaOutputModeSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testOutputModes()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * TODO (PARQUET-1809): This is a temporary workaround; it is intended to be moved to Parquet.\n */",
		"name": "org.apache.parquet.filter2.predicate.SparkFilterApi",
		"extends": "",
		"Methods": [
			{
				"signature": "public static IntColumn intColumn(String[] path)",
				"documentation": "/**\n * TODO (PARQUET-1809): This is a temporary workaround; it is intended to be moved to Parquet.\n */"
			},
			{
				"signature": "public static LongColumn longColumn(String[] path)",
				"documentation": ""
			},
			{
				"signature": "public static FloatColumn floatColumn(String[] path)",
				"documentation": ""
			},
			{
				"signature": "public static DoubleColumn doubleColumn(String[] path)",
				"documentation": ""
			},
			{
				"signature": "public static BooleanColumn booleanColumn(String[] path)",
				"documentation": ""
			},
			{
				"signature": "public static BinaryColumn binaryColumn(String[] path)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This is a workaround since methods below are not public in {@link ColumnIO}.\n *\n * TODO(SPARK-36511): we should remove this once PARQUET-2050 and PARQUET-2083 are released with\n *   Parquet 1.13.\n */",
		"name": "org.apache.parquet.io.ColumnIOUtil",
		"extends": "",
		"Methods": [
			{
				"signature": "private ColumnIOUtil()",
				"documentation": "/**\n * This is a workaround since methods below are not public in {@link ColumnIO}.\n *\n * TODO(SPARK-36511): we should remove this once PARQUET-2050 and PARQUET-2083 are released with\n *   Parquet 1.13.\n */"
			},
			{
				"signature": "public static int getDefinitionLevel(ColumnIO column)",
				"documentation": ""
			},
			{
				"signature": "public static int getRepetitionLevel(ColumnIO column)",
				"documentation": ""
			},
			{
				"signature": "public static String[] getFieldPath(ColumnIO column)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ::Experimental::\n * Base interface for a map function used in\n * {@code org.apache.spark.sql.KeyValueGroupedDataset.flatMapGroupsWithState(\n * FlatMapGroupsWithStateFunction, org.apache.spark.sql.streaming.OutputMode,\n * org.apache.spark.sql.Encoder, org.apache.spark.sql.Encoder)}\n * @since 2.1.1\n */",
		"name": "org.apache.spark.api.java.function.FlatMapGroupsWithStateFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [
			"org.apache.spark.examples.sql.streaming.JavaStructuredComplexSessionization"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ::Experimental::\n * Base interface for a map function used in\n * {@link org.apache.spark.sql.KeyValueGroupedDataset#mapGroupsWithState(\n * MapGroupsWithStateFunction, org.apache.spark.sql.Encoder, org.apache.spark.sql.Encoder)}\n * @since 2.1.1\n */",
		"name": "org.apache.spark.api.java.function.MapGroupsWithStateFunction",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving",
			"org.apache.spark.annotation.Experimental"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * SaveMode is used to specify the expected behavior of saving a DataFrame to a data source.\n *\n * @since 1.3.0\n */",
		"name": "org.apache.spark.sql.SaveMode",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 0 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF0",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 1 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF1",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 10 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF10",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 11 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF11",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 12 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF12",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 13 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF13",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 14 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF14",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 15 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF15",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 16 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF16",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 17 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF17",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 18 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF18",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 19 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF19",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 2 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF2",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 20 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF20",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 21 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF21",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 22 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF22",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 3 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF3",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 4 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF4",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 5 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF5",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 6 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF6",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 7 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF7",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 8 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF8",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 9 arguments.\n */",
		"name": "org.apache.spark.sql.api.java.UDF9",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A trait that should be implemented by V1 DataSources that would like to leverage the DataSource\n * V2 read code paths.\n *\n * This interface is designed to provide Spark DataSources time to migrate to DataSource V2 and\n * will be removed in a future Spark release.\n *\n * @since 3.0.0\n */",
		"name": "org.apache.spark.sql.connector.read.V1Scan",
		"extends": "org.apache.spark.sql.connector.read.Scan",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Unstable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A logical write that should be executed using V1 InsertableRelation interface.\n * \u003cp\u003e\n * Tables that have {@link TableCapability#V1_BATCH_WRITE} in the list of their capabilities\n * must build {@link V1Write}.\n *\n * @since 3.2.0\n */",
		"name": "org.apache.spark.sql.connector.write.V1Write",
		"extends": "org.apache.spark.sql.connector.write.Write",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Unstable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An iterator interface used to pull the output from generated function for multiple operators\n * (whole stage codegen).\n */",
		"name": "org.apache.spark.sql.execution.BufferedRowIterator",
		"extends": "",
		"Methods": [
			{
				"signature": "public boolean hasNext() throws IOException",
				"documentation": "/**\n * An iterator interface used to pull the output from generated function for multiple operators\n * (whole stage codegen).\n */"
			},
			{
				"signature": "public InternalRow next()",
				"documentation": ""
			},
			{
				"signature": "public long durationMs()",
				"documentation": "/**\n   * Returns the elapsed time since this object is created. This object represents a pipeline so\n   * this is a measure of how long the pipeline has been running.\n   */"
			},
			{
				"signature": "public void append(InternalRow row)",
				"documentation": "/**\n   * Append a row to currentRows.\n   */"
			},
			{
				"signature": "public boolean shouldStop()",
				"documentation": "/**\n   * Returns whether `processNext()` should stop processing next row from `input` or not.\n   *\n   * If it returns true, the caller should exit the loop (return from processNext()).\n   */"
			},
			{
				"signature": "public void incPeakExecutionMemory(long size)",
				"documentation": "/**\n   * Increase the peak execution memory for current task.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.RecordBinaryComparator",
		"extends": "org.apache.spark.util.collection.unsafe.sort.RecordComparator",
		"Methods": [
			{
				"signature": "@Override\n  public int compare(\n      Object leftObj, long leftOff, int leftLen, Object rightObj, long rightOff, int rightLen)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.UnsafeExternalRowSorter",
		"extends": "",
		"Methods": [
			{
				"signature": "public static UnsafeExternalRowSorter createWithRecordComparator(\n      StructType schema,\n      Supplier\u003cRecordComparator\u003e recordComparatorSupplier,\n      PrefixComparator prefixComparator,\n      UnsafeExternalRowSorter.PrefixComputer prefixComputer,\n      long pageSizeBytes,\n      boolean canUseRadixSort) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public static UnsafeExternalRowSorter create(\n      StructType schema,\n      Ordering\u003cInternalRow\u003e ordering,\n      PrefixComparator prefixComparator,\n      UnsafeExternalRowSorter.PrefixComputer prefixComputer,\n      long pageSizeBytes,\n      boolean canUseRadixSort) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private UnsafeExternalRowSorter(\n      StructType schema,\n      Supplier\u003cRecordComparator\u003e recordComparatorSupplier,\n      PrefixComparator prefixComparator,\n      UnsafeExternalRowSorter.PrefixComputer prefixComputer,\n      long pageSizeBytes,\n      boolean canUseRadixSort)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  void setTestSpillFrequency(int frequency)",
				"documentation": "/**\n   * Forces spills to occur every `frequency` records. Only for use in tests.\n   */"
			},
			{
				"signature": "public void insertRow(UnsafeRow row) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public long getPeakMemoryUsage()",
				"documentation": "/**\n   * Return the peak memory used so far, in bytes.\n   */"
			},
			{
				"signature": "public long getSortTimeNanos()",
				"documentation": "/**\n   * @return the total amount of time spent sorting data (in-memory only).\n   */"
			},
			{
				"signature": "public void cleanupResources()",
				"documentation": ""
			},
			{
				"signature": "public Iterator\u003cInternalRow\u003e sort() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public boolean advanceNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public UnsafeRow getRow()",
				"documentation": ""
			},
			{
				"signature": "public Iterator\u003cInternalRow\u003e sort(Iterator\u003cUnsafeRow\u003e inputIterator) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow",
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter.RowComparator"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter.PrefixComputer",
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter.RowComparator"
		]
	},
	{
		"documentation": "/**\n   * If positive, forces records to be spilled to disk at the given frequency (measured in numbers\n   * of records). This is only intended to be used in tests.\n   */",
		"name": "org.apache.spark.sql.execution.UnsafeExternalRowSorter.PrefixComputer",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter.PrefixComputer.Prefix"
		]
	},
	{
		"documentation": "/**\n   * If positive, forces records to be spilled to disk at the given frequency (measured in numbers\n   * of records). This is only intended to be used in tests.\n   */",
		"name": "org.apache.spark.sql.execution.UnsafeExternalRowSorter.PrefixComputer.Prefix",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.UnsafeExternalRowSorter.RowComparator",
		"extends": "org.apache.spark.util.collection.unsafe.sort.RecordComparator",
		"Methods": [
			{
				"signature": "RowComparator(Ordering\u003cInternalRow\u003e ordering, int numFields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int compare(\n        Object baseObj1,\n        long baseOff1,\n        int baseLen1,\n        Object baseObj2,\n        long baseOff2,\n        int baseLen2)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.UnsafeExternalRowSorter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Unsafe-based HashMap for performing aggregations where the aggregated values are fixed-width.\n *\n * This map supports a maximum of 2 billion keys.\n */",
		"name": "org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap",
		"extends": "",
		"Methods": [
			{
				"signature": "public static boolean supportsAggregationBufferSchema(StructType schema)",
				"documentation": "/**\n   * @return true if UnsafeFixedWidthAggregationMap supports aggregation buffers with the given\n   *         schema, false otherwise.\n   */"
			},
			{
				"signature": "public UnsafeFixedWidthAggregationMap(\n      InternalRow emptyAggregationBuffer,\n      StructType aggregationBufferSchema,\n      StructType groupingKeySchema,\n      TaskContext taskContext,\n      int initialCapacity,\n      long pageSizeBytes)",
				"documentation": "/**\n   * Create a new UnsafeFixedWidthAggregationMap.\n   *\n   * @param emptyAggregationBuffer the default value for new keys (a \"zero\" of the agg. function)\n   * @param aggregationBufferSchema the schema of the aggregation buffer, used for row conversion.\n   * @param groupingKeySchema the schema of the grouping key, used for row conversion.\n   * @param taskContext the current task context.\n   * @param initialCapacity the initial capacity of the map (a sizing hint to avoid re-hashing).\n   * @param pageSizeBytes the data page size, in bytes; limits the maximum record size.\n   */"
			},
			{
				"signature": "public UnsafeRow getAggregationBuffer(InternalRow groupingKey)",
				"documentation": "/**\n   * Return the aggregation buffer for the current group. For efficiency, all calls to this method\n   * return the same object. If additional memory could not be allocated, then this method will\n   * signal an error by returning null.\n   */"
			},
			{
				"signature": "public UnsafeRow getAggregationBufferFromUnsafeRow(UnsafeRow key)",
				"documentation": ""
			},
			{
				"signature": "public UnsafeRow getAggregationBufferFromUnsafeRow(UnsafeRow key, int hash)",
				"documentation": ""
			},
			{
				"signature": "public KVIterator\u003cUnsafeRow, UnsafeRow\u003e iterator()",
				"documentation": "/**\n   * Returns an iterator over the keys and values in this map. This uses destructive iterator of\n   * BytesToBytesMap. So it is illegal to call any other method on this map after `iterator()` has\n   * been called.\n   *\n   * For efficiency, each call returns the same object.\n   */"
			},
			{
				"signature": "@Override\n      public boolean next()",
				"documentation": "/**\n   * Returns an iterator over the keys and values in this map. This uses destructive iterator of\n   * BytesToBytesMap. So it is illegal to call any other method on this map after `iterator()` has\n   * been called.\n   *\n   * For efficiency, each call returns the same object.\n   */"
			},
			{
				"signature": "@Override\n      public UnsafeRow getKey()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public UnsafeRow getValue()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void close()",
				"documentation": ""
			},
			{
				"signature": "public long getPeakMemoryUsedBytes()",
				"documentation": "/**\n   * Return the peak memory used so far, in bytes.\n   */"
			},
			{
				"signature": "public void free()",
				"documentation": "/**\n   * Free the memory associated with this map. This is idempotent and can be called multiple times.\n   */"
			},
			{
				"signature": "public double getAvgHashProbeBucketListIterations()",
				"documentation": "/**\n   * Gets the average bucket list iterations per lookup in the underlying `BytesToBytesMap`.\n   */"
			},
			{
				"signature": "public UnsafeKVExternalSorter destructAndCreateExternalSorter() throws IOException",
				"documentation": "/**\n   * Sorts the map's records in place, spill them to disk, and returns an [[UnsafeKVExternalSorter]]\n   *\n   * Note that the map will be reset for inserting new records, and the returned sorter can NOT be\n   * used to insert records.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A class for performing external sorting on key-value records. Both key and value are UnsafeRows.\n *\n * Note that this class allows optionally passing in a {@link BytesToBytesMap} directly in order\n * to perform in-place sorting of records in the map.\n */",
		"name": "org.apache.spark.sql.execution.UnsafeKVExternalSorter",
		"extends": "",
		"Methods": [
			{
				"signature": "public UnsafeKVExternalSorter(\n      StructType keySchema,\n      StructType valueSchema,\n      BlockManager blockManager,\n      SerializerManager serializerManager,\n      long pageSizeBytes,\n      int numElementsForSpillThreshold) throws IOException",
				"documentation": "/**\n * A class for performing external sorting on key-value records. Both key and value are UnsafeRows.\n *\n * Note that this class allows optionally passing in a {@link BytesToBytesMap} directly in order\n * to perform in-place sorting of records in the map.\n */"
			},
			{
				"signature": "public UnsafeKVExternalSorter(\n      StructType keySchema,\n      StructType valueSchema,\n      BlockManager blockManager,\n      SerializerManager serializerManager,\n      long pageSizeBytes,\n      int numElementsForSpillThreshold,\n      @Nullable BytesToBytesMap map) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public void insertKV(UnsafeRow key, UnsafeRow value) throws IOException",
				"documentation": "/**\n   * Inserts a key-value record into the sorter. If the sorter no longer has enough memory to hold\n   * the record, the sorter sorts the existing records in-memory, writes them out as partially\n   * sorted runs, and then reallocates memory to hold the new record.\n   */"
			},
			{
				"signature": "public void merge(UnsafeKVExternalSorter other) throws IOException",
				"documentation": "/**\n   * Merges another UnsafeKVExternalSorter into `this`, the other one will be emptied.\n   *\n   * @throws IOException\n   */"
			},
			{
				"signature": "public KVSorterIterator sortedIterator() throws IOException",
				"documentation": "/**\n   * Returns a sorted iterator. It is the caller's responsibility to call `cleanupResources()`\n   * after consuming this iterator.\n   */"
			},
			{
				"signature": "public long getSpillSize()",
				"documentation": "/**\n   * Return the total number of bytes that has been spilled into disk so far.\n   */"
			},
			{
				"signature": "public long getPeakMemoryUsedBytes()",
				"documentation": "/**\n   * Return the peak memory used so far, in bytes.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  void closeCurrentPage()",
				"documentation": "/**\n   * Marks the current page as no-more-space-available, and as a result, either allocate a\n   * new page or spill when we see the next record.\n   */"
			},
			{
				"signature": "public void cleanupResources()",
				"documentation": "/**\n   * Frees this sorter's in-memory data structures and cleans up its spill files.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UnsafeAlignedOffset",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow",
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter.KVComparator",
			"org.apache.spark.sql.execution.KVSorterIterator"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter.KVComparator",
			"org.apache.spark.sql.execution.KVSorterIterator"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.UnsafeKVExternalSorter.KVComparator",
		"extends": "RecordComparator",
		"Methods": [
			{
				"signature": "KVComparator(BaseOrdering ordering, int numKeyFields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int compare(\n        Object baseObj1,\n        long baseOff1,\n        int baseLen1,\n        Object baseObj2,\n        long baseOff2,\n        int baseLen2)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.UnsafeAlignedOffset",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.KVSorterIterator",
		"extends": "org.apache.spark.unsafe.KVIterator",
		"Methods": [
			{
				"signature": "private KVSorterIterator(UnsafeSorterIterator underlying)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean next() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public UnsafeRow getKey()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public UnsafeRow getValue()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UnsafeAlignedOffset",
			"org.apache.spark.sql.catalyst.expressions.UnsafeRow"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.UnsafeKVExternalSorter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.columnar.ColumnDictionary",
		"extends": "",
		"Methods": [
			{
				"signature": "public ColumnDictionary(int[] dictionary)",
				"documentation": ""
			},
			{
				"signature": "public ColumnDictionary(long[] dictionary)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int decodeToInt(int id)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long decodeToLong(int id)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float decodeToFloat(int id)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double decodeToDouble(int id)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] decodeToBinary(int id)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.vectorized.Dictionary"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Exception thrown when the parquet reader find column type mismatches.\n */",
		"name": "org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException",
		"extends": "RuntimeException",
		"Methods": [
			{
				"signature": "public String getColumn()",
				"documentation": "/**\n   * Logical column type in the parquet schema the parquet reader use to parse all files.\n   */"
			},
			{
				"signature": "public String getPhysicalType()",
				"documentation": ""
			},
			{
				"signature": "public String getLogicalType()",
				"documentation": ""
			},
			{
				"signature": "public SchemaColumnConvertNotSupportedException(\n      String column,\n      String physicalType,\n      String logicalType)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Unstable"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A column vector implementation for Spark's {@link ArrayType}.\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcArrayColumnVector",
		"extends": "org.apache.spark.sql.execution.datasources.orc.OrcColumnVector",
		"Methods": [
			{
				"signature": "OrcArrayColumnVector(\n      DataType type,\n      ColumnVector vector,\n      OrcColumnVector data)",
				"documentation": "/**\n * A column vector implementation for Spark's {@link ArrayType}.\n */"
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public org.apache.spark.sql.vectorized.ColumnVector getChild(int ordinal)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.vectorized.ColumnarArray"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A column vector implementation for Spark's AtomicType.\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcAtomicColumnVector",
		"extends": "org.apache.spark.sql.execution.datasources.orc.OrcColumnVector",
		"Methods": [
			{
				"signature": "OrcAtomicColumnVector(DataType type, ColumnVector vector)",
				"documentation": "/**\n * A column vector implementation for Spark's AtomicType.\n */"
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public org.apache.spark.sql.vectorized.ColumnVector getChild(int ordinal)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Columns statistics interface wrapping ORC {@link ColumnStatistics}s.\n *\n * Because ORC {@link ColumnStatistics}s are stored as an flatten array in ORC file footer,\n * this class is used to covert ORC {@link ColumnStatistics}s from array to nested tree structure,\n * according to data types. The flatten array stores all data types (including nested types) in\n * tree pre-ordering. This is used for aggregate push down in ORC.\n *\n * For nested data types (array, map and struct), the sub-field statistics are stored recursively\n * inside parent column's children field. Here is an example of {@link OrcColumnStatistics}:\n *\n * Data schema:\n * c1: int\n * c2: struct\u003cf1: int, f2: float\u003e\n * c3: map\u003ckey: int, value: string\u003e\n * c4: array\u003cint\u003e\n *\n *                        OrcColumnStatistics\n *                                | (children)\n *             ---------------------------------------------\n *            /         |                 \\                 \\\n *           c1        c2                 c3                c4\n *      (integer)    (struct)            (map)             (array)\n*        (min:1,        | (children)       | (children)      | (children)\n *       max:10)      -----              -----             element\n *                   /     \\            /     \\           (integer)\n *                c2.f1    c2.f2      key     value\n *              (integer) (float)  (integer) (string)\n *                       (min:0.1,           (min:\"a\",\n *                       max:100.5)          max:\"zzz\")\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcColumnStatistics",
		"extends": "",
		"Methods": [
			{
				"signature": "public OrcColumnStatistics(ColumnStatistics statistics)",
				"documentation": "/**\n * Columns statistics interface wrapping ORC {@link ColumnStatistics}s.\n *\n * Because ORC {@link ColumnStatistics}s are stored as an flatten array in ORC file footer,\n * this class is used to covert ORC {@link ColumnStatistics}s from array to nested tree structure,\n * according to data types. The flatten array stores all data types (including nested types) in\n * tree pre-ordering. This is used for aggregate push down in ORC.\n *\n * For nested data types (array, map and struct), the sub-field statistics are stored recursively\n * inside parent column's children field. Here is an example of {@link OrcColumnStatistics}:\n *\n * Data schema:\n * c1: int\n * c2: struct\u003cf1: int, f2: float\u003e\n * c3: map\u003ckey: int, value: string\u003e\n * c4: array\u003cint\u003e\n *\n *                        OrcColumnStatistics\n *                                | (children)\n *             ---------------------------------------------\n *            /         |                 \\                 \\\n *           c1        c2                 c3                c4\n *      (integer)    (struct)            (map)             (array)\n*        (min:1,        | (children)       | (children)      | (children)\n *       max:10)      -----              -----             element\n *                   /     \\            /     \\           (integer)\n *                c2.f1    c2.f2      key     value\n *              (integer) (float)  (integer) (string)\n *                       (min:0.1,           (min:\"a\",\n *                       max:100.5)          max:\"zzz\")\n */"
			},
			{
				"signature": "public ColumnStatistics getStatistics()",
				"documentation": ""
			},
			{
				"signature": "public OrcColumnStatistics get(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "public void add(OrcColumnStatistics newChild)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A column vector interface wrapping Hive's {@link ColumnVector}.\n *\n * Because Spark {@link ColumnarBatch} only accepts Spark's vectorized.ColumnVector,\n * this column vector is used to adapt Hive ColumnVector with Spark ColumnarVector.\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcColumnVector",
		"extends": "org.apache.spark.sql.vectorized.ColumnVector",
		"Methods": [
			{
				"signature": "OrcColumnVector(DataType type, ColumnVector vector)",
				"documentation": "/**\n * A column vector interface wrapping Hive's {@link ColumnVector}.\n *\n * Because Spark {@link ColumnarBatch} only accepts Spark's vectorized.ColumnVector,\n * this column vector is used to adapt Hive ColumnVector with Spark ColumnarVector.\n */"
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean hasNull()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numNulls()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setBatchSize(int batchSize)",
				"documentation": ""
			},
			{
				"signature": "protected int getRowIndex(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.execution.datasources.orc.OrcArrayColumnVector",
			"org.apache.spark.sql.execution.datasources.orc.OrcAtomicColumnVector",
			"org.apache.spark.sql.execution.datasources.orc.OrcMapColumnVector",
			"org.apache.spark.sql.execution.datasources.orc.OrcStructColumnVector"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Utility class for {@link OrcColumnVector}.\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcColumnVectorUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "static OrcColumnVector toOrcColumnVector(DataType type, ColumnVector vector)",
				"documentation": "/**\n   * Convert a Hive's {@link ColumnVector} to a Spark's {@link OrcColumnVector}.\n   *\n   * @param type The data type of column vector\n   * @param vector Hive's column vector\n   * @return Spark's column vector\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * To support vectorization in WholeStageCodeGen, this reader returns ColumnarBatch.\n * After creating, `initialize` and `initBatch` should be called sequentially.\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader",
		"extends": "org.apache.hadoop.mapreduce.RecordReader",
		"Methods": [
			{
				"signature": "public OrcColumnarBatchReader(int capacity)",
				"documentation": "/**\n   * The column IDs of the physical ORC file schema which are required by this reader.\n   * -1 means this required column is partition column, or it doesn't exist in the ORC file.\n   * Ideally partition column should never appear in the physical file, and should only appear\n   * in the directory name. However, Spark allows partition columns inside physical file,\n   * but Spark will discard the values from the file, and use the partition value got from\n   * directory name. The column order will be reserved though.\n   */"
			},
			{
				"signature": "@Override\n  public Void getCurrentKey()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarBatch getCurrentValue()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getProgress() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean nextKeyValue() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initialize(\n      InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException",
				"documentation": "/**\n   * Initialize ORC file reader and batch record reader.\n   * Please note that `initBatch` is needed to be called after this.\n   */"
			},
			{
				"signature": "public void initBatch(\n      TypeDescription orcSchema,\n      StructField[] requiredFields,\n      int[] requestedDataColIds,\n      int[] requestedPartitionColIds,\n      InternalRow partitionValues)",
				"documentation": "/**\n   * Initialize columnar batch by setting required schema and partition information.\n   * With this information, this creates ColumnarBatch with the full schema.\n   *\n   * @param orcSchema Schema from ORC file reader.\n   * @param requiredFields All the fields that are required to return, including partition fields.\n   * @param requestedDataColIds Requested column ids from orcSchema. -1 if not existed.\n   * @param requestedPartitionColIds Requested column ids from partition schema. -1 if not existed.\n   * @param partitionValues Values of partition columns.\n   */"
			},
			{
				"signature": "private boolean nextBatch() throws IOException",
				"documentation": "/**\n   * Return true if there exists more data in the next batch. If exists, prepare the next batch\n   * by copying from ORC VectorizedRowBatch columns to Spark ColumnarBatch columns.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.vectorized.ColumnarBatch",
			"org.apache.spark.sql.execution.vectorized.ColumnVectorUtils",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * {@link OrcFooterReader} is a util class which encapsulates the helper\n * methods of reading ORC file footer.\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcFooterReader",
		"extends": "",
		"Methods": [
			{
				"signature": "public static OrcColumnStatistics readStatistics(Reader orcReader)",
				"documentation": "/**\n   * Read the columns statistics from ORC file footer.\n   *\n   * @param orcReader the reader to read ORC file footer.\n   * @return Statistics for all columns in the file.\n   */"
			},
			{
				"signature": "private static OrcColumnStatistics convertStatistics(\n      DataType sparkSchema, Queue\u003cColumnStatistics\u003e orcStatistics)",
				"documentation": "/**\n   * Convert a queue of ORC {@link ColumnStatistics}s into Spark {@link OrcColumnStatistics}.\n   * The queue of ORC {@link ColumnStatistics}s are assumed to be ordered as tree pre-order.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A column vector implementation for Spark's {@link MapType}.\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcMapColumnVector",
		"extends": "org.apache.spark.sql.execution.datasources.orc.OrcColumnVector",
		"Methods": [
			{
				"signature": "OrcMapColumnVector(\n      DataType type,\n      ColumnVector vector,\n      OrcColumnVector keys,\n      OrcColumnVector values)",
				"documentation": "/**\n * A column vector implementation for Spark's {@link MapType}.\n */"
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public org.apache.spark.sql.vectorized.ColumnVector getChild(int ordinal)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.vectorized.ColumnarMap"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A column vector implementation for Spark's {@link StructType}.\n */",
		"name": "org.apache.spark.sql.execution.datasources.orc.OrcStructColumnVector",
		"extends": "org.apache.spark.sql.execution.datasources.orc.OrcColumnVector",
		"Methods": [
			{
				"signature": "OrcStructColumnVector(DataType type, ColumnVector vector, OrcColumnVector[] fields)",
				"documentation": "/**\n * A column vector implementation for Spark's {@link StructType}.\n */"
			},
			{
				"signature": "@Override\n  public org.apache.spark.sql.vectorized.ColumnVector getChild(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int rowId)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Contains necessary information representing a Parquet column, either of primitive or nested type.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetColumnVector",
		"extends": "",
		"Methods": [
			{
				"signature": "ParquetColumnVector(\n      ParquetColumn column,\n      WritableColumnVector vector,\n      int capacity,\n      MemoryMode memoryMode,\n      Set\u003cParquetColumn\u003e missingColumns)",
				"documentation": "/** Reader for this column - only set if 'isPrimitive' is true */"
			},
			{
				"signature": "List\u003cParquetColumnVector\u003e getChildren()",
				"documentation": "/**\n   * Returns all the children of this column.\n   */"
			},
			{
				"signature": "List\u003cParquetColumnVector\u003e getLeaves()",
				"documentation": "/**\n   * Returns all the leaf columns in depth-first order.\n   */"
			},
			{
				"signature": "private static void getLeavesHelper(ParquetColumnVector vector, List\u003cParquetColumnVector\u003e coll)",
				"documentation": ""
			},
			{
				"signature": "void assemble()",
				"documentation": "/**\n   * Assembles this column and calculate collection offsets recursively.\n   * This is a no-op for primitive columns.\n   */"
			},
			{
				"signature": "void reset()",
				"documentation": "/**\n   * Resets this Parquet column vector, which includes resetting all the writable column vectors\n   * (used to store values, definition levels, and repetition levels) for this and all its children.\n   */"
			},
			{
				"signature": "ParquetColumn getColumn()",
				"documentation": "/**\n   * Returns the {@link ParquetColumn} of this column vector.\n   */"
			},
			{
				"signature": "WritableColumnVector getValueVector()",
				"documentation": "/**\n   * Returns the writable column vector used to store values.\n   */"
			},
			{
				"signature": "WritableColumnVector getRepetitionLevelVector()",
				"documentation": "/**\n   * Returns the writable column vector used to store repetition levels.\n   */"
			},
			{
				"signature": "WritableColumnVector getDefinitionLevelVector()",
				"documentation": "/**\n   * Returns the writable column vector used to store definition levels.\n   */"
			},
			{
				"signature": "VectorizedColumnReader getColumnReader()",
				"documentation": "/**\n   * Returns the column reader for reading a Parquet column.\n   */"
			},
			{
				"signature": "void setColumnReader(VectorizedColumnReader reader)",
				"documentation": "/**\n   * Sets the column vector to 'reader'. Note this can only be called on a primitive Parquet\n   * column.\n   */"
			},
			{
				"signature": "private void assembleCollection()",
				"documentation": "/**\n   * Assemble collections, e.g., array, map.\n   */"
			},
			{
				"signature": "private void assembleStruct()",
				"documentation": ""
			},
			{
				"signature": "private static WritableColumnVector allocateLevelsVector(int capacity, MemoryMode memoryMode)",
				"documentation": ""
			},
			{
				"signature": "private int getNextCollectionStart(int maxRepetitionLevel, int idx)",
				"documentation": "/**\n   * For a collection (i.e., array or map) element at index 'idx', returns the starting index of\n   * the next collection after it.\n   *\n   * @param maxRepetitionLevel the maximum repetition level for the elements in this collection\n   * @param idx the index of this collection in the Parquet column\n   * @return the starting index of the next collection\n   */"
			},
			{
				"signature": "private int getCollectionSize(int maxRepetitionLevel, int idx)",
				"documentation": "/**\n   * Gets the size of a collection (i.e., array or map) element, starting at 'idx'.\n   *\n   * @param maxRepetitionLevel the maximum repetition level for the elements in this collection\n   * @param idx the index of this collection in the Parquet column\n   * @return the size of this collection\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.vectorized.OffHeapColumnVector",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary",
		"extends": "",
		"Methods": [
			{
				"signature": "public ParquetDictionary(org.apache.parquet.column.Dictionary dictionary, boolean needTransform)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int decodeToInt(int id)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long decodeToLong(int id)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float decodeToFloat(int id)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double decodeToDouble(int id)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] decodeToBinary(int id)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.vectorized.Dictionary"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * `ParquetFooterReader` is a util class which encapsulates the helper\n * methods of reading parquet file footer\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader",
		"extends": "",
		"Methods": [
			{
				"signature": "public static ParquetMetadata readFooter(Configuration configuration,\n      Path file, ParquetMetadataConverter.MetadataFilter filter) throws IOException",
				"documentation": "/**\n * `ParquetFooterReader` is a util class which encapsulates the helper\n * methods of reading parquet file footer\n */"
			},
			{
				"signature": "public static ParquetMetadata readFooter(Configuration configuration,\n      FileStatus fileStatus, ParquetMetadataConverter.MetadataFilter filter) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static ParquetMetadata readFooter(HadoopInputFile inputFile,\n      ParquetMetadataConverter.MetadataFilter filter) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetLogRedirector",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": ""
			},
			{
				"signature": "private ParquetLogRedirector()",
				"documentation": ""
			},
			{
				"signature": "private static void redirect(Logger logger)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Helper class to store intermediate state while reading a Parquet column chunk.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetReadState",
		"extends": "",
		"Methods": [
			{
				"signature": "ParquetReadState(\n      ColumnDescriptor descriptor,\n      boolean isRequired,\n      PrimitiveIterator.OfLong rowIndexes)",
				"documentation": "/** When processing repeated types, whether we should skip the current batch of definition\n   * levels. */"
			},
			{
				"signature": "private Iterator\u003cRowRange\u003e constructRanges(PrimitiveIterator.OfLong rowIndexes)",
				"documentation": "/**\n   * Construct a list of row ranges from the given `rowIndexes`. For example, suppose the\n   * `rowIndexes` are `[0, 1, 2, 4, 5, 7, 8, 9]`, it will be converted into 3 row ranges:\n   * `[0-2], [4-5], [7-9]`.\n   */"
			},
			{
				"signature": "void resetForNewBatch(int batchSize)",
				"documentation": "/**\n   * Must be called at the beginning of reading a new batch.\n   */"
			},
			{
				"signature": "void resetForNewPage(int totalValuesInPage, long pageFirstRowIndex)",
				"documentation": "/**\n   * Must be called at the beginning of reading a new page.\n   */"
			},
			{
				"signature": "long currentRangeStart()",
				"documentation": "/**\n   * Returns the start index of the current row range.\n   */"
			},
			{
				"signature": "long currentRangeEnd()",
				"documentation": "/**\n   * Returns the end index of the current row range.\n   */"
			},
			{
				"signature": "void nextRange()",
				"documentation": "/**\n   * Advance to the next range.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetReadState.RowRange"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetReadState.RowRange"
		]
	},
	{
		"documentation": "/**\n   * Helper struct to represent a range of row indexes `[start, end]`.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetReadState.RowRange",
		"extends": "",
		"Methods": [
			{
				"signature": "RowRange(long start, long end)",
				"documentation": "/**\n   * Helper struct to represent a range of row indexes `[start, end]`.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetReadState"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "default void decodeDictionaryIds(\n      int total,\n      int offset,\n      WritableColumnVector values,\n      WritableColumnVector dictionaryIds,\n      Dictionary dictionary)",
				"documentation": "/**\n   * Process a batch of `total` values starting from `offset` in `values`, whose null slots\n   * should have already been filled, and fills the non-null slots using dictionary IDs from\n   * `dictionaryIds`, together with Parquet `dictionary`.\n   *\n   * @param total total number slots to process in `values`\n   * @param offset starting offset in `values`\n   * @param values destination value vector\n   * @param dictionaryIds vector storing the dictionary IDs\n   * @param dictionary Parquet dictionary used to decode a dictionary ID to its value\n   */"
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BooleanUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.IntegerUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.UnsignedIntegerUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.ByteUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.ShortUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.IntegerWithRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.DowncastLongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.UnsignedLongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongWithRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongAsMicrosUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongAsMicrosRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FloatUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.DoubleUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampConvertTzUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampConvertTzRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayAsIntUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayAsLongUpdater"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "ParquetVectorUpdaterFactory(\n      LogicalTypeAnnotation logicalTypeAnnotation,\n      ZoneId convertTz,\n      String datetimeRebaseMode,\n      String datetimeRebaseTz,\n      String int96RebaseMode,\n      String int96RebaseTz)",
				"documentation": ""
			},
			{
				"signature": "public ParquetVectorUpdater getUpdater(ColumnDescriptor descriptor, DataType sparkType)",
				"documentation": ""
			},
			{
				"signature": "boolean isTimestampTypeMatched(LogicalTypeAnnotation.TimeUnit unit)",
				"documentation": ""
			},
			{
				"signature": "void validateTimestampType(DataType sparkType)",
				"documentation": ""
			},
			{
				"signature": "void convertErrorForTimestampNTZ(String parquetType)",
				"documentation": ""
			},
			{
				"signature": "boolean isUnsignedIntTypeMatched(int bitWidth)",
				"documentation": ""
			},
			{
				"signature": "private static int rebaseDays(int julianDays, final boolean failIfRebase)",
				"documentation": ""
			},
			{
				"signature": "private static long rebaseTimestamp(\n      long julianMicros,\n      final boolean failIfRebase,\n      final String format,\n      final String timeZone)",
				"documentation": ""
			},
			{
				"signature": "private static long rebaseMicros(\n      long julianMicros,\n      final boolean failIfRebase,\n      final String timeZone)",
				"documentation": ""
			},
			{
				"signature": "private static long rebaseInt96(\n      long julianMicros,\n      final boolean failIfRebase,\n      final String timeZone)",
				"documentation": ""
			},
			{
				"signature": "private boolean shouldConvertTimestamps()",
				"documentation": ""
			},
			{
				"signature": "private SchemaColumnConvertNotSupportedException constructConvertNotSupportedException(\n      ColumnDescriptor descriptor,\n      DataType sparkType)",
				"documentation": "/**\n   * Helper function to construct exception for parquet schema mismatch.\n   */"
			},
			{
				"signature": "private static boolean canReadAsIntDecimal(ColumnDescriptor descriptor, DataType dt)",
				"documentation": ""
			},
			{
				"signature": "private static boolean canReadAsLongDecimal(ColumnDescriptor descriptor, DataType dt)",
				"documentation": ""
			},
			{
				"signature": "private static boolean canReadAsBinaryDecimal(ColumnDescriptor descriptor, DataType dt)",
				"documentation": ""
			},
			{
				"signature": "private static boolean isLongDecimal(DataType dt)",
				"documentation": ""
			},
			{
				"signature": "private static boolean isDecimalTypeMatched(ColumnDescriptor descriptor, DataType dt)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BooleanUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.IntegerUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.UnsignedIntegerUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.ByteUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.ShortUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.IntegerWithRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.DowncastLongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.UnsignedLongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongWithRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongAsMicrosUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongAsMicrosRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FloatUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.DoubleUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampConvertTzUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampConvertTzRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayAsIntUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayAsLongUpdater"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BooleanUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.IntegerUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.UnsignedIntegerUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.ByteUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.ShortUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.IntegerWithRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.DowncastLongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.UnsignedLongUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongWithRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongAsMicrosUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongAsMicrosRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FloatUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.DoubleUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampConvertTzUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampConvertTzRebaseUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayAsIntUpdater",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayAsLongUpdater"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BooleanUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.IntegerUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.UnsignedIntegerUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.ByteUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.ShortUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.IntegerWithRebaseUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "IntegerWithRebaseUpdater(boolean failIfRebase)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.DowncastLongUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.UnsignedLongUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongWithRebaseUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "LongWithRebaseUpdater(boolean failIfRebase, String timeZone)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongAsMicrosUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.LongAsMicrosRebaseUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "LongAsMicrosRebaseUpdater(boolean failIfRebase, String timeZone)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FloatUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.DoubleUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampConvertTzUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "BinaryToSQLTimestampConvertTzUpdater(ZoneId convertTz)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampRebaseUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "BinaryToSQLTimestampRebaseUpdater(boolean failIfRebase, String timeZone)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.BinaryToSQLTimestampConvertTzRebaseUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "BinaryToSQLTimestampConvertTzRebaseUpdater(\n        boolean failIfRebase,\n        ZoneId convertTz,\n        String timeZone)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "FixedLenByteArrayUpdater(int arrayLen)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayAsIntUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "FixedLenByteArrayAsIntUpdater(int arrayLen)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.FixedLenByteArrayAsLongUpdater",
		"extends": "",
		"Methods": [
			{
				"signature": "FixedLenByteArrayAsLongUpdater(int arrayLen)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValues(\n        int total,\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void skipValues(int total, VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void readValue(\n        int offset,\n        WritableColumnVector values,\n        VectorizedValuesReader valuesReader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void decodeSingleDictionaryId(\n        int offset,\n        WritableColumnVector values,\n        WritableColumnVector dictionaryIds,\n        Dictionary dictionary)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdater"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base class for custom RecordReaders for Parquet that directly materialize to `T`.\n * This class handles computing row groups, filtering on them, setting up the column readers,\n * etc.\n * This is heavily based on parquet-mr's RecordReader.\n * TODO: move this to the parquet-mr project. There are performance benefits of doing it\n * this way, albeit at a higher cost to implement. This base class is reusable.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase",
		"extends": "org.apache.hadoop.mapreduce.RecordReader",
		"Methods": [
			{
				"signature": "@Override\n  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n      throws IOException, InterruptedException",
				"documentation": "/**\n   * The total number of rows this RecordReader will eventually read. The sum of the\n   * rows of all the row groups.\n   */"
			},
			{
				"signature": "protected void initialize(String path, List\u003cString\u003e columns) throws IOException",
				"documentation": "/**\n   * Initializes the reader to read the file at `path` with `columns` projected. If columns is\n   * null, all the columns are projected.\n   *\n   * This is exposed for testing to be able to create this reader without the rest of the Hadoop\n   * split machinery. It is not intended for general use and those not support all the\n   * configurations.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  protected void initialize(\n      MessageType fileSchema,\n      MessageType requestedSchema,\n      ParquetRowGroupReader rowGroupReader,\n      int totalRowCount) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Void getCurrentKey()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private static \u003cK, V\u003e Map\u003cK, Set\u003cV\u003e\u003e toSetMultiMap(Map\u003cK, V\u003e map)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  private Class\u003c? extends ReadSupport\u003cT\u003e\u003e getReadSupportClass(Configuration configuration)",
				"documentation": ""
			},
			{
				"signature": "private static \u003cT\u003e ReadSupport\u003cT\u003e getReadSupportInstance(\n      Class\u003c? extends ReadSupport\u003cT\u003e\u003e readSupportClass)",
				"documentation": "/**\n   * @param readSupportClass to instantiate\n   * @return the configured read support\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.ParquetRowGroupReaderImpl"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetRowGroupReader",
			"org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.ParquetRowGroupReaderImpl"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ParquetRowGroupReader",
		"extends": "java.io.Closeable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.ParquetRowGroupReaderImpl"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.ParquetRowGroupReaderImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "ParquetRowGroupReaderImpl(ParquetFileReader reader)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PageReadStore readNextRowGroup() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetRowGroupReader"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Decoder to return values from a single column.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader",
		"extends": "",
		"Methods": [
			{
				"signature": "public VectorizedColumnReader(\n      ColumnDescriptor descriptor,\n      boolean isRequired,\n      PageReadStore pageReadStore,\n      ZoneId convertTz,\n      String datetimeRebaseMode,\n      String datetimeRebaseTz,\n      String int96RebaseMode,\n      String int96RebaseTz,\n      ParsedVersion writerVersion) throws IOException",
				"documentation": "/**\n   * The index for the first row in the current page, among all rows across all pages in the\n   * column chunk for this reader. If there is no column index, the value is 0.\n   */"
			},
			{
				"signature": "private boolean isLazyDecodingSupported(PrimitiveType.PrimitiveTypeName typeName)",
				"documentation": ""
			},
			{
				"signature": "void readBatch(\n      int total,\n      WritableColumnVector column,\n      WritableColumnVector repetitionLevels,\n      WritableColumnVector definitionLevels) throws IOException",
				"documentation": "/**\n   * Reads `total` rows from this columnReader into column.\n   */"
			},
			{
				"signature": "private int readPage()",
				"documentation": ""
			},
			{
				"signature": "private void initDataReader(\n      int pageValueCount,\n      Encoding dataEncoding,\n      ByteBufferInputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private ValuesReader getValuesReader(Encoding encoding)",
				"documentation": ""
			},
			{
				"signature": "private int readPageV1(DataPageV1 page) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private int readPageV2(DataPageV2 page) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An implementation of the Parquet DELTA_BINARY_PACKED decoder that supports the vectorized\n * interface. DELTA_BINARY_PACKED is a delta encoding for integer and long types that stores values\n * as a delta between consecutive values. Delta values are themselves bit packed. Similar to RLE but\n * is more effective in the case of large variation of values in the encoded column.\n * \u003cp\u003e\n * DELTA_BINARY_PACKED is the default encoding for integer and long columns in Parquet V2.\n * \u003cp\u003e\n * Supported Types: INT32, INT64\n * \u003cp\u003e\n *\n * @see \u003ca href=\"https://github.com/apache/parquet-format/blob/master/Encodings.md#delta-encoding-delta_binary_packed--5\"\u003e\n * Parquet format encodings: DELTA_BINARY_PACKED\u003c/a\u003e\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaBinaryPackedReader",
		"extends": "org.apache.spark.sql.execution.datasources.parquet.VectorizedReaderBase",
		"Methods": [
			{
				"signature": "@Override\n  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException",
				"documentation": "/**\n * An implementation of the Parquet DELTA_BINARY_PACKED decoder that supports the vectorized\n * interface. DELTA_BINARY_PACKED is a delta encoding for integer and long types that stores values\n * as a delta between consecutive values. Delta values are themselves bit packed. Similar to RLE but\n * is more effective in the case of large variation of values in the encoded column.\n * \u003cp\u003e\n * DELTA_BINARY_PACKED is the default encoding for integer and long columns in Parquet V2.\n * \u003cp\u003e\n * Supported Types: INT32, INT64\n * \u003cp\u003e\n *\n * @see \u003ca href=\"https://github.com/apache/parquet-format/blob/master/Encodings.md#delta-encoding-delta_binary_packed--5\"\u003e\n * Parquet format encodings: DELTA_BINARY_PACKED\u003c/a\u003e\n */"
			},
			{
				"signature": "int getTotalValueCount()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte readByte()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short readShort()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int readInteger()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long readLong()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBytes(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readShorts(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readIntegers(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readIntegersWithRebase(\n      int total, WritableColumnVector c, int rowId, boolean failIfRebase)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readUnsignedIntegers(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readUnsignedLongs(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readLongs(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readLongsWithRebase(\n      int total, WritableColumnVector c, int rowId, boolean failIfRebase, String timeZone)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBytes(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipShorts(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipIntegers(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipLongs(int total)",
				"documentation": ""
			},
			{
				"signature": "private void readValues(int total, WritableColumnVector c, int rowId,\n      IntegerOutputWriter outputWriter)",
				"documentation": ""
			},
			{
				"signature": "private int loadMiniBlockToOutput(int remaining, WritableColumnVector c, int rowId,\n      IntegerOutputWriter outputWriter) throws IOException",
				"documentation": "/**\n   * Read from a mini block.  Read at most 'remaining' values into output.\n   *\n   * @return the number of values read into output\n   */"
			},
			{
				"signature": "private void readBlockHeader()",
				"documentation": ""
			},
			{
				"signature": "private void unpackMiniBlock() throws IOException",
				"documentation": "/**\n   * mini block has a size of 8*n, unpack 32 value each time\n   *\n   * see org.apache.parquet.column.values.delta.DeltaBinaryPackingValuesReader#unpackMiniBlock\n   */"
			},
			{
				"signature": "private void readBitWidthsForMiniBlocks()",
				"documentation": ""
			},
			{
				"signature": "private void skipValues(int total)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An implementation of the Parquet DELTA_BYTE_ARRAY decoder that supports the vectorized\n * interface.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaByteArrayReader",
		"extends": "org.apache.spark.sql.execution.datasources.parquet.VectorizedReaderBase",
		"Methods": [
			{
				"signature": "VectorizedDeltaByteArrayReader()",
				"documentation": "/**\n * An implementation of the Parquet DELTA_BYTE_ARRAY decoder that supports the vectorized\n * interface.\n */"
			},
			{
				"signature": "@Override\n  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Binary readBinary(int len)",
				"documentation": ""
			},
			{
				"signature": "private void readValues(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBinary(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setPreviousReader(ValuesReader reader)",
				"documentation": "/**\n   * There was a bug (PARQUET-246) in which DeltaByteArrayWriter's reset() method did not clear the\n   * previous value state that it tracks internally. This resulted in the first value of all pages\n   * (except for the first page) to be a delta from the last value of the previous page. In order to\n   * read corrupted files written with this bug, when reading a new page we need to recover the\n   * previous page's last value to use it (if needed) to read the first value.\n   */"
			},
			{
				"signature": "@Override\n  public void skipBinary(int total)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedValuesReader",
			"org.apache.parquet.column.values.RequiresPreviousReader"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An implementation of the Parquet DELTA_LENGTH_BYTE_ARRAY decoder that supports the vectorized\n * interface.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaLengthByteArrayReader",
		"extends": "org.apache.spark.sql.execution.datasources.parquet.VectorizedReaderBase",
		"Methods": [
			{
				"signature": "VectorizedDeltaLengthByteArrayReader()",
				"documentation": "/**\n * An implementation of the Parquet DELTA_LENGTH_BYTE_ARRAY decoder that supports the vectorized\n * interface.\n */"
			},
			{
				"signature": "@Override\n  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBinary(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "public ByteBuffer getBytes(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBinary(int total)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedValuesReader"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A specialized RecordReader that reads into InternalRows or ColumnarBatches directly using the\n * Parquet column APIs. This is somewhat based on parquet-mr's ColumnReader.\n *\n * TODO: decimal requiring more than 8 bytes, INT96. Schema mismatch.\n * All of these can be handled efficiently and easily with codegen.\n *\n * This class can either return InternalRows or ColumnarBatches. With whole stage codegen\n * enabled, this class returns ColumnarBatches which offers significant performance gains.\n * TODO: make this always return ColumnarBatches.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader",
		"extends": "org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase",
		"Methods": [
			{
				"signature": "public VectorizedParquetRecordReader(\n      ZoneId convertTz,\n      String datetimeRebaseMode,\n      String datetimeRebaseTz,\n      String int96RebaseMode,\n      String int96RebaseTz,\n      boolean useOffHeap,\n      int capacity)",
				"documentation": "/**\n   * The memory mode of the columnarBatch\n   */"
			},
			{
				"signature": "public VectorizedParquetRecordReader(boolean useOffHeap, int capacity)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)\n      throws IOException, InterruptedException, UnsupportedOperationException",
				"documentation": "/**\n   * Implementation of RecordReader API.\n   */"
			},
			{
				"signature": "@Override\n  public void initialize(String path, List\u003cString\u003e columns) throws IOException,\n      UnsupportedOperationException",
				"documentation": "/**\n   * Utility API that will read all the data in path. This circumvents the need to create Hadoop\n   * objects to use this class. `columns` can contain the list of columns to project.\n   */"
			},
			{
				"signature": "@VisibleForTesting\n  @Override\n  public void initialize(\n      MessageType fileSchema,\n      MessageType requestedSchema,\n      ParquetRowGroupReader rowGroupReader,\n      int totalRowCount) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean nextKeyValue() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object getCurrentValue()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getProgress()",
				"documentation": ""
			},
			{
				"signature": "private void initBatch(\n      MemoryMode memMode,\n      StructType partitionColumns,\n      InternalRow partitionValues)",
				"documentation": ""
			},
			{
				"signature": "private void initBatch()",
				"documentation": ""
			},
			{
				"signature": "public void initBatch(StructType partitionColumns, InternalRow partitionValues)",
				"documentation": ""
			},
			{
				"signature": "public ColumnarBatch resultBatch()",
				"documentation": "/**\n   * Returns the ColumnarBatch object that will be used for all rows returned by this reader.\n   * This object is reused. Calling this enables the vectorized reader. This should be called\n   * before any calls to nextKeyValue/nextBatch.\n   */"
			},
			{
				"signature": "public void enableReturningBatches()",
				"documentation": "/**\n   * Can be called before any rows are returned to enable returning columnar batches directly.\n   */"
			},
			{
				"signature": "public boolean nextBatch() throws IOException",
				"documentation": "/**\n   * Advances to the next batch of rows. Returns false if there are no more.\n   */"
			},
			{
				"signature": "private void initializeInternal() throws IOException, UnsupportedOperationException",
				"documentation": ""
			},
			{
				"signature": "private void checkColumn(ParquetColumn column) throws IOException",
				"documentation": "/**\n   * Check whether a column from requested schema is missing from the file schema, or whether it\n   * conforms to the type of the file schema.\n   */"
			},
			{
				"signature": "private boolean containsPath(Type parquetType, String[] path)",
				"documentation": "/**\n   * Checks whether the given 'path' exists in 'parquetType'. The difference between this and\n   * {@link MessageType#containsPath(String[])} is that the latter only support paths to leaf\n   * nodes, while this support paths both to leaf and non-leaf nodes.\n   */"
			},
			{
				"signature": "private boolean containsPath(Type parquetType, String[] path, int depth)",
				"documentation": ""
			},
			{
				"signature": "private void checkEndOfRowGroup() throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void initColumnReader(PageReadStore pages, ParquetColumnVector cv) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.vectorized.ColumnarBatch",
			"org.apache.spark.sql.execution.vectorized.ColumnVectorUtils",
			"org.apache.spark.sql.execution.vectorized.OffHeapColumnVector",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An implementation of the Parquet PLAIN decoder that supports the vectorized interface.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader",
		"extends": "org.apache.parquet.column.values.ValuesReader",
		"Methods": [
			{
				"signature": "public VectorizedPlainValuesReader()",
				"documentation": "/**\n * An implementation of the Parquet PLAIN decoder that supports the vectorized interface.\n */"
			},
			{
				"signature": "@Override\n  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skip()",
				"documentation": ""
			},
			{
				"signature": "private void updateCurrentByte()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readBooleans(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void skipBooleans(int total)",
				"documentation": ""
			},
			{
				"signature": "private ByteBuffer getBuffer(int length)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readIntegers(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipIntegers(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readUnsignedIntegers(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readIntegersWithRebase(\n      int total, WritableColumnVector c, int rowId, boolean failIfRebase)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readLongs(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipLongs(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readUnsignedLongs(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readLongsWithRebase(\n      int total,\n      WritableColumnVector c,\n      int rowId,\n      boolean failIfRebase,\n      String timeZone)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readFloats(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipFloats(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readDoubles(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipDoubles(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readBytes(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void skipBytes(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readShorts(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipShorts(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final boolean readBoolean()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final int readInteger()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final long readLong()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final byte readByte()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short readShort()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final float readFloat()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final double readDouble()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final void readBinary(int total, WritableColumnVector v, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBinary(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final Binary readBinary(int len)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipFixedLenByteArray(int total, int len)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedValuesReader"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Base class for implementations of VectorizedValuesReader. Mainly to avoid duplication\n * of methods that are not supported by concrete implementations\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedReaderBase",
		"extends": "org.apache.parquet.column.values.ValuesReader",
		"Methods": [
			{
				"signature": "@Override\n  public void skip()",
				"documentation": "/**\n * Base class for implementations of VectorizedValuesReader. Mainly to avoid duplication\n * of methods that are not supported by concrete implementations\n */"
			},
			{
				"signature": "@Override\n  public byte readByte()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short readShort()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Binary readBinary(int len)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBooleans(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBytes(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readShorts(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readIntegers(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readIntegersWithRebase(int total, WritableColumnVector c, int rowId,\n      boolean failIfRebase)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readUnsignedIntegers(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readUnsignedLongs(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readLongs(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readLongsWithRebase(int total, WritableColumnVector c, int rowId,\n      boolean failIfRebase, String timeZone)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readFloats(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readDoubles(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBinary(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBooleans(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBytes(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipShorts(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipIntegers(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipLongs(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipFloats(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipDoubles(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBinary(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipFixedLenByteArray(int total, int len)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedValuesReader"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaBinaryPackedReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaByteArrayReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaLengthByteArrayReader"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A values reader for Parquet's run-length encoded data. This is based off of the version in\n * parquet-mr with these changes:\n *  - Supports the vectorized interface.\n *  - Works on byte arrays(byte[]) instead of making byte streams.\n *\n * This encoding is used in multiple places:\n *  - Definition/Repetition levels\n *  - Dictionary ids.\n *  - Boolean type values of Parquet DataPageV2\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader",
		"extends": "org.apache.parquet.column.values.ValuesReader",
		"Methods": [
			{
				"signature": "public VectorizedRleValuesReader()",
				"documentation": ""
			},
			{
				"signature": "public VectorizedRleValuesReader(int bitWidth)",
				"documentation": ""
			},
			{
				"signature": "public VectorizedRleValuesReader(int bitWidth, boolean readLength)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException",
				"documentation": ""
			},
			{
				"signature": "private void init(int bitWidth)",
				"documentation": "/**\n   * Initializes the internal state for decoding ints of `bitWidth`.\n   */"
			},
			{
				"signature": "@Override\n  public boolean readBoolean()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skip()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int readValueDictionaryId()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int readInteger()",
				"documentation": ""
			},
			{
				"signature": "public void readBatch(\n      ParquetReadState state,\n      WritableColumnVector values,\n      WritableColumnVector defLevels,\n      VectorizedValuesReader valueReader,\n      ParquetVectorUpdater updater)",
				"documentation": "/**\n   * Reads a batch of definition levels and values into vector 'defLevels' and 'values'\n   * respectively. The values are read using 'valueReader'.\n   * \u003cp\u003e\n   * The related states such as row index, offset, number of values left in the batch and page,\n   * are tracked by 'state'. The type-specific 'updater' is used to update or skip values.\n   * \u003cp\u003e\n   * This reader reads the definition levels and then will read from 'valueReader' for the\n   * non-null values. If the value is null, 'values' will be populated with null value.\n   */"
			},
			{
				"signature": "public void readIntegers(\n      ParquetReadState state,\n      WritableColumnVector values,\n      WritableColumnVector nulls,\n      WritableColumnVector defLevels,\n      VectorizedValuesReader valueReader)",
				"documentation": "/**\n   * Decoding for dictionary ids. The IDs are populated into 'values' and the nullability is\n   * populated into 'nulls'.\n   */"
			},
			{
				"signature": "private void readBatchInternal(\n      ParquetReadState state,\n      WritableColumnVector values,\n      WritableColumnVector nulls,\n      WritableColumnVector defLevels,\n      VectorizedValuesReader valueReader,\n      ParquetVectorUpdater updater)",
				"documentation": ""
			},
			{
				"signature": "public void readBatchRepeated(\n      ParquetReadState state,\n      WritableColumnVector repLevels,\n      VectorizedRleValuesReader defLevelsReader,\n      WritableColumnVector defLevels,\n      WritableColumnVector values,\n      VectorizedValuesReader valueReader,\n      ParquetVectorUpdater updater)",
				"documentation": "/**\n   * Reads a batch of repetition levels, definition levels and values into 'repLevels',\n   * 'defLevels' and 'values' respectively. The definition levels and values are read via\n   * 'defLevelsReader' and 'valueReader' respectively.\n   * \u003cp\u003e\n   * The related states such as row index, offset, number of rows left in the batch and page,\n   * are tracked by 'state'. The type-specific 'updater' is used to update or skip values.\n   */"
			},
			{
				"signature": "public void readIntegersRepeated(\n      ParquetReadState state,\n      WritableColumnVector repLevels,\n      VectorizedRleValuesReader defLevelsReader,\n      WritableColumnVector defLevels,\n      WritableColumnVector values,\n      WritableColumnVector nulls,\n      VectorizedValuesReader valueReader)",
				"documentation": "/**\n   * Reads a batch of repetition levels, definition levels and integer values into 'repLevels',\n   * 'defLevels', 'values' and 'nulls' respectively. The definition levels and values are read via\n   * 'defLevelsReader' and 'valueReader' respectively.\n   * \u003cp\u003e\n   * The 'values' vector is used to hold non-null values, while 'nulls' vector is used to hold\n   * null values.\n   * \u003cp\u003e\n   * The related states such as row index, offset, number of rows left in the batch and page,\n   * are tracked by 'state'.\n   * \u003cp\u003e\n   * Unlike 'readBatchRepeated', this is used to decode dictionary indices in dictionary encoding.\n   */"
			},
			{
				"signature": "public void readBatchRepeatedInternal(\n      ParquetReadState state,\n      WritableColumnVector repLevels,\n      VectorizedRleValuesReader defLevelsReader,\n      WritableColumnVector defLevels,\n      WritableColumnVector values,\n      WritableColumnVector nulls,\n      boolean valuesReused,\n      VectorizedValuesReader valueReader,\n      ParquetVectorUpdater updater)",
				"documentation": "/**\n   * Keep reading repetition level values from the page until either: 1) we've read enough\n   * top-level rows to fill the current batch, or 2) we've drained the data page completely.\n   *\n   * @param valuesReused whether 'values' vector is reused for 'nulls'\n   */"
			},
			{
				"signature": "private void readValues(\n      int total,\n      ParquetReadState state,\n      WritableColumnVector defLevels,\n      WritableColumnVector values,\n      WritableColumnVector nulls,\n      boolean valuesReused,\n      VectorizedValuesReader valueReader,\n      ParquetVectorUpdater updater)",
				"documentation": "/**\n   * Read the next 'total' values (either null or non-null) from this definition level reader and\n   * 'valueReader'. The definition levels are read into 'defLevels'. If a value is not\n   * null, it is appended to 'values'. Otherwise, a null bit will be set in 'nulls'.\n   *\n   * This is only used when reading repeated values.\n   */"
			},
			{
				"signature": "private void readValuesN(\n      int n,\n      ParquetReadState state,\n      WritableColumnVector defLevels,\n      WritableColumnVector values,\n      WritableColumnVector nulls,\n      VectorizedValuesReader valueReader,\n      ParquetVectorUpdater updater)",
				"documentation": ""
			},
			{
				"signature": "private void skipValues(\n      int n,\n      ParquetReadState state,\n      VectorizedValuesReader valuesReader,\n      ParquetVectorUpdater updater)",
				"documentation": "/**\n   * Skip the next `n` values (either null or non-null) from this definition level reader and\n   * `valueReader`.\n   */"
			},
			{
				"signature": "@Override\n  public void readIntegers(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readUnsignedIntegers(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readUnsignedLongs(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readIntegersWithRebase(\n      int total, WritableColumnVector c, int rowId, boolean failIfRebase)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte readByte()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short readShort()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBytes(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readShorts(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readLongs(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readLongsWithRebase(\n      int total,\n      WritableColumnVector c,\n      int rowId,\n      boolean failIfRebase,\n      String timeZone)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBinary(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readBooleans(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readFloats(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void readDoubles(int total, WritableColumnVector c, int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Binary readBinary(int len)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipIntegers(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBooleans(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBytes(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipShorts(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipLongs(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipFloats(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipDoubles(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipBinary(int total)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void skipFixedLenByteArray(int total, int len)",
				"documentation": ""
			},
			{
				"signature": "private int readUnsignedVarInt() throws IOException",
				"documentation": "/**\n   * Reads the next varint encoded int.\n   */"
			},
			{
				"signature": "do",
				"documentation": "/**\n   * Reads the next varint encoded int.\n   */"
			},
			{
				"signature": "private int readIntLittleEndian() throws IOException",
				"documentation": "/**\n   * Reads the next 4 byte little endian int.\n   */"
			},
			{
				"signature": "private int readIntLittleEndianPaddedOnBitWidth() throws IOException",
				"documentation": "/**\n   * Reads the next byteWidth little endian int.\n   */"
			},
			{
				"signature": "case 2:",
				"documentation": "/**\n   * Reads the next byteWidth little endian int.\n   */"
			},
			{
				"signature": "case 3:",
				"documentation": ""
			},
			{
				"signature": "case 4:",
				"documentation": ""
			},
			{
				"signature": "private boolean readNextGroup()",
				"documentation": "/**\n   * Reads the next group. Returns false if no more group available.\n   */"
			},
			{
				"signature": "private void skipValues(int n)",
				"documentation": "/**\n   * Skip `n` values from the current reader.\n   */"
			}
		],
		"interfaces": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedValuesReader"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.DefLevelProcessor"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.MODE",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.DefLevelProcessor"
		]
	},
	{
		"documentation": "/**\n * A values reader for Parquet's run-length encoded data. This is based off of the version in\n * parquet-mr with these changes:\n *  - Supports the vectorized interface.\n *  - Works on byte arrays(byte[]) instead of making byte streams.\n *\n * This encoding is used in multiple places:\n *  - Definition/Repetition levels\n *  - Dictionary ids.\n *  - Boolean type values of Parquet DataPageV2\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.MODE",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.DefLevelProcessor",
		"extends": "",
		"Methods": [
			{
				"signature": "DefLevelProcessor(\n        VectorizedRleValuesReader reader,\n        ParquetReadState state,\n        WritableColumnVector defLevels,\n        WritableColumnVector values,\n        WritableColumnVector nulls,\n        boolean valuesReused,\n        VectorizedValuesReader valueReader,\n        ParquetVectorUpdater updater)",
				"documentation": ""
			},
			{
				"signature": "void readValues(int n)",
				"documentation": ""
			},
			{
				"signature": "void skipValues(int n)",
				"documentation": ""
			},
			{
				"signature": "void finish()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Interface for value decoding that supports vectorized (aka batched) decoding.\n * TODO: merge this into parquet-mr.\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.VectorizedValuesReader",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaByteArrayReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaLengthByteArrayReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedReaderBase",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.IntegerOutputWriter",
			"org.apache.spark.sql.execution.datasources.parquet.ByteBufferOutputWriter"
		]
	},
	{
		"documentation": "/**\n   * A functional interface to write integer values to columnar output\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.IntegerOutputWriter",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.execution.datasources.parquet.ByteBufferOutputWriter",
		"extends": "",
		"Methods": [
			{
				"signature": "static void writeArrayByteBuffer(WritableColumnVector c, int rowId, ByteBuffer val,\n        int length)",
				"documentation": ""
			},
			{
				"signature": "static void skipWrite(WritableColumnVector c, int rowId, ByteBuffer val, int length)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class is an alias of {@link org.apache.spark.sql.connector.read.streaming.Offset}. It's\n * internal and deprecated. New streaming data source implementations should use data source v2 API,\n * which will be supported in the long term.\n *\n * This class will be removed in a future release.\n */",
		"name": "org.apache.spark.sql.execution.streaming.Offset",
		"extends": "org.apache.spark.sql.connector.read.streaming.Offset",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This is an illustrative implementation of an append-only single-key/single value aggregate hash\n * map that can act as a 'cache' for extremely fast key-value lookups while evaluating aggregates\n * (and fall back to the `BytesToBytesMap` if a given key isn't found). This can be potentially\n * 'codegened' in HashAggregate to speed up aggregates w/ key.\n *\n * It is backed by a power-of-2-sized array for index lookups and a columnar batch that stores the\n * key-value pairs. The index lookups in the array rely on linear probing (with a small number of\n * maximum tries) and use an inexpensive hash function which makes it really efficient for a\n * majority of lookups. However, using linear probing and an inexpensive hash function also makes it\n * less robust as compared to the `BytesToBytesMap` (especially for a large number of keys or even\n * for certain distribution of keys) and requires us to fall back on the latter for correctness.\n */",
		"name": "org.apache.spark.sql.execution.vectorized.AggregateHashMap",
		"extends": "",
		"Methods": [
			{
				"signature": "public AggregateHashMap(StructType schema, int capacity, double loadFactor, int maxSteps)",
				"documentation": "/**\n * This is an illustrative implementation of an append-only single-key/single value aggregate hash\n * map that can act as a 'cache' for extremely fast key-value lookups while evaluating aggregates\n * (and fall back to the `BytesToBytesMap` if a given key isn't found). This can be potentially\n * 'codegened' in HashAggregate to speed up aggregates w/ key.\n *\n * It is backed by a power-of-2-sized array for index lookups and a columnar batch that stores the\n * key-value pairs. The index lookups in the array rely on linear probing (with a small number of\n * maximum tries) and use an inexpensive hash function which makes it really efficient for a\n * majority of lookups. However, using linear probing and an inexpensive hash function also makes it\n * less robust as compared to the `BytesToBytesMap` (especially for a large number of keys or even\n * for certain distribution of keys) and requires us to fall back on the latter for correctness.\n */"
			},
			{
				"signature": "public AggregateHashMap(StructType schema)",
				"documentation": ""
			},
			{
				"signature": "public MutableColumnarRow findOrInsert(long key)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public int find(long key)",
				"documentation": ""
			},
			{
				"signature": "private long hash(long key)",
				"documentation": ""
			},
			{
				"signature": "private boolean equals(int idx, long key1)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Utilities to help manipulate data associate with ColumnVectors. These should be used mostly\n * for debugging or other non-performance critical paths.\n * These utilities are mostly used to convert ColumnVectors into other formats.\n */",
		"name": "org.apache.spark.sql.execution.vectorized.ColumnVectorUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static void populate(WritableColumnVector col, InternalRow row, int fieldIdx)",
				"documentation": "/**\n   * Populates the entire `col` with `row[fieldIdx]`\n   */"
			},
			{
				"signature": "public static int[] toJavaIntArray(ColumnarArray array)",
				"documentation": "/**\n   * Returns the array data as the java primitive array.\n   * For example, an array of IntegerType will return an int[].\n   * Throws exceptions for unhandled schemas.\n   */"
			},
			{
				"signature": "public static Map\u003cInteger, Integer\u003e toJavaIntMap(ColumnarMap map)",
				"documentation": ""
			},
			{
				"signature": "private static void appendValue(WritableColumnVector dst, DataType t, Object o)",
				"documentation": ""
			},
			{
				"signature": "private static void appendValue(WritableColumnVector dst, DataType t, Row src, int fieldIdx)",
				"documentation": ""
			},
			{
				"signature": "public static ColumnarBatch toBatch(\n      StructType schema, MemoryMode memMode, Iterator\u003cRow\u003e row)",
				"documentation": "/**\n   * Converts an iterator of rows into a single ColumnBatch.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.vectorized.ColumnarBatch"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class adds the constant support to ColumnVector.\n * It supports all the types and contains `set` APIs,\n * which will set the exact same value to all rows.\n *\n * Capacity: The vector stores only one copy of the data.\n */",
		"name": "org.apache.spark.sql.execution.vectorized.ConstantColumnVector",
		"extends": "org.apache.spark.sql.vectorized.ColumnVector",
		"Methods": [
			{
				"signature": "public ConstantColumnVector(int numRows, DataType type)",
				"documentation": "/**\n   * @param numRows: The number of rows for this ConstantColumnVector\n   * @param type: The data type of this ConstantColumnVector\n   */"
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean hasNull()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numNulls()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setNull()",
				"documentation": "/**\n   * Sets all rows as `null`\n   */"
			},
			{
				"signature": "public void setNotNull()",
				"documentation": "/**\n   * Sets all rows as not `null`\n   */"
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setBoolean(boolean value)",
				"documentation": "/**\n   * Sets the boolean `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setByte(byte value)",
				"documentation": "/**\n   * Sets the byte `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setShort(short value)",
				"documentation": "/**\n   * Sets the short `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setInt(int value)",
				"documentation": "/**\n   * Sets the int `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setLong(long value)",
				"documentation": "/**\n   * Sets the long `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setFloat(float value)",
				"documentation": "/**\n   * Sets the float `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setDouble(double value)",
				"documentation": "/**\n   * Sets the double `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setArray(ColumnarArray value)",
				"documentation": "/**\n   * Sets the `ColumnarArray` `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "public void setMap(ColumnarMap value)",
				"documentation": "/**\n   * Sets the `ColumnarMap` `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "public void setDecimal(Decimal value, int precision)",
				"documentation": "/**\n   * Sets the `Decimal` `value` with the precision for all rows\n   */"
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setUtf8String(UTF8String value)",
				"documentation": "/**\n   * Sets the `UTF8String` `value` for all rows\n   */"
			},
			{
				"signature": "private void setByteArray(byte[] value)",
				"documentation": "/**\n   * Sets the byte array `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public void setBinary(byte[] value)",
				"documentation": "/**\n   * Sets the binary `value` for all rows\n   */"
			},
			{
				"signature": "@Override\n  public ColumnVector getChild(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "public void setChild(int ordinal, ConstantColumnVector value)",
				"documentation": "/**\n   * Sets the child `ConstantColumnVector` `value` at the given ordinal for all rows\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The interface for dictionary in ColumnVector to decode dictionary encoded values.\n */",
		"name": "org.apache.spark.sql.execution.vectorized.Dictionary",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A mutable version of {@link ColumnarRow}, which is used in the vectorized hash map for hash\n * aggregate, and {@link ColumnarBatch} to save object creation.\n *\n * Note that this class intentionally has a lot of duplicated code with {@link ColumnarRow}, to\n * avoid java polymorphism overhead by keeping {@link ColumnarRow} and this class final classes.\n */",
		"name": "org.apache.spark.sql.execution.vectorized.MutableColumnarRow",
		"extends": "org.apache.spark.sql.catalyst.InternalRow",
		"Methods": [
			{
				"signature": "public MutableColumnarRow(WritableColumnVector[] writableColumns)",
				"documentation": "/**\n * A mutable version of {@link ColumnarRow}, which is used in the vectorized hash map for hash\n * aggregate, and {@link ColumnarBatch} to save object creation.\n *\n * Note that this class intentionally has a lot of duplicated code with {@link ColumnarRow}, to\n * avoid java polymorphism overhead by keeping {@link ColumnarRow} and this class final classes.\n */"
			},
			{
				"signature": "@Override\n  public int numFields()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public InternalRow copy()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean anyNull()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int ordinal, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public CalendarInterval getInterval(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarRow getStruct(int ordinal, int numFields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarArray getArray(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnarMap getMap(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object get(int ordinal, DataType dataType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void update(int ordinal, Object value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setNullAt(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setBoolean(int ordinal, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setByte(int ordinal, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setShort(int ordinal, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setInt(int ordinal, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setLong(int ordinal, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setFloat(int ordinal, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setDouble(int ordinal, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setDecimal(int ordinal, Decimal value, int precision)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setInterval(int ordinal, CalendarInterval value)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Column data backed using offheap memory.\n */",
		"name": "org.apache.spark.sql.execution.vectorized.OffHeapColumnVector",
		"extends": "org.apache.spark.sql.execution.vectorized.WritableColumnVector",
		"Methods": [
			{
				"signature": "public static OffHeapColumnVector[] allocateColumns(int capacity, StructType schema)",
				"documentation": "/**\n   * Allocates columns to store elements of each field of the schema off heap.\n   * Capacity is the initial capacity of the vector and it will grow as necessary. Capacity is\n   * in number of elements, not number of bytes.\n   */"
			},
			{
				"signature": "public static OffHeapColumnVector[] allocateColumns(int capacity, StructField[] fields)",
				"documentation": "/**\n   * Allocates columns to store elements of each field off heap.\n   * Capacity is the initial capacity of the vector and it will grow as necessary. Capacity is\n   * in number of elements, not number of bytes.\n   */"
			},
			{
				"signature": "public OffHeapColumnVector(int capacity, DataType type)",
				"documentation": ""
			},
			{
				"signature": "@VisibleForTesting\n  public long valuesNativeAddress()",
				"documentation": "/**\n   * Returns the off heap pointer for the values buffer.\n   */"
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putNotNull(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putNull(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putNulls(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putNotNulls(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBoolean(int rowId, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBooleans(int rowId, int count, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBooleans(int rowId, byte src)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean[] getBooleans(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putByte(int rowId, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBytes(int rowId, int count, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBytes(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBytes(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected UTF8String getBytesAsUTF8String(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ByteBuffer getByteBuffer(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putShort(int rowId, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putShorts(int rowId, int count, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putShorts(int rowId, int count, short[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putShorts(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short[] getShorts(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putInt(int rowId, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putInts(int rowId, int count, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putInts(int rowId, int count, int[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putInts(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int[] getInts(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "public int getDictId(int rowId)",
				"documentation": "/**\n   * Returns the dictionary Id for rowId.\n   * This should only be called when the ColumnVector is dictionaryIds.\n   * We have this separate method for dictionaryIds as per SPARK-16928.\n   */"
			},
			{
				"signature": "@Override\n  public void putLong(int rowId, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putLongs(int rowId, int count, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putLongs(int rowId, int count, long[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putLongs(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long[] getLongs(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloat(int rowId, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloats(int rowId, int count, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloats(int rowId, int count, float[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloats(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloatsLittleEndian(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float[] getFloats(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDouble(int rowId, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDoubles(int rowId, int count, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDoubles(int rowId, int count, double[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDoubles(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDoublesLittleEndian(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double[] getDoubles(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putArray(int rowId, int offset, int length)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getArrayLength(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getArrayOffset(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int putByteArray(int rowId, byte[] value, int offset, int length)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected void reserveInternal(int newCapacity)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected OffHeapColumnVector reserveNewColumn(int capacity, DataType type)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.ParquetColumnVector",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A column backed by an in memory JVM array. This stores the NULLs as a byte per value\n * and a java array for the values.\n */",
		"name": "org.apache.spark.sql.execution.vectorized.OnHeapColumnVector",
		"extends": "org.apache.spark.sql.execution.vectorized.WritableColumnVector",
		"Methods": [
			{
				"signature": "public static OnHeapColumnVector[] allocateColumns(int capacity, StructType schema)",
				"documentation": "/**\n   * Allocates columns to store elements of each field of the schema on heap.\n   * Capacity is the initial capacity of the vector and it will grow as necessary. Capacity is\n   * in number of elements, not number of bytes.\n   */"
			},
			{
				"signature": "public static OnHeapColumnVector[] allocateColumns(int capacity, StructField[] fields)",
				"documentation": "/**\n   * Allocates columns to store elements of each field on heap.\n   * Capacity is the initial capacity of the vector and it will grow as necessary. Capacity is\n   * in number of elements, not number of bytes.\n   */"
			},
			{
				"signature": "public OnHeapColumnVector(int capacity, DataType type)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putNotNull(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putNull(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putNulls(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putNotNulls(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isNullAt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBoolean(int rowId, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBooleans(int rowId, int count, boolean value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBooleans(int rowId, byte src)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean getBoolean(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean[] getBooleans(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putByte(int rowId, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBytes(int rowId, int count, byte value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putBytes(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte getByte(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBytes(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected UTF8String getBytesAsUTF8String(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ByteBuffer getByteBuffer(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putShort(int rowId, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putShorts(int rowId, int count, short value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putShorts(int rowId, int count, short[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putShorts(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short getShort(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public short[] getShorts(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putInt(int rowId, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putInts(int rowId, int count, int value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putInts(int rowId, int count, int[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putInts(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getInt(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int[] getInts(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "public int getDictId(int rowId)",
				"documentation": "/**\n   * Returns the dictionary Id for rowId.\n   * This should only be called when the ColumnVector is dictionaryIds.\n   * We have this separate method for dictionaryIds as per SPARK-16928.\n   */"
			},
			{
				"signature": "@Override\n  public void putLong(int rowId, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putLongs(int rowId, int count, long value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putLongs(int rowId, int count, long[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putLongs(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLong(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long[] getLongs(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloat(int rowId, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloats(int rowId, int count, float value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloats(int rowId, int count, float[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloats(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putFloatsLittleEndian(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getFloat(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float[] getFloats(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDouble(int rowId, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDoubles(int rowId, int count, double value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDoubles(int rowId, int count, double[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDoubles(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putDoublesLittleEndian(int rowId, int count, byte[] src, int srcIndex)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double getDouble(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public double[] getDoubles(int rowId, int count)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getArrayLength(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int getArrayOffset(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void putArray(int rowId, int offset, int length)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int putByteArray(int rowId, byte[] value, int offset, int length)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected void reserveInternal(int newCapacity)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected OnHeapColumnVector reserveNewColumn(int capacity, DataType type)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.orc.OrcColumnarBatchReader",
			"org.apache.spark.sql.execution.datasources.parquet.ParquetColumnVector",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaByteArrayReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedDeltaLengthByteArrayReader",
			"org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.ColumnarReaderFactory"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class adds write APIs to ColumnVector.\n * It supports all the types and contains put APIs as well as their batched versions.\n * The batched versions are preferable whenever possible.\n *\n * Capacity: The data stored is dense but the arrays are not fixed capacity. It is the\n * responsibility of the caller to call reserve() to ensure there is enough room before adding\n * elements. This means that the put() APIs do not check as in common cases (i.e. flat schemas),\n * the lengths are known up front.\n *\n * A WritableColumnVector should be considered immutable once originally created. In other words,\n * it is not valid to call put APIs after reads until reset() is called.\n *\n * WritableColumnVector are intended to be reused.\n */",
		"name": "org.apache.spark.sql.execution.vectorized.WritableColumnVector",
		"extends": "org.apache.spark.sql.vectorized.ColumnVector",
		"Methods": [
			{
				"signature": "public void reset()",
				"documentation": "/**\n   * Resets this column for writing. The currently stored values are no longer accessible.\n   */"
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "public void reserveAdditional(int additionalCapacity)",
				"documentation": ""
			},
			{
				"signature": "public void reserve(int requiredCapacity)",
				"documentation": ""
			},
			{
				"signature": "private void throwUnsupportedException(int requiredCapacity, Throwable cause)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean hasNull()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numNulls()",
				"documentation": ""
			},
			{
				"signature": "public boolean hasDictionary()",
				"documentation": "/**\n   * Returns true if this column has a dictionary.\n   */"
			},
			{
				"signature": "public WritableColumnVector getDictionaryIds()",
				"documentation": "/**\n   * Returns the underlying integer column for ids of dictionary.\n   */"
			},
			{
				"signature": "public void setDictionary(Dictionary dictionary)",
				"documentation": "/**\n   * Update the dictionary.\n   */"
			},
			{
				"signature": "public WritableColumnVector reserveDictionaryIds(int capacity)",
				"documentation": "/**\n   * Reserve a integer column for ids of dictionary.\n   */"
			},
			{
				"signature": "public void putBooleans(int rowId, int count, byte src, int srcIndex)",
				"documentation": "/**\n   * Sets bits from [src[srcIndex], src[srcIndex + count]) to [rowId, rowId + count)\n   * src must contain bit-packed 8 booleans in the byte.\n   */"
			},
			{
				"signature": "public final int putByteArray(int rowId, byte[] value)",
				"documentation": "/**\n   * Sets values from [value + offset, value + offset + count) to the values at rowId.\n   */"
			},
			{
				"signature": "@Override\n  public Decimal getDecimal(int rowId, int precision, int scale)",
				"documentation": ""
			},
			{
				"signature": "public void putDecimal(int rowId, Decimal value, int precision)",
				"documentation": ""
			},
			{
				"signature": "public void putInterval(int rowId, CalendarInterval value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public UTF8String getUTF8String(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] getBinary(int rowId)",
				"documentation": "/**\n   * Gets the values of bytes from [rowId, rowId + count), as a UTF8String.\n   * This method is similar to {@link ColumnVector#getBytes(int, int)}, but can save data copy as\n   * UTF8String is used as a pointer.\n   */"
			},
			{
				"signature": "public final int appendNull()",
				"documentation": "/**\n   * Append APIs. These APIs all behave similarly and will append data to the current vector.  It\n   * is not valid to mix the put and append APIs. The append APIs are slower and should only be\n   * used if the sizes are not known up front.\n   * In all these cases, the return value is the rowId for the first appended element.\n   */"
			},
			{
				"signature": "public final int appendNotNull()",
				"documentation": ""
			},
			{
				"signature": "public final int appendNulls(int count)",
				"documentation": ""
			},
			{
				"signature": "public final int appendNotNulls(int count)",
				"documentation": ""
			},
			{
				"signature": "public final int appendBoolean(boolean v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendBooleans(int count, boolean v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendBooleans(int count, byte src, int offset)",
				"documentation": "/**\n   * Append bits from [src[offset], src[offset + count])\n   * src must contain bit-packed 8 booleans in the byte.\n   */"
			},
			{
				"signature": "public final int appendByte(byte v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendBytes(int count, byte v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendBytes(int length, byte[] src, int offset)",
				"documentation": ""
			},
			{
				"signature": "public final int appendShort(short v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendShorts(int count, short v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendShorts(int length, short[] src, int offset)",
				"documentation": ""
			},
			{
				"signature": "public final int appendInt(int v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendInts(int count, int v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendInts(int length, int[] src, int offset)",
				"documentation": ""
			},
			{
				"signature": "public final int appendLong(long v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendLongs(int count, long v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendLongs(int length, long[] src, int offset)",
				"documentation": ""
			},
			{
				"signature": "public final int appendFloat(float v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendFloats(int count, float v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendFloats(int length, float[] src, int offset)",
				"documentation": ""
			},
			{
				"signature": "public final int appendDouble(double v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendDoubles(int count, double v)",
				"documentation": ""
			},
			{
				"signature": "public final int appendDoubles(int length, double[] src, int offset)",
				"documentation": ""
			},
			{
				"signature": "public final int appendByteArray(byte[] value, int offset, int length)",
				"documentation": ""
			},
			{
				"signature": "public final int appendArray(int length)",
				"documentation": ""
			},
			{
				"signature": "public final int appendStruct(boolean isNull)",
				"documentation": "/**\n   * Appends a NULL struct. This *has* to be used for structs instead of appendNull() as this\n   * recursively appends a NULL to its children.\n   * We don't have this logic as the general appendNull implementation to optimize the more\n   * common non-struct case.\n   */"
			},
			{
				"signature": "@Override\n  public final ColumnarArray getArray(int rowId)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public final ColumnarMap getMap(int rowId)",
				"documentation": ""
			},
			{
				"signature": "public WritableColumnVector arrayData()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public WritableColumnVector getChild(int ordinal)",
				"documentation": ""
			},
			{
				"signature": "public int getNumChildren()",
				"documentation": "/**\n   * Returns the number of child vectors.\n   */"
			},
			{
				"signature": "public final int getElementsAppended()",
				"documentation": "/**\n   * Returns the elements appended. This is useful\n   */"
			},
			{
				"signature": "public final void addElementsAppended(int num)",
				"documentation": "/**\n   * Increment number of elements appended by 'num'.\n   *\n   * This is useful when one wants to use the 'putXXX' API to add new elements to the vector, but\n   * still want to keep count of how many elements have been added (since the 'putXXX' APIs don't\n   * increment count).\n   */"
			},
			{
				"signature": "public final void setIsConstant()",
				"documentation": "/**\n   * Marks this column as being constant.\n   */"
			},
			{
				"signature": "public final void setAllNull()",
				"documentation": "/**\n   * Marks this column only contains null values.\n   */"
			},
			{
				"signature": "public final boolean isAllNull()",
				"documentation": "/**\n   * Whether this column only contains null values.\n   */"
			},
			{
				"signature": "protected boolean isArray()",
				"documentation": "/**\n   * Reserve a new column.\n   */"
			},
			{
				"signature": "protected WritableColumnVector(int capacity, DataType type)",
				"documentation": "/**\n   * Sets up the common state and also handles creating the child columns if this is a nested\n   * type.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.execution.vectorized.OffHeapColumnVector",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.types.UTF8String",
			"org.apache.spark.sql.vectorized.ColumnarArray",
			"org.apache.spark.sql.vectorized.ColumnarMap"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Type-safe functions available for {@link org.apache.spark.sql.Dataset} operations in Java.\n *\n * Scala users should use {@link org.apache.spark.sql.expressions.scalalang.typed}.\n *\n * @since 2.0.0\n * @deprecated As of release 3.0.0, please use the untyped builtin aggregate functions.\n */",
		"name": "org.apache.spark.sql.expressions.javalang.typed",
		"extends": "",
		"Methods": [
			{
				"signature": "public static \u003cT\u003e TypedColumn\u003cT, Double\u003e avg(MapFunction\u003cT, Double\u003e f)",
				"documentation": "/**\n   * Average aggregate function.\n   *\n   * @since 2.0.0\n   */"
			},
			{
				"signature": "public static \u003cT\u003e TypedColumn\u003cT, Long\u003e count(MapFunction\u003cT, Object\u003e f)",
				"documentation": "/**\n   * Count aggregate function.\n   *\n   * @since 2.0.0\n   */"
			},
			{
				"signature": "public static \u003cT\u003e TypedColumn\u003cT, Double\u003e sum(MapFunction\u003cT, Double\u003e f)",
				"documentation": "/**\n   * Sum aggregate function for floating point (double) type.\n   *\n   * @since 2.0.0\n   */"
			},
			{
				"signature": "public static \u003cT\u003e TypedColumn\u003cT, Long\u003e sumLong(MapFunction\u003cT, Long\u003e f)",
				"documentation": "/**\n   * Sum aggregate function for integral (long, i.e. 64 bit integer) type.\n   *\n   * @since 2.0.0\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class loader cannot be closed (its `close` method is a no-op).\n */",
		"name": "org.apache.spark.sql.internal.NonClosableMutableURLClassLoader",
		"extends": "org.apache.spark.util.MutableURLClassLoader",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * This class loader cannot be closed (its `close` method is a no-op).\n */"
			},
			{
				"signature": "public NonClosableMutableURLClassLoader(ClassLoader parent)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Policy used to indicate how often results should be produced by a [[StreamingQuery]].\n *\n * @since 2.0.0\n */",
		"name": "org.apache.spark.sql.streaming.Trigger",
		"extends": "",
		"Methods": [
			{
				"signature": "public static Trigger ProcessingTime(long intervalMs)",
				"documentation": "/**\n   * A trigger policy that runs a query periodically based on an interval in processing time.\n   * If `interval` is 0, the query will run as fast as possible.\n   *\n   * @since 2.2.0\n   */"
			},
			{
				"signature": "public static Trigger ProcessingTime(long interval, TimeUnit timeUnit)",
				"documentation": "/**\n   * (Java-friendly)\n   * A trigger policy that runs a query periodically based on an interval in processing time.\n   * If `interval` is 0, the query will run as fast as possible.\n   *\n   * {{{\n   *    import java.util.concurrent.TimeUnit\n   *    df.writeStream().trigger(Trigger.ProcessingTime(10, TimeUnit.SECONDS))\n   * }}}\n   *\n   * @since 2.2.0\n   */"
			},
			{
				"signature": "public static Trigger ProcessingTime(Duration interval)",
				"documentation": "/**\n   * (Scala-friendly)\n   * A trigger policy that runs a query periodically based on an interval in processing time.\n   * If `duration` is 0, the query will run as fast as possible.\n   *\n   * {{{\n   *    import scala.concurrent.duration._\n   *    df.writeStream.trigger(Trigger.ProcessingTime(10.seconds))\n   * }}}\n   * @since 2.2.0\n   */"
			},
			{
				"signature": "public static Trigger ProcessingTime(String interval)",
				"documentation": "/**\n   * A trigger policy that runs a query periodically based on an interval in processing time.\n   * If `interval` is effectively 0, the query will run as fast as possible.\n   *\n   * {{{\n   *    df.writeStream.trigger(Trigger.ProcessingTime(\"10 seconds\"))\n   * }}}\n   * @since 2.2.0\n   */"
			},
			{
				"signature": "public static Trigger Once()",
				"documentation": "/**\n   * A trigger that processes all available data in a single batch then terminates the query.\n   *\n   * For better scalability, AvailableNow can be used alternatively to process the data in\n   * multiple batches.\n   *\n   * @since 2.2.0\n   */"
			},
			{
				"signature": "public static Trigger AvailableNow()",
				"documentation": "/**\n   * A trigger that processes all available data at the start of the query in one or multiple\n   * batches, then terminates the query.\n   *\n   * @since 3.3.0\n   */"
			},
			{
				"signature": "public static Trigger Continuous(long intervalMs)",
				"documentation": "/**\n   * A trigger that continuously processes streaming data, asynchronously checkpointing at\n   * the specified interval.\n   *\n   * @since 2.3.0\n   */"
			},
			{
				"signature": "public static Trigger Continuous(long interval, TimeUnit timeUnit)",
				"documentation": "/**\n   * A trigger that continuously processes streaming data, asynchronously checkpointing at\n   * the specified interval.\n   *\n   * {{{\n   *    import java.util.concurrent.TimeUnit\n   *    df.writeStream.trigger(Trigger.Continuous(10, TimeUnit.SECONDS))\n   * }}}\n   *\n   * @since 2.3.0\n   */"
			},
			{
				"signature": "public static Trigger Continuous(Duration interval)",
				"documentation": "/**\n   * (Scala-friendly)\n   * A trigger that continuously processes streaming data, asynchronously checkpointing at\n   * the specified interval.\n   *\n   * {{{\n   *    import scala.concurrent.duration._\n   *    df.writeStream.trigger(Trigger.Continuous(10.seconds))\n   * }}}\n   * @since 2.3.0\n   */"
			},
			{
				"signature": "public static Trigger Continuous(String interval)",
				"documentation": "/**\n   * A trigger that continuously processes streaming data, asynchronously checkpointing at\n   * the specified interval.\n   *\n   * {{{\n   *    df.writeStream.trigger(Trigger.Continuous(\"10 seconds\"))\n   * }}}\n   * @since 2.3.0\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Evolving"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray",
		"extends": "org.apache.avro.specific.SpecificRecordBase",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			},
			{
				"signature": "public AvroArrayOfArray()",
				"documentation": "/**\n   * Default constructor.  Note that this does not initialize fields\n   * to their default values from the schema.  If that is desired then\n   * one should use \u003ccode\u003enewBuilder()\u003c/code\u003e. \n   */"
			},
			{
				"signature": "public AvroArrayOfArray(java.util.List\u003cjava.util.List\u003cjava.lang.Integer\u003e\u003e int_arrays_column)",
				"documentation": "/**\n   * All-args constructor.\n   */"
			},
			{
				"signature": "public org.apache.avro.Schema getSchema()",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Object get(int field$)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(value=\"unchecked\")\n  public void put(int field$, java.lang.Object value$)",
				"documentation": ""
			},
			{
				"signature": "public java.util.List\u003cjava.util.List\u003cjava.lang.Integer\u003e\u003e getIntArraysColumn()",
				"documentation": "/**\n   * Gets the value of the 'int_arrays_column' field.\n   */"
			},
			{
				"signature": "public void setIntArraysColumn(java.util.List\u003cjava.util.List\u003cjava.lang.Integer\u003e\u003e value)",
				"documentation": "/**\n   * Sets the value of the 'int_arrays_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder newBuilder()",
				"documentation": "/** Creates a new AvroArrayOfArray RecordBuilder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder other)",
				"documentation": "/** Creates a new AvroArrayOfArray RecordBuilder by copying an existing Builder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray other)",
				"documentation": "/** Creates a new AvroArrayOfArray RecordBuilder by copying an existing AvroArrayOfArray instance */"
			}
		],
		"interfaces": [
			"org.apache.avro.specific.SpecificRecord"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder"
		]
	},
	{
		"documentation": "/**\n   * RecordBuilder for AvroArrayOfArray instances.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder",
		"extends": "org.apache.avro.specific.SpecificRecordBuilderBase",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": "/** Creates a new Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder other)",
				"documentation": "/** Creates a Builder by copying an existing Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray other)",
				"documentation": "/** Creates a Builder by copying an existing AvroArrayOfArray instance */"
			},
			{
				"signature": "public java.util.List\u003cjava.util.List\u003cjava.lang.Integer\u003e\u003e getIntArraysColumn()",
				"documentation": "/** Gets the value of the 'int_arrays_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder setIntArraysColumn(java.util.List\u003cjava.util.List\u003cjava.lang.Integer\u003e\u003e value)",
				"documentation": "/** Sets the value of the 'int_arrays_column' field */"
			},
			{
				"signature": "public boolean hasIntArraysColumn()",
				"documentation": "/** Checks whether the 'int_arrays_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray.Builder clearIntArraysColumn()",
				"documentation": "/** Clears the value of the 'int_arrays_column' field */"
			},
			{
				"signature": "@Override\n    @SuppressWarnings(value=\"unchecked\")\n    public AvroArrayOfArray build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.data.RecordBuilder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroArrayOfArray"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray",
		"extends": "org.apache.avro.specific.SpecificRecordBase",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			},
			{
				"signature": "public AvroMapOfArray()",
				"documentation": "/**\n   * Default constructor.  Note that this does not initialize fields\n   * to their default values from the schema.  If that is desired then\n   * one should use \u003ccode\u003enewBuilder()\u003c/code\u003e. \n   */"
			},
			{
				"signature": "public AvroMapOfArray(java.util.Map\u003cjava.lang.String,java.util.List\u003cjava.lang.Integer\u003e\u003e string_to_ints_column)",
				"documentation": "/**\n   * All-args constructor.\n   */"
			},
			{
				"signature": "public org.apache.avro.Schema getSchema()",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Object get(int field$)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(value=\"unchecked\")\n  public void put(int field$, java.lang.Object value$)",
				"documentation": ""
			},
			{
				"signature": "public java.util.Map\u003cjava.lang.String,java.util.List\u003cjava.lang.Integer\u003e\u003e getStringToIntsColumn()",
				"documentation": "/**\n   * Gets the value of the 'string_to_ints_column' field.\n   */"
			},
			{
				"signature": "public void setStringToIntsColumn(java.util.Map\u003cjava.lang.String,java.util.List\u003cjava.lang.Integer\u003e\u003e value)",
				"documentation": "/**\n   * Sets the value of the 'string_to_ints_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder newBuilder()",
				"documentation": "/** Creates a new AvroMapOfArray RecordBuilder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder other)",
				"documentation": "/** Creates a new AvroMapOfArray RecordBuilder by copying an existing Builder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray other)",
				"documentation": "/** Creates a new AvroMapOfArray RecordBuilder by copying an existing AvroMapOfArray instance */"
			}
		],
		"interfaces": [
			"org.apache.avro.specific.SpecificRecord"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder"
		]
	},
	{
		"documentation": "/**\n   * RecordBuilder for AvroMapOfArray instances.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder",
		"extends": "org.apache.avro.specific.SpecificRecordBuilderBase",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": "/** Creates a new Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder other)",
				"documentation": "/** Creates a Builder by copying an existing Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray other)",
				"documentation": "/** Creates a Builder by copying an existing AvroMapOfArray instance */"
			},
			{
				"signature": "public java.util.Map\u003cjava.lang.String,java.util.List\u003cjava.lang.Integer\u003e\u003e getStringToIntsColumn()",
				"documentation": "/** Gets the value of the 'string_to_ints_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder setStringToIntsColumn(java.util.Map\u003cjava.lang.String,java.util.List\u003cjava.lang.Integer\u003e\u003e value)",
				"documentation": "/** Sets the value of the 'string_to_ints_column' field */"
			},
			{
				"signature": "public boolean hasStringToIntsColumn()",
				"documentation": "/** Checks whether the 'string_to_ints_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray.Builder clearStringToIntsColumn()",
				"documentation": "/** Clears the value of the 'string_to_ints_column' field */"
			},
			{
				"signature": "@Override\n    @SuppressWarnings(value=\"unchecked\")\n    public AvroMapOfArray build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.data.RecordBuilder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroMapOfArray"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays",
		"extends": "org.apache.avro.specific.SpecificRecordBase",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			},
			{
				"signature": "public AvroNonNullableArrays()",
				"documentation": "/**\n   * Default constructor.  Note that this does not initialize fields\n   * to their default values from the schema.  If that is desired then\n   * one should use \u003ccode\u003enewBuilder()\u003c/code\u003e. \n   */"
			},
			{
				"signature": "public AvroNonNullableArrays(java.util.List\u003cjava.lang.String\u003e strings_column, java.util.List\u003cjava.lang.Integer\u003e maybe_ints_column)",
				"documentation": "/**\n   * All-args constructor.\n   */"
			},
			{
				"signature": "public org.apache.avro.Schema getSchema()",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Object get(int field$)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(value=\"unchecked\")\n  public void put(int field$, java.lang.Object value$)",
				"documentation": ""
			},
			{
				"signature": "public java.util.List\u003cjava.lang.String\u003e getStringsColumn()",
				"documentation": "/**\n   * Gets the value of the 'strings_column' field.\n   */"
			},
			{
				"signature": "public void setStringsColumn(java.util.List\u003cjava.lang.String\u003e value)",
				"documentation": "/**\n   * Sets the value of the 'strings_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.util.List\u003cjava.lang.Integer\u003e getMaybeIntsColumn()",
				"documentation": "/**\n   * Gets the value of the 'maybe_ints_column' field.\n   */"
			},
			{
				"signature": "public void setMaybeIntsColumn(java.util.List\u003cjava.lang.Integer\u003e value)",
				"documentation": "/**\n   * Sets the value of the 'maybe_ints_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder newBuilder()",
				"documentation": "/** Creates a new AvroNonNullableArrays RecordBuilder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder other)",
				"documentation": "/** Creates a new AvroNonNullableArrays RecordBuilder by copying an existing Builder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays other)",
				"documentation": "/** Creates a new AvroNonNullableArrays RecordBuilder by copying an existing AvroNonNullableArrays instance */"
			}
		],
		"interfaces": [
			"org.apache.avro.specific.SpecificRecord"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder"
		]
	},
	{
		"documentation": "/**\n   * RecordBuilder for AvroNonNullableArrays instances.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder",
		"extends": "org.apache.avro.specific.SpecificRecordBuilderBase",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": "/** Creates a new Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder other)",
				"documentation": "/** Creates a Builder by copying an existing Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays other)",
				"documentation": "/** Creates a Builder by copying an existing AvroNonNullableArrays instance */"
			},
			{
				"signature": "public java.util.List\u003cjava.lang.String\u003e getStringsColumn()",
				"documentation": "/** Gets the value of the 'strings_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder setStringsColumn(java.util.List\u003cjava.lang.String\u003e value)",
				"documentation": "/** Sets the value of the 'strings_column' field */"
			},
			{
				"signature": "public boolean hasStringsColumn()",
				"documentation": "/** Checks whether the 'strings_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder clearStringsColumn()",
				"documentation": "/** Clears the value of the 'strings_column' field */"
			},
			{
				"signature": "public java.util.List\u003cjava.lang.Integer\u003e getMaybeIntsColumn()",
				"documentation": "/** Gets the value of the 'maybe_ints_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder setMaybeIntsColumn(java.util.List\u003cjava.lang.Integer\u003e value)",
				"documentation": "/** Sets the value of the 'maybe_ints_column' field */"
			},
			{
				"signature": "public boolean hasMaybeIntsColumn()",
				"documentation": "/** Checks whether the 'maybe_ints_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays.Builder clearMaybeIntsColumn()",
				"documentation": "/** Clears the value of the 'maybe_ints_column' field */"
			},
			{
				"signature": "@Override\n    @SuppressWarnings(value=\"unchecked\")\n    public AvroNonNullableArrays build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.data.RecordBuilder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroNonNullableArrays"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives",
		"extends": "org.apache.avro.specific.SpecificRecordBase",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			},
			{
				"signature": "public AvroOptionalPrimitives()",
				"documentation": "/**\n   * Default constructor.  Note that this does not initialize fields\n   * to their default values from the schema.  If that is desired then\n   * one should use \u003ccode\u003enewBuilder()\u003c/code\u003e. \n   */"
			},
			{
				"signature": "public AvroOptionalPrimitives(java.lang.Boolean maybe_bool_column, java.lang.Integer maybe_int_column, java.lang.Long maybe_long_column, java.lang.Float maybe_float_column, java.lang.Double maybe_double_column, java.nio.ByteBuffer maybe_binary_column, java.lang.String maybe_string_column)",
				"documentation": "/**\n   * All-args constructor.\n   */"
			},
			{
				"signature": "public org.apache.avro.Schema getSchema()",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Object get(int field$)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(value=\"unchecked\")\n  public void put(int field$, java.lang.Object value$)",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Boolean getMaybeBoolColumn()",
				"documentation": "/**\n   * Gets the value of the 'maybe_bool_column' field.\n   */"
			},
			{
				"signature": "public void setMaybeBoolColumn(java.lang.Boolean value)",
				"documentation": "/**\n   * Sets the value of the 'maybe_bool_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.Integer getMaybeIntColumn()",
				"documentation": "/**\n   * Gets the value of the 'maybe_int_column' field.\n   */"
			},
			{
				"signature": "public void setMaybeIntColumn(java.lang.Integer value)",
				"documentation": "/**\n   * Sets the value of the 'maybe_int_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.Long getMaybeLongColumn()",
				"documentation": "/**\n   * Gets the value of the 'maybe_long_column' field.\n   */"
			},
			{
				"signature": "public void setMaybeLongColumn(java.lang.Long value)",
				"documentation": "/**\n   * Sets the value of the 'maybe_long_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.Float getMaybeFloatColumn()",
				"documentation": "/**\n   * Gets the value of the 'maybe_float_column' field.\n   */"
			},
			{
				"signature": "public void setMaybeFloatColumn(java.lang.Float value)",
				"documentation": "/**\n   * Sets the value of the 'maybe_float_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.Double getMaybeDoubleColumn()",
				"documentation": "/**\n   * Gets the value of the 'maybe_double_column' field.\n   */"
			},
			{
				"signature": "public void setMaybeDoubleColumn(java.lang.Double value)",
				"documentation": "/**\n   * Sets the value of the 'maybe_double_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.nio.ByteBuffer getMaybeBinaryColumn()",
				"documentation": "/**\n   * Gets the value of the 'maybe_binary_column' field.\n   */"
			},
			{
				"signature": "public void setMaybeBinaryColumn(java.nio.ByteBuffer value)",
				"documentation": "/**\n   * Sets the value of the 'maybe_binary_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.String getMaybeStringColumn()",
				"documentation": "/**\n   * Gets the value of the 'maybe_string_column' field.\n   */"
			},
			{
				"signature": "public void setMaybeStringColumn(java.lang.String value)",
				"documentation": "/**\n   * Sets the value of the 'maybe_string_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder newBuilder()",
				"documentation": "/** Creates a new AvroOptionalPrimitives RecordBuilder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder other)",
				"documentation": "/** Creates a new AvroOptionalPrimitives RecordBuilder by copying an existing Builder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives other)",
				"documentation": "/** Creates a new AvroOptionalPrimitives RecordBuilder by copying an existing AvroOptionalPrimitives instance */"
			}
		],
		"interfaces": [
			"org.apache.avro.specific.SpecificRecord"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder"
		]
	},
	{
		"documentation": "/**\n   * RecordBuilder for AvroOptionalPrimitives instances.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder",
		"extends": "org.apache.avro.specific.SpecificRecordBuilderBase",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": "/** Creates a new Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder other)",
				"documentation": "/** Creates a Builder by copying an existing Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives other)",
				"documentation": "/** Creates a Builder by copying an existing AvroOptionalPrimitives instance */"
			},
			{
				"signature": "public java.lang.Boolean getMaybeBoolColumn()",
				"documentation": "/** Gets the value of the 'maybe_bool_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder setMaybeBoolColumn(java.lang.Boolean value)",
				"documentation": "/** Sets the value of the 'maybe_bool_column' field */"
			},
			{
				"signature": "public boolean hasMaybeBoolColumn()",
				"documentation": "/** Checks whether the 'maybe_bool_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder clearMaybeBoolColumn()",
				"documentation": "/** Clears the value of the 'maybe_bool_column' field */"
			},
			{
				"signature": "public java.lang.Integer getMaybeIntColumn()",
				"documentation": "/** Gets the value of the 'maybe_int_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder setMaybeIntColumn(java.lang.Integer value)",
				"documentation": "/** Sets the value of the 'maybe_int_column' field */"
			},
			{
				"signature": "public boolean hasMaybeIntColumn()",
				"documentation": "/** Checks whether the 'maybe_int_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder clearMaybeIntColumn()",
				"documentation": "/** Clears the value of the 'maybe_int_column' field */"
			},
			{
				"signature": "public java.lang.Long getMaybeLongColumn()",
				"documentation": "/** Gets the value of the 'maybe_long_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder setMaybeLongColumn(java.lang.Long value)",
				"documentation": "/** Sets the value of the 'maybe_long_column' field */"
			},
			{
				"signature": "public boolean hasMaybeLongColumn()",
				"documentation": "/** Checks whether the 'maybe_long_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder clearMaybeLongColumn()",
				"documentation": "/** Clears the value of the 'maybe_long_column' field */"
			},
			{
				"signature": "public java.lang.Float getMaybeFloatColumn()",
				"documentation": "/** Gets the value of the 'maybe_float_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder setMaybeFloatColumn(java.lang.Float value)",
				"documentation": "/** Sets the value of the 'maybe_float_column' field */"
			},
			{
				"signature": "public boolean hasMaybeFloatColumn()",
				"documentation": "/** Checks whether the 'maybe_float_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder clearMaybeFloatColumn()",
				"documentation": "/** Clears the value of the 'maybe_float_column' field */"
			},
			{
				"signature": "public java.lang.Double getMaybeDoubleColumn()",
				"documentation": "/** Gets the value of the 'maybe_double_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder setMaybeDoubleColumn(java.lang.Double value)",
				"documentation": "/** Sets the value of the 'maybe_double_column' field */"
			},
			{
				"signature": "public boolean hasMaybeDoubleColumn()",
				"documentation": "/** Checks whether the 'maybe_double_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder clearMaybeDoubleColumn()",
				"documentation": "/** Clears the value of the 'maybe_double_column' field */"
			},
			{
				"signature": "public java.nio.ByteBuffer getMaybeBinaryColumn()",
				"documentation": "/** Gets the value of the 'maybe_binary_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder setMaybeBinaryColumn(java.nio.ByteBuffer value)",
				"documentation": "/** Sets the value of the 'maybe_binary_column' field */"
			},
			{
				"signature": "public boolean hasMaybeBinaryColumn()",
				"documentation": "/** Checks whether the 'maybe_binary_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder clearMaybeBinaryColumn()",
				"documentation": "/** Clears the value of the 'maybe_binary_column' field */"
			},
			{
				"signature": "public java.lang.String getMaybeStringColumn()",
				"documentation": "/** Gets the value of the 'maybe_string_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder setMaybeStringColumn(java.lang.String value)",
				"documentation": "/** Sets the value of the 'maybe_string_column' field */"
			},
			{
				"signature": "public boolean hasMaybeStringColumn()",
				"documentation": "/** Checks whether the 'maybe_string_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives.Builder clearMaybeStringColumn()",
				"documentation": "/** Clears the value of the 'maybe_string_column' field */"
			},
			{
				"signature": "@Override\n    public AvroOptionalPrimitives build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.data.RecordBuilder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroOptionalPrimitives"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives",
		"extends": "org.apache.avro.specific.SpecificRecordBase",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			},
			{
				"signature": "public AvroPrimitives()",
				"documentation": "/**\n   * Default constructor.  Note that this does not initialize fields\n   * to their default values from the schema.  If that is desired then\n   * one should use \u003ccode\u003enewBuilder()\u003c/code\u003e. \n   */"
			},
			{
				"signature": "public AvroPrimitives(java.lang.Boolean bool_column, java.lang.Integer int_column, java.lang.Long long_column, java.lang.Float float_column, java.lang.Double double_column, java.nio.ByteBuffer binary_column, java.lang.String string_column)",
				"documentation": "/**\n   * All-args constructor.\n   */"
			},
			{
				"signature": "public org.apache.avro.Schema getSchema()",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Object get(int field$)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(value=\"unchecked\")\n  public void put(int field$, java.lang.Object value$)",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Boolean getBoolColumn()",
				"documentation": "/**\n   * Gets the value of the 'bool_column' field.\n   */"
			},
			{
				"signature": "public void setBoolColumn(java.lang.Boolean value)",
				"documentation": "/**\n   * Sets the value of the 'bool_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.Integer getIntColumn()",
				"documentation": "/**\n   * Gets the value of the 'int_column' field.\n   */"
			},
			{
				"signature": "public void setIntColumn(java.lang.Integer value)",
				"documentation": "/**\n   * Sets the value of the 'int_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.Long getLongColumn()",
				"documentation": "/**\n   * Gets the value of the 'long_column' field.\n   */"
			},
			{
				"signature": "public void setLongColumn(java.lang.Long value)",
				"documentation": "/**\n   * Sets the value of the 'long_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.Float getFloatColumn()",
				"documentation": "/**\n   * Gets the value of the 'float_column' field.\n   */"
			},
			{
				"signature": "public void setFloatColumn(java.lang.Float value)",
				"documentation": "/**\n   * Sets the value of the 'float_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.Double getDoubleColumn()",
				"documentation": "/**\n   * Gets the value of the 'double_column' field.\n   */"
			},
			{
				"signature": "public void setDoubleColumn(java.lang.Double value)",
				"documentation": "/**\n   * Sets the value of the 'double_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.nio.ByteBuffer getBinaryColumn()",
				"documentation": "/**\n   * Gets the value of the 'binary_column' field.\n   */"
			},
			{
				"signature": "public void setBinaryColumn(java.nio.ByteBuffer value)",
				"documentation": "/**\n   * Sets the value of the 'binary_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.String getStringColumn()",
				"documentation": "/**\n   * Gets the value of the 'string_column' field.\n   */"
			},
			{
				"signature": "public void setStringColumn(java.lang.String value)",
				"documentation": "/**\n   * Sets the value of the 'string_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder newBuilder()",
				"documentation": "/** Creates a new AvroPrimitives RecordBuilder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder other)",
				"documentation": "/** Creates a new AvroPrimitives RecordBuilder by copying an existing Builder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives other)",
				"documentation": "/** Creates a new AvroPrimitives RecordBuilder by copying an existing AvroPrimitives instance */"
			}
		],
		"interfaces": [
			"org.apache.avro.specific.SpecificRecord"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder"
		]
	},
	{
		"documentation": "/**\n   * RecordBuilder for AvroPrimitives instances.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder",
		"extends": "org.apache.avro.specific.SpecificRecordBuilderBase",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": "/** Creates a new Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder other)",
				"documentation": "/** Creates a Builder by copying an existing Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives other)",
				"documentation": "/** Creates a Builder by copying an existing AvroPrimitives instance */"
			},
			{
				"signature": "public java.lang.Boolean getBoolColumn()",
				"documentation": "/** Gets the value of the 'bool_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder setBoolColumn(boolean value)",
				"documentation": "/** Sets the value of the 'bool_column' field */"
			},
			{
				"signature": "public boolean hasBoolColumn()",
				"documentation": "/** Checks whether the 'bool_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder clearBoolColumn()",
				"documentation": "/** Clears the value of the 'bool_column' field */"
			},
			{
				"signature": "public java.lang.Integer getIntColumn()",
				"documentation": "/** Gets the value of the 'int_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder setIntColumn(int value)",
				"documentation": "/** Sets the value of the 'int_column' field */"
			},
			{
				"signature": "public boolean hasIntColumn()",
				"documentation": "/** Checks whether the 'int_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder clearIntColumn()",
				"documentation": "/** Clears the value of the 'int_column' field */"
			},
			{
				"signature": "public java.lang.Long getLongColumn()",
				"documentation": "/** Gets the value of the 'long_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder setLongColumn(long value)",
				"documentation": "/** Sets the value of the 'long_column' field */"
			},
			{
				"signature": "public boolean hasLongColumn()",
				"documentation": "/** Checks whether the 'long_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder clearLongColumn()",
				"documentation": "/** Clears the value of the 'long_column' field */"
			},
			{
				"signature": "public java.lang.Float getFloatColumn()",
				"documentation": "/** Gets the value of the 'float_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder setFloatColumn(float value)",
				"documentation": "/** Sets the value of the 'float_column' field */"
			},
			{
				"signature": "public boolean hasFloatColumn()",
				"documentation": "/** Checks whether the 'float_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder clearFloatColumn()",
				"documentation": "/** Clears the value of the 'float_column' field */"
			},
			{
				"signature": "public java.lang.Double getDoubleColumn()",
				"documentation": "/** Gets the value of the 'double_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder setDoubleColumn(double value)",
				"documentation": "/** Sets the value of the 'double_column' field */"
			},
			{
				"signature": "public boolean hasDoubleColumn()",
				"documentation": "/** Checks whether the 'double_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder clearDoubleColumn()",
				"documentation": "/** Clears the value of the 'double_column' field */"
			},
			{
				"signature": "public java.nio.ByteBuffer getBinaryColumn()",
				"documentation": "/** Gets the value of the 'binary_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder setBinaryColumn(java.nio.ByteBuffer value)",
				"documentation": "/** Sets the value of the 'binary_column' field */"
			},
			{
				"signature": "public boolean hasBinaryColumn()",
				"documentation": "/** Checks whether the 'binary_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder clearBinaryColumn()",
				"documentation": "/** Clears the value of the 'binary_column' field */"
			},
			{
				"signature": "public java.lang.String getStringColumn()",
				"documentation": "/** Gets the value of the 'string_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder setStringColumn(java.lang.String value)",
				"documentation": "/** Sets the value of the 'string_column' field */"
			},
			{
				"signature": "public boolean hasStringColumn()",
				"documentation": "/** Checks whether the 'string_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives.Builder clearStringColumn()",
				"documentation": "/** Clears the value of the 'string_column' field */"
			},
			{
				"signature": "@Override\n    public AvroPrimitives build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.data.RecordBuilder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.AvroPrimitives"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.CompatibilityTest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": true,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.Callback"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.Callback"
		]
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.Callback",
		"extends": "org.apache.spark.sql.execution.datasources.parquet.test.avro.CompatibilityTest",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested",
		"extends": "org.apache.avro.specific.SpecificRecordBase",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			},
			{
				"signature": "public Nested()",
				"documentation": "/**\n   * Default constructor.  Note that this does not initialize fields\n   * to their default values from the schema.  If that is desired then\n   * one should use \u003ccode\u003enewBuilder()\u003c/code\u003e. \n   */"
			},
			{
				"signature": "public Nested(java.util.List\u003cjava.lang.Integer\u003e nested_ints_column, java.lang.String nested_string_column)",
				"documentation": "/**\n   * All-args constructor.\n   */"
			},
			{
				"signature": "public org.apache.avro.Schema getSchema()",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Object get(int field$)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(value=\"unchecked\")\n  public void put(int field$, java.lang.Object value$)",
				"documentation": ""
			},
			{
				"signature": "public java.util.List\u003cjava.lang.Integer\u003e getNestedIntsColumn()",
				"documentation": "/**\n   * Gets the value of the 'nested_ints_column' field.\n   */"
			},
			{
				"signature": "public void setNestedIntsColumn(java.util.List\u003cjava.lang.Integer\u003e value)",
				"documentation": "/**\n   * Sets the value of the 'nested_ints_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.lang.String getNestedStringColumn()",
				"documentation": "/**\n   * Gets the value of the 'nested_string_column' field.\n   */"
			},
			{
				"signature": "public void setNestedStringColumn(java.lang.String value)",
				"documentation": "/**\n   * Sets the value of the 'nested_string_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder newBuilder()",
				"documentation": "/** Creates a new Nested RecordBuilder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder other)",
				"documentation": "/** Creates a new Nested RecordBuilder by copying an existing Builder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested other)",
				"documentation": "/** Creates a new Nested RecordBuilder by copying an existing Nested instance */"
			}
		],
		"interfaces": [
			"org.apache.avro.specific.SpecificRecord"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder"
		]
	},
	{
		"documentation": "/**\n   * RecordBuilder for Nested instances.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder",
		"extends": "org.apache.avro.specific.SpecificRecordBuilderBase",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": "/** Creates a new Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder other)",
				"documentation": "/** Creates a Builder by copying an existing Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested other)",
				"documentation": "/** Creates a Builder by copying an existing Nested instance */"
			},
			{
				"signature": "public java.util.List\u003cjava.lang.Integer\u003e getNestedIntsColumn()",
				"documentation": "/** Gets the value of the 'nested_ints_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder setNestedIntsColumn(java.util.List\u003cjava.lang.Integer\u003e value)",
				"documentation": "/** Sets the value of the 'nested_ints_column' field */"
			},
			{
				"signature": "public boolean hasNestedIntsColumn()",
				"documentation": "/** Checks whether the 'nested_ints_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder clearNestedIntsColumn()",
				"documentation": "/** Clears the value of the 'nested_ints_column' field */"
			},
			{
				"signature": "public java.lang.String getNestedStringColumn()",
				"documentation": "/** Gets the value of the 'nested_string_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder setNestedStringColumn(java.lang.String value)",
				"documentation": "/** Sets the value of the 'nested_string_column' field */"
			},
			{
				"signature": "public boolean hasNestedStringColumn()",
				"documentation": "/** Checks whether the 'nested_string_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested.Builder clearNestedStringColumn()",
				"documentation": "/** Clears the value of the 'nested_string_column' field */"
			},
			{
				"signature": "@Override\n    @SuppressWarnings(value=\"unchecked\")\n    public Nested build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.data.RecordBuilder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.Nested"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat",
		"extends": "org.apache.avro.specific.SpecificRecordBase",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			},
			{
				"signature": "public ParquetAvroCompat()",
				"documentation": "/**\n   * Default constructor.  Note that this does not initialize fields\n   * to their default values from the schema.  If that is desired then\n   * one should use \u003ccode\u003enewBuilder()\u003c/code\u003e. \n   */"
			},
			{
				"signature": "public ParquetAvroCompat(java.util.List\u003cjava.lang.String\u003e strings_column, java.util.Map\u003cjava.lang.String,java.lang.Integer\u003e string_to_int_column, java.util.Map\u003cjava.lang.String,java.util.List\u003corg.apache.spark.sql.execution.datasources.parquet.test.avro.Nested\u003e\u003e complex_column)",
				"documentation": "/**\n   * All-args constructor.\n   */"
			},
			{
				"signature": "public org.apache.avro.Schema getSchema()",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Object get(int field$)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(value=\"unchecked\")\n  public void put(int field$, java.lang.Object value$)",
				"documentation": ""
			},
			{
				"signature": "public java.util.List\u003cjava.lang.String\u003e getStringsColumn()",
				"documentation": "/**\n   * Gets the value of the 'strings_column' field.\n   */"
			},
			{
				"signature": "public void setStringsColumn(java.util.List\u003cjava.lang.String\u003e value)",
				"documentation": "/**\n   * Sets the value of the 'strings_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.util.Map\u003cjava.lang.String,java.lang.Integer\u003e getStringToIntColumn()",
				"documentation": "/**\n   * Gets the value of the 'string_to_int_column' field.\n   */"
			},
			{
				"signature": "public void setStringToIntColumn(java.util.Map\u003cjava.lang.String,java.lang.Integer\u003e value)",
				"documentation": "/**\n   * Sets the value of the 'string_to_int_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public java.util.Map\u003cjava.lang.String,java.util.List\u003corg.apache.spark.sql.execution.datasources.parquet.test.avro.Nested\u003e\u003e getComplexColumn()",
				"documentation": "/**\n   * Gets the value of the 'complex_column' field.\n   */"
			},
			{
				"signature": "public void setComplexColumn(java.util.Map\u003cjava.lang.String,java.util.List\u003corg.apache.spark.sql.execution.datasources.parquet.test.avro.Nested\u003e\u003e value)",
				"documentation": "/**\n   * Sets the value of the 'complex_column' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder newBuilder()",
				"documentation": "/** Creates a new ParquetAvroCompat RecordBuilder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder other)",
				"documentation": "/** Creates a new ParquetAvroCompat RecordBuilder by copying an existing Builder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat other)",
				"documentation": "/** Creates a new ParquetAvroCompat RecordBuilder by copying an existing ParquetAvroCompat instance */"
			}
		],
		"interfaces": [
			"org.apache.avro.specific.SpecificRecord"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder"
		]
	},
	{
		"documentation": "/**\n   * RecordBuilder for ParquetAvroCompat instances.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder",
		"extends": "org.apache.avro.specific.SpecificRecordBuilderBase",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": "/** Creates a new Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder other)",
				"documentation": "/** Creates a Builder by copying an existing Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat other)",
				"documentation": "/** Creates a Builder by copying an existing ParquetAvroCompat instance */"
			},
			{
				"signature": "public java.util.List\u003cjava.lang.String\u003e getStringsColumn()",
				"documentation": "/** Gets the value of the 'strings_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder setStringsColumn(java.util.List\u003cjava.lang.String\u003e value)",
				"documentation": "/** Sets the value of the 'strings_column' field */"
			},
			{
				"signature": "public boolean hasStringsColumn()",
				"documentation": "/** Checks whether the 'strings_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder clearStringsColumn()",
				"documentation": "/** Clears the value of the 'strings_column' field */"
			},
			{
				"signature": "public java.util.Map\u003cjava.lang.String,java.lang.Integer\u003e getStringToIntColumn()",
				"documentation": "/** Gets the value of the 'string_to_int_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder setStringToIntColumn(java.util.Map\u003cjava.lang.String,java.lang.Integer\u003e value)",
				"documentation": "/** Sets the value of the 'string_to_int_column' field */"
			},
			{
				"signature": "public boolean hasStringToIntColumn()",
				"documentation": "/** Checks whether the 'string_to_int_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder clearStringToIntColumn()",
				"documentation": "/** Clears the value of the 'string_to_int_column' field */"
			},
			{
				"signature": "public java.util.Map\u003cjava.lang.String,java.util.List\u003corg.apache.spark.sql.execution.datasources.parquet.test.avro.Nested\u003e\u003e getComplexColumn()",
				"documentation": "/** Gets the value of the 'complex_column' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder setComplexColumn(java.util.Map\u003cjava.lang.String,java.util.List\u003corg.apache.spark.sql.execution.datasources.parquet.test.avro.Nested\u003e\u003e value)",
				"documentation": "/** Sets the value of the 'complex_column' field */"
			},
			{
				"signature": "public boolean hasComplexColumn()",
				"documentation": "/** Checks whether the 'complex_column' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat.Builder clearComplexColumn()",
				"documentation": "/** Clears the value of the 'complex_column' field */"
			},
			{
				"signature": "@Override\n    @SuppressWarnings(value=\"unchecked\")\n    public ParquetAvroCompat build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.data.RecordBuilder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetAvroCompat"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum",
		"extends": "org.apache.avro.specific.SpecificRecordBase",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			},
			{
				"signature": "public ParquetEnum()",
				"documentation": "/**\n   * Default constructor.  Note that this does not initialize fields\n   * to their default values from the schema.  If that is desired then\n   * one should use \u003ccode\u003enewBuilder()\u003c/code\u003e. \n   */"
			},
			{
				"signature": "public ParquetEnum(org.apache.spark.sql.execution.datasources.parquet.test.avro.Suit suit)",
				"documentation": "/**\n   * All-args constructor.\n   */"
			},
			{
				"signature": "public org.apache.avro.Schema getSchema()",
				"documentation": ""
			},
			{
				"signature": "public java.lang.Object get(int field$)",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(value=\"unchecked\")\n  public void put(int field$, java.lang.Object value$)",
				"documentation": ""
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.Suit getSuit()",
				"documentation": "/**\n   * Gets the value of the 'suit' field.\n   */"
			},
			{
				"signature": "public void setSuit(org.apache.spark.sql.execution.datasources.parquet.test.avro.Suit value)",
				"documentation": "/**\n   * Sets the value of the 'suit' field.\n   * @param value the value to set.\n   */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder newBuilder()",
				"documentation": "/** Creates a new ParquetEnum RecordBuilder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder other)",
				"documentation": "/** Creates a new ParquetEnum RecordBuilder by copying an existing Builder */"
			},
			{
				"signature": "public static org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder newBuilder(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum other)",
				"documentation": "/** Creates a new ParquetEnum RecordBuilder by copying an existing ParquetEnum instance */"
			}
		],
		"interfaces": [
			"org.apache.avro.specific.SpecificRecord"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder"
		]
	},
	{
		"documentation": "/**\n   * RecordBuilder for ParquetEnum instances.\n   */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder",
		"extends": "org.apache.avro.specific.SpecificRecordBuilderBase",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": "/** Creates a new Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder other)",
				"documentation": "/** Creates a Builder by copying an existing Builder */"
			},
			{
				"signature": "private Builder(org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum other)",
				"documentation": "/** Creates a Builder by copying an existing ParquetEnum instance */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.Suit getSuit()",
				"documentation": "/** Gets the value of the 'suit' field */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder setSuit(org.apache.spark.sql.execution.datasources.parquet.test.avro.Suit value)",
				"documentation": "/** Sets the value of the 'suit' field */"
			},
			{
				"signature": "public boolean hasSuit()",
				"documentation": "/** Checks whether the 'suit' field has been set */"
			},
			{
				"signature": "public org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum.Builder clearSuit()",
				"documentation": "/** Clears the value of the 'suit' field */"
			},
			{
				"signature": "@Override\n    public ParquetEnum build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.avro.data.RecordBuilder"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.execution.datasources.parquet.test.avro.ParquetEnum"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */",
		"name": "org.apache.spark.sql.execution.datasources.parquet.test.avro.Suit",
		"extends": "",
		"Methods": [
			{
				"signature": "public static org.apache.avro.Schema getClassSchema()",
				"documentation": "/**\n * Autogenerated by Avro\n * \n * DO NOT EDIT DIRECTLY\n */"
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Spark SQL UDF that has 23 arguments for test.\n */",
		"name": "org.apache.spark.sql.api.java.UDF23Test",
		"extends": "java.io.Serializable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.Stable"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A hack to create Parquet data pages with customized first row index. We have to put it under\n * 'org.apache.parquet.column.page' since the constructor of `DataPage` is package-private.\n */",
		"name": "org.apache.parquet.column.page.TestDataPage",
		"extends": "DataPage",
		"Methods": [
			{
				"signature": "public TestDataPage(DataPage wrapped, long firstRowIndex)",
				"documentation": "/**\n * A hack to create Parquet data pages with customized first row index. We have to put it under\n * 'org.apache.parquet.column.page' since the constructor of `DataPage` is package-private.\n */"
			},
			{
				"signature": "@Override\n  public Optional\u003cInteger\u003e getIndexRowCount()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public \u003cT\u003e T accept(Visitor\u003cT\u003e visitor)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Suite that replicates tests in JavaDatasetAggregatorSuite using lambda syntax.\n */",
		"name": "test.org.apache.spark.sql.Java8DatasetAggregatorSuite",
		"extends": "test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testTypedAggregationAverage()",
				"documentation": "/**\n * Suite that replicates tests in JavaDatasetAggregatorSuite using lambda syntax.\n */"
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testTypedAggregationCount()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testTypedAggregationSumDouble()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testTypedAggregationSumLong()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaApplySchemaSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void applySchema()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void dataFrameRDDOperations()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void applySchemaToJSON()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory",
			"org.apache.spark.sql.types.DataTypes",
			"test.org.apache.spark.sql.JavaApplySchemaSuite.Person"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaApplySchemaSuite.Person"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaApplySchemaSuite.Person",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getName()",
				"documentation": ""
			},
			{
				"signature": "public void setName(String name)",
				"documentation": ""
			},
			{
				"signature": "public int getAge()",
				"documentation": ""
			},
			{
				"signature": "public void setAge(int age)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaApplySchemaSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaBeanDeserializationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "static",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBeanWithArrayFieldDeserialization()",
				"documentation": ""
			},
			{
				"signature": "static",
				"documentation": ""
			},
			{
				"signature": "private static \u003cK, V\u003e Map\u003cK, V\u003e toMap(Collection\u003cK\u003e keys, Collection\u003cV\u003e values)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBeanWithMapFieldsDeserialization()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSpark22000()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSpark22000FailToUpcast()",
				"documentation": ""
			},
			{
				"signature": "private static Row createRecordSpark22000Row(Long index)",
				"documentation": ""
			},
			{
				"signature": "private static String timestampToString(Timestamp ts)",
				"documentation": ""
			},
			{
				"signature": "private static RecordSpark22000 createRecordSpark22000(Row recordRow)",
				"documentation": ""
			},
			{
				"signature": "private static Row createRecordSpark22000FailToUpcastRow(Long index)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBeanWithLocalDateAndInstant()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSPARK38823NoBeanReuse()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public String call(Item item) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public Item call(Item item1, Item item2) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private static Row createLocalDateInstantRow(Long index)",
				"documentation": ""
			},
			{
				"signature": "private static LocalDateInstantRecord createLocalDateInstantRecord(Row recordRow)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.ArrayRecord",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.MapRecord",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.Interval",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.RecordSpark22000",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.RecordSpark22000FailToUpcast",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.LocalDateInstantRecord"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.ArrayRecord",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.MapRecord",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.Interval",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.RecordSpark22000",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.RecordSpark22000FailToUpcast",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.Item",
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite.LocalDateInstantRecord"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaBeanDeserializationSuite.ArrayRecord",
		"extends": "",
		"Methods": [
			{
				"signature": "public ArrayRecord()",
				"documentation": ""
			},
			{
				"signature": "ArrayRecord(int id, List\u003cInterval\u003e intervals, int[] ints)",
				"documentation": ""
			},
			{
				"signature": "public int getId()",
				"documentation": ""
			},
			{
				"signature": "public void setId(int id)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cInterval\u003e getIntervals()",
				"documentation": ""
			},
			{
				"signature": "public void setIntervals(List\u003cInterval\u003e intervals)",
				"documentation": ""
			},
			{
				"signature": "public int[] getInts()",
				"documentation": ""
			},
			{
				"signature": "public void setInts(int[] ints)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object obj)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaBeanDeserializationSuite.MapRecord",
		"extends": "",
		"Methods": [
			{
				"signature": "public MapRecord()",
				"documentation": ""
			},
			{
				"signature": "MapRecord(int id, Map\u003cString, Interval\u003e intervals)",
				"documentation": ""
			},
			{
				"signature": "public int getId()",
				"documentation": ""
			},
			{
				"signature": "public void setId(int id)",
				"documentation": ""
			},
			{
				"signature": "public Map\u003cString, Interval\u003e getIntervals()",
				"documentation": ""
			},
			{
				"signature": "public void setIntervals(Map\u003cString, Interval\u003e intervals)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object obj)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaBeanDeserializationSuite.Interval",
		"extends": "",
		"Methods": [
			{
				"signature": "public Interval()",
				"documentation": ""
			},
			{
				"signature": "Interval(long startTime, long endTime)",
				"documentation": ""
			},
			{
				"signature": "public long getStartTime()",
				"documentation": ""
			},
			{
				"signature": "public void setStartTime(long startTime)",
				"documentation": ""
			},
			{
				"signature": "public long getEndTime()",
				"documentation": ""
			},
			{
				"signature": "public void setEndTime(long endTime)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object obj)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaBeanDeserializationSuite.RecordSpark22000",
		"extends": "",
		"Methods": [
			{
				"signature": "public RecordSpark22000()",
				"documentation": ""
			},
			{
				"signature": "public String getShortField()",
				"documentation": ""
			},
			{
				"signature": "public void setShortField(String shortField)",
				"documentation": ""
			},
			{
				"signature": "public String getIntField()",
				"documentation": ""
			},
			{
				"signature": "public void setIntField(String intField)",
				"documentation": ""
			},
			{
				"signature": "public String getLongField()",
				"documentation": ""
			},
			{
				"signature": "public void setLongField(String longField)",
				"documentation": ""
			},
			{
				"signature": "public String getFloatField()",
				"documentation": ""
			},
			{
				"signature": "public void setFloatField(String floatField)",
				"documentation": ""
			},
			{
				"signature": "public String getDoubleField()",
				"documentation": ""
			},
			{
				"signature": "public void setDoubleField(String doubleField)",
				"documentation": ""
			},
			{
				"signature": "public String getStringField()",
				"documentation": ""
			},
			{
				"signature": "public void setStringField(String stringField)",
				"documentation": ""
			},
			{
				"signature": "public String getBooleanField()",
				"documentation": ""
			},
			{
				"signature": "public void setBooleanField(String booleanField)",
				"documentation": ""
			},
			{
				"signature": "public String getTimestampField()",
				"documentation": ""
			},
			{
				"signature": "public void setTimestampField(String timestampField)",
				"documentation": ""
			},
			{
				"signature": "public String getNullIntField()",
				"documentation": ""
			},
			{
				"signature": "public void setNullIntField(String nullIntField)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaBeanDeserializationSuite.RecordSpark22000FailToUpcast",
		"extends": "",
		"Methods": [
			{
				"signature": "public RecordSpark22000FailToUpcast()",
				"documentation": ""
			},
			{
				"signature": "public Integer getId()",
				"documentation": ""
			},
			{
				"signature": "public void setId(Integer id)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaBeanDeserializationSuite.Item",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getK()",
				"documentation": ""
			},
			{
				"signature": "public int getV()",
				"documentation": ""
			},
			{
				"signature": "public void setK(String k)",
				"documentation": ""
			},
			{
				"signature": "public void setV(int v)",
				"documentation": ""
			},
			{
				"signature": "public Item()",
				"documentation": ""
			},
			{
				"signature": "public Item(String k, int v)",
				"documentation": ""
			},
			{
				"signature": "public Item addValue(int inc)",
				"documentation": ""
			},
			{
				"signature": "public String toString()",
				"documentation": ""
			},
			{
				"signature": "public boolean equals(Object o)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaBeanDeserializationSuite.LocalDateInstantRecord",
		"extends": "",
		"Methods": [
			{
				"signature": "public LocalDateInstantRecord()",
				"documentation": ""
			},
			{
				"signature": "public String getLocalDateField()",
				"documentation": ""
			},
			{
				"signature": "public void setLocalDateField(String localDateField)",
				"documentation": ""
			},
			{
				"signature": "public String getInstantField()",
				"documentation": ""
			},
			{
				"signature": "public void setInstantField(String instantField)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaBeanDeserializationSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaColumnExpressionSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void isInCollectionWorksCorrectlyOnJava()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void isInCollectionCheckExceptionMessage()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFormatAPI()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOptionsAPI()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSaveModeAPI()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLoadAPI()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTextAPI()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTextFileAPI()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCsvAPI()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJsonAPI()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testParquetAPI()",
				"documentation": ""
			},
			{
				"signature": "public void testOrcAPI()",
				"documentation": "/**\n   * This only tests whether API compiles, but does not run it as orc()\n   * cannot be run without Hive classes.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDataFrameSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testExecution()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCollectAndTake()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testVarargMethods()",
				"documentation": "/**\n   * See SPARK-5904. Abstract vararg methods defined in Scala do not work in Java.\n   */"
			},
			{
				"signature": "@Ignore\n  public void testShow()",
				"documentation": ""
			},
			{
				"signature": "void validateDataFrameWithBeans(Bean bean, Dataset\u003cRow\u003e df)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCreateDataFrameFromLocalJavaBeans()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCreateDataFrameFromJavaBeans()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCreateDataFromFromList()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCreateStructTypeFromList()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCrosstab()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFrequentItems()",
				"documentation": ""
			},
			{
				"signature": "String[] cols =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCorrelation()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCovariance()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSampleBy()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testwithColumns()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSampleByColumn()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void pivot()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void pivotColumnValues()",
				"documentation": ""
			},
			{
				"signature": "private String getResource(String resource)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGenericLoad()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTextLoad()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCountMinSketch()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBloomFilter()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBeanWithoutGetter()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testJsonRDDToDataFrame()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCircularReferenceBean()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUDF()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDataFrameSuite.Bean.NestedBean"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaDataFrameSuite.Bean",
			"test.org.apache.spark.sql.JavaDataFrameSuite.BeanWithoutGetter",
			"test.org.apache.spark.sql.CircularReference1Bean",
			"test.org.apache.spark.sql.CircularReference2Bean"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDataFrameSuite.Bean",
		"extends": "",
		"Methods": [
			{
				"signature": "private Integer[] b =",
				"documentation": ""
			},
			{
				"signature": "public double getA()",
				"documentation": ""
			},
			{
				"signature": "public Integer[] getB()",
				"documentation": ""
			},
			{
				"signature": "public Map\u003cString, int[]\u003e getC()",
				"documentation": ""
			},
			{
				"signature": "public List\u003cString\u003e getD()",
				"documentation": ""
			},
			{
				"signature": "public BigInteger getE()",
				"documentation": ""
			},
			{
				"signature": "public NestedBean getF()",
				"documentation": ""
			},
			{
				"signature": "public NestedBean getG()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDataFrameSuite.Bean.NestedBean"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaDataFrameSuite.Bean.NestedBean"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDataFrameSuite.Bean.NestedBean",
		"extends": "",
		"Methods": [
			{
				"signature": "public int getA()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDataFrameSuite",
			"test.org.apache.spark.sql.JavaDataFrameSuite.Bean"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDataFrameSuite.BeanWithoutGetter",
		"extends": "",
		"Methods": [
			{
				"signature": "public void setA(String a)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.CircularReference1Bean",
		"extends": "",
		"Methods": [
			{
				"signature": "public CircularReference2Bean getChild()",
				"documentation": ""
			},
			{
				"signature": "public void setChild(CircularReference2Bean child)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.CircularReference2Bean",
		"extends": "",
		"Methods": [
			{
				"signature": "public CircularReference1Bean getChild()",
				"documentation": ""
			},
			{
				"signature": "public void setChild(CircularReference1Bean child)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDataFrameWriterV2Suite",
		"extends": "",
		"Methods": [
			{
				"signature": "public Dataset\u003cRow\u003e df()",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void createTestTable()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void dropTestTable()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testAppendAPI() throws NoSuchTableException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOverwritePartitionsAPI() throws NoSuchTableException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testOverwriteAPI() throws NoSuchTableException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCreateAPI() throws TableAlreadyExistsException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReplaceAPI() throws CannotReplaceMissingTableException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCreateOrReplaceAPI()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Suite for testing the aggregate functionality of Datasets in Java.\n */",
		"name": "test.org.apache.spark.sql.JavaDatasetAggregatorSuite",
		"extends": "test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase",
		"Methods": [
			{
				"signature": "@Test\n  public void testTypedAggregationAnonClass()",
				"documentation": "/**\n * Suite for testing the aggregate functionality of Datasets in Java.\n */"
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testTypedAggregationAverage()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testTypedAggregationCount()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testTypedAggregationSumDouble()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"deprecation\")\n  @Test\n  public void testTypedAggregationSumLong()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDatasetAggregatorSuite.IntSumOf"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaDatasetAggregatorSuite.IntSumOf"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetAggregatorSuite.IntSumOf",
		"extends": "org.apache.spark.sql.expressions.Aggregator",
		"Methods": [
			{
				"signature": "@Override\n    public Integer zero()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Integer reduce(Integer l, Tuple2\u003cString, Integer\u003e t)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Integer merge(Integer b1, Integer b2)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Integer finish(Integer reduction)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Encoder\u003cInteger\u003e bufferEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Encoder\u003cInteger\u003e outputEncoder()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetAggregatorSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Common test base shared across this and Java8DatasetAggregatorSuite.\n */",
		"name": "test.org.apache.spark.sql.JavaDatasetAggregatorSuiteBase",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": "/**\n * Common test base shared across this and Java8DatasetAggregatorSuite.\n */"
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "protected KeyValueGroupedDataset\u003cString, Tuple2\u003cString, Integer\u003e\u003e generateGroupedDataset()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"test.org.apache.spark.sql.Java8DatasetAggregatorSuite",
			"test.org.apache.spark.sql.JavaDatasetAggregatorSuite"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "private \u003cT1, T2\u003e Tuple2\u003cT1, T2\u003e tuple2(T1 t1, T2 t2)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCollect()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTake()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testToLocalIterator()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTypedFilterPreservingSchema()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCommonOperation()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testForeach()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduce()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInitialStateFlatMapGroupsWithState()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testIllegalTestGroupStateCreations()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMappingFunctionWithTestGroupState() throws Exception",
				"documentation": ""
			},
			{
				"signature": "Integer[] values =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGroupBy()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testObservation()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSelect()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSetOperation()",
				"documentation": ""
			},
			{
				"signature": "private static \u003cT\u003e Set\u003cT\u003e toSet(List\u003cT\u003e records)",
				"documentation": ""
			},
			{
				"signature": "@SafeVarargs\n  @SuppressWarnings(\"varargs\")\n  private static \u003cT\u003e Set\u003cT\u003e asSet(T... records)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJoin()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTupleEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTupleEncoderSchema()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNestedTupleEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPrimitiveEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLocalDateAndInstantEncoders()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLocalDateTimeEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDurationEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPeriodEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKryoEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJavaEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRandomSplit()",
				"documentation": ""
			},
			{
				"signature": "double[] arraySplit =",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJavaEncoderErrorMessageForPrivateClass()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testKryoEncoderErrorMessageForPrivateClass()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJavaBeanEncoder()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJavaBeanEncoder2()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRuntimeNullabilityCheck()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void test()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBeanWithEnum()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testEmptyBean()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCircularReferenceBean1()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCircularReferenceBean2()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCircularReferenceBean3()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNullInTopLevelBean()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSerializeNull()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testNonNullField()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSpecificLists()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.api.java.Optional",
			"org.apache.spark.sql.streaming.GroupStateTimeout",
			"org.apache.spark.sql.streaming.OutputMode",
			"test.org.apache.spark.sql.CircularReference1Bean",
			"test.org.apache.spark.sql.JavaDatasetSuite.KryoSerializable",
			"test.org.apache.spark.sql.JavaDatasetSuite.JavaSerializable",
			"test.org.apache.spark.sql.JavaDatasetSuite.SimpleJavaBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.SimpleJavaBean2",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedJavaBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.SmallBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBeanWithNonNullField",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBean2",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean.Builder",
			"test.org.apache.spark.sql.MyEnum",
			"test.org.apache.spark.sql.JavaDatasetSuite.BeanWithEnum",
			"test.org.apache.spark.sql.JavaDatasetSuite.EmptyBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.CircularReference3Bean",
			"test.org.apache.spark.sql.CircularReference4Bean",
			"test.org.apache.spark.sql.JavaDatasetSuite.SpecificListsBean"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.KryoSerializable",
			"test.org.apache.spark.sql.JavaDatasetSuite.JavaSerializable",
			"test.org.apache.spark.sql.JavaDatasetSuite.PrivateClassTest",
			"test.org.apache.spark.sql.JavaDatasetSuite.SimpleJavaBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.SimpleJavaBean2",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedJavaBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.SmallBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBeanWithNonNullField",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBean2",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean",
			"test.org.apache.spark.sql.MyEnum",
			"test.org.apache.spark.sql.JavaDatasetSuite.BeanWithEnum",
			"test.org.apache.spark.sql.JavaDatasetSuite.EmptyBean",
			"test.org.apache.spark.sql.CircularReference1Bean",
			"test.org.apache.spark.sql.CircularReference2Bean",
			"test.org.apache.spark.sql.JavaDatasetSuite.CircularReference3Bean",
			"test.org.apache.spark.sql.CircularReference4Bean",
			"test.org.apache.spark.sql.CircularReference5Bean",
			"test.org.apache.spark.sql.JavaDatasetSuite.SpecificListsBean"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.KryoSerializable",
		"extends": "",
		"Methods": [
			{
				"signature": "KryoSerializable(String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.JavaSerializable",
		"extends": "",
		"Methods": [
			{
				"signature": "JavaSerializable(String value)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object other)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * For testing error messages when creating an encoder on a private class. This is done\n   * here since we cannot create truly private classes in Scala.\n   */",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.PrivateClassTest",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.SimpleJavaBean",
		"extends": "",
		"Methods": [
			{
				"signature": "public boolean isA()",
				"documentation": ""
			},
			{
				"signature": "public void setA(boolean a)",
				"documentation": ""
			},
			{
				"signature": "public int getB()",
				"documentation": ""
			},
			{
				"signature": "public void setB(int b)",
				"documentation": ""
			},
			{
				"signature": "public byte[] getC()",
				"documentation": ""
			},
			{
				"signature": "public void setC(byte[] c)",
				"documentation": ""
			},
			{
				"signature": "public String[] getD()",
				"documentation": ""
			},
			{
				"signature": "public void setD(String[] d)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cString\u003e getE()",
				"documentation": ""
			},
			{
				"signature": "public void setE(List\u003cString\u003e e)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cLong\u003e getF()",
				"documentation": ""
			},
			{
				"signature": "public void setF(List\u003cLong\u003e f)",
				"documentation": ""
			},
			{
				"signature": "public Map\u003cInteger, String\u003e getG()",
				"documentation": ""
			},
			{
				"signature": "public void setG(Map\u003cInteger, String\u003e g)",
				"documentation": ""
			},
			{
				"signature": "public Map\u003cList\u003cLong\u003e, Map\u003cString, String\u003e\u003e getH()",
				"documentation": ""
			},
			{
				"signature": "public void setH(Map\u003cList\u003cLong\u003e, Map\u003cString, String\u003e\u003e h)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.SimpleJavaBean2",
		"extends": "",
		"Methods": [
			{
				"signature": "public Timestamp getA()",
				"documentation": ""
			},
			{
				"signature": "public void setA(Timestamp a)",
				"documentation": ""
			},
			{
				"signature": "public Date getB()",
				"documentation": ""
			},
			{
				"signature": "public void setB(Date b)",
				"documentation": ""
			},
			{
				"signature": "public java.math.BigDecimal getC()",
				"documentation": ""
			},
			{
				"signature": "public void setC(java.math.BigDecimal c)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.NestedJavaBean",
		"extends": "",
		"Methods": [
			{
				"signature": "public SimpleJavaBean getA()",
				"documentation": ""
			},
			{
				"signature": "public void setA(SimpleJavaBean a)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.SmallBean",
		"extends": "",
		"Methods": [
			{
				"signature": "public int getB()",
				"documentation": ""
			},
			{
				"signature": "public void setB(int b)",
				"documentation": ""
			},
			{
				"signature": "public String getA()",
				"documentation": ""
			},
			{
				"signature": "public void setA(String a)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBean",
		"extends": "",
		"Methods": [
			{
				"signature": "public SmallBean getF()",
				"documentation": ""
			},
			{
				"signature": "public void setF(SmallBean f)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBeanWithNonNullField",
		"extends": "",
		"Methods": [
			{
				"signature": "@Nonnull\n    public SmallBean getNonNull_f()",
				"documentation": ""
			},
			{
				"signature": "public void setNonNull_f(SmallBean f)",
				"documentation": ""
			},
			{
				"signature": "public SmallBean getNullable_f()",
				"documentation": ""
			},
			{
				"signature": "public void setNullable_f(SmallBean f)",
				"documentation": ""
			},
			{
				"signature": "@Nonnull\n    public Map\u003cString, SmallBean\u003e getChildMap()",
				"documentation": ""
			},
			{
				"signature": "public void setChildMap(Map\u003cString, SmallBean\u003e childMap)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.NestedSmallBean2",
		"extends": "",
		"Methods": [
			{
				"signature": "@Nonnull\n    public NestedSmallBeanWithNonNullField getF()",
				"documentation": ""
			},
			{
				"signature": "public void setF(NestedSmallBeanWithNonNullField f)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.Nesting3",
		"extends": "",
		"Methods": [
			{
				"signature": "public Nesting3()",
				"documentation": ""
			},
			{
				"signature": "public Nesting3(Integer field3_1, Double field3_2, String field3_3)",
				"documentation": ""
			},
			{
				"signature": "private Nesting3(Builder builder)",
				"documentation": ""
			},
			{
				"signature": "public static Builder newBuilder()",
				"documentation": ""
			},
			{
				"signature": "public Integer getField3_1()",
				"documentation": ""
			},
			{
				"signature": "public void setField3_1(Integer field3_1)",
				"documentation": ""
			},
			{
				"signature": "public Double getField3_2()",
				"documentation": ""
			},
			{
				"signature": "public void setField3_2(Double field3_2)",
				"documentation": ""
			},
			{
				"signature": "public String getField3_3()",
				"documentation": ""
			},
			{
				"signature": "public void setField3_3(String field3_3)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder"
		],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder",
		"extends": "",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": ""
			},
			{
				"signature": "public Builder field3_1(Integer field3_1)",
				"documentation": ""
			},
			{
				"signature": "public Builder field3_2(Double field3_2)",
				"documentation": ""
			},
			{
				"signature": "public Builder field3_3(String field3_3)",
				"documentation": ""
			},
			{
				"signature": "public Nesting3 build()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean.Builder"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.Nesting2",
		"extends": "",
		"Methods": [
			{
				"signature": "public Nesting2()",
				"documentation": ""
			},
			{
				"signature": "public Nesting2(Nesting3 field2_1, Nesting3 field2_2, Nesting3 field2_3)",
				"documentation": ""
			},
			{
				"signature": "private Nesting2(Builder builder)",
				"documentation": ""
			},
			{
				"signature": "public static Builder newBuilder()",
				"documentation": ""
			},
			{
				"signature": "public Nesting3 getField2_1()",
				"documentation": ""
			},
			{
				"signature": "public void setField2_1(Nesting3 field2_1)",
				"documentation": ""
			},
			{
				"signature": "public Nesting3 getField2_2()",
				"documentation": ""
			},
			{
				"signature": "public void setField2_2(Nesting3 field2_2)",
				"documentation": ""
			},
			{
				"signature": "public Nesting3 getField2_3()",
				"documentation": ""
			},
			{
				"signature": "public void setField2_3(Nesting3 field2_3)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder"
		],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder",
		"extends": "",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": ""
			},
			{
				"signature": "public Builder field2_1(Nesting3 field2_1)",
				"documentation": ""
			},
			{
				"signature": "public Builder field2_2(Nesting3 field2_2)",
				"documentation": ""
			},
			{
				"signature": "public Builder field2_3(Nesting3 field2_3)",
				"documentation": ""
			},
			{
				"signature": "public Nesting2 build()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder"
		],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean.Builder"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.Nesting1",
		"extends": "",
		"Methods": [
			{
				"signature": "public Nesting1()",
				"documentation": ""
			},
			{
				"signature": "public Nesting1(Nesting2 field1_1, Nesting2 field1_2, Nesting2 field1_3)",
				"documentation": ""
			},
			{
				"signature": "private Nesting1(Builder builder)",
				"documentation": ""
			},
			{
				"signature": "public static Builder newBuilder()",
				"documentation": ""
			},
			{
				"signature": "public Nesting2 getField1_1()",
				"documentation": ""
			},
			{
				"signature": "public void setField1_1(Nesting2 field1_1)",
				"documentation": ""
			},
			{
				"signature": "public Nesting2 getField1_2()",
				"documentation": ""
			},
			{
				"signature": "public void setField1_2(Nesting2 field1_2)",
				"documentation": ""
			},
			{
				"signature": "public Nesting2 getField1_3()",
				"documentation": ""
			},
			{
				"signature": "public void setField1_3(Nesting2 field1_3)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1.Builder"
		],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1.Builder"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.Nesting1.Builder",
		"extends": "",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": ""
			},
			{
				"signature": "public Builder field1_1(Nesting2 field1_1)",
				"documentation": ""
			},
			{
				"signature": "public Builder field1_2(Nesting2 field1_2)",
				"documentation": ""
			},
			{
				"signature": "public Builder field1_3(Nesting2 field1_3)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 build()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder"
		],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean.Builder"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean",
		"extends": "",
		"Methods": [
			{
				"signature": "public NestedComplicatedJavaBean()",
				"documentation": ""
			},
			{
				"signature": "private NestedComplicatedJavaBean(Builder builder)",
				"documentation": ""
			},
			{
				"signature": "public static Builder newBuilder()",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField1()",
				"documentation": ""
			},
			{
				"signature": "public void setField1(Nesting1 field1)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField2()",
				"documentation": ""
			},
			{
				"signature": "public void setField2(Nesting1 field2)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField3()",
				"documentation": ""
			},
			{
				"signature": "public void setField3(Nesting1 field3)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField4()",
				"documentation": ""
			},
			{
				"signature": "public void setField4(Nesting1 field4)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField5()",
				"documentation": ""
			},
			{
				"signature": "public void setField5(Nesting1 field5)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField6()",
				"documentation": ""
			},
			{
				"signature": "public void setField6(Nesting1 field6)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField7()",
				"documentation": ""
			},
			{
				"signature": "public void setField7(Nesting1 field7)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField8()",
				"documentation": ""
			},
			{
				"signature": "public void setField8(Nesting1 field8)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField9()",
				"documentation": ""
			},
			{
				"signature": "public void setField9(Nesting1 field9)",
				"documentation": ""
			},
			{
				"signature": "public Nesting1 getField10()",
				"documentation": ""
			},
			{
				"signature": "public void setField10(Nesting1 field10)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean.Builder"
		],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean.Builder"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean.Builder",
		"extends": "",
		"Methods": [
			{
				"signature": "private Builder()",
				"documentation": ""
			},
			{
				"signature": "public Builder field1(Nesting1 field1)",
				"documentation": ""
			},
			{
				"signature": "public Builder field2(Nesting1 field2)",
				"documentation": ""
			},
			{
				"signature": "public Builder field3(Nesting1 field3)",
				"documentation": ""
			},
			{
				"signature": "public Builder field4(Nesting1 field4)",
				"documentation": ""
			},
			{
				"signature": "public Builder field5(Nesting1 field5)",
				"documentation": ""
			},
			{
				"signature": "public Builder field6(Nesting1 field6)",
				"documentation": ""
			},
			{
				"signature": "public Builder field7(Nesting1 field7)",
				"documentation": ""
			},
			{
				"signature": "public Builder field8(Nesting1 field8)",
				"documentation": ""
			},
			{
				"signature": "public Builder field9(Nesting1 field9)",
				"documentation": ""
			},
			{
				"signature": "public Builder field10(Nesting1 field10)",
				"documentation": ""
			},
			{
				"signature": "public NestedComplicatedJavaBean build()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting3.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting2.Builder",
			"test.org.apache.spark.sql.JavaDatasetSuite.Nesting1.Builder"
		],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite",
			"test.org.apache.spark.sql.JavaDatasetSuite.NestedComplicatedJavaBean"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.MyEnum",
		"extends": "",
		"Methods": [
			{
				"signature": "MyEnum(String url)",
				"documentation": ""
			},
			{
				"signature": "public String getUrl()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.BeanWithEnum",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getRegularField()",
				"documentation": ""
			},
			{
				"signature": "public void setRegularField(String regularField)",
				"documentation": ""
			},
			{
				"signature": "public MyEnum getEnumField()",
				"documentation": ""
			},
			{
				"signature": "public void setEnumField(MyEnum field)",
				"documentation": ""
			},
			{
				"signature": "public BeanWithEnum(MyEnum enumField, String regularField)",
				"documentation": ""
			},
			{
				"signature": "public BeanWithEnum()",
				"documentation": ""
			},
			{
				"signature": "public String toString()",
				"documentation": ""
			},
			{
				"signature": "public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "public boolean equals(Object other)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.EmptyBean",
		"extends": "",
		"Methods": [],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.CircularReference1Bean",
		"extends": "",
		"Methods": [
			{
				"signature": "public CircularReference2Bean getChild()",
				"documentation": ""
			},
			{
				"signature": "public void setChild(CircularReference2Bean child)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.CircularReference2Bean",
		"extends": "",
		"Methods": [
			{
				"signature": "public CircularReference1Bean getChild()",
				"documentation": ""
			},
			{
				"signature": "public void setChild(CircularReference1Bean child)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.CircularReference3Bean",
		"extends": "",
		"Methods": [
			{
				"signature": "public CircularReference3Bean[] getChild()",
				"documentation": ""
			},
			{
				"signature": "public void setChild(CircularReference3Bean[] child)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.CircularReference4Bean",
		"extends": "",
		"Methods": [
			{
				"signature": "public Map\u003cString, CircularReference5Bean\u003e getChild()",
				"documentation": ""
			},
			{
				"signature": "public void setChild(Map\u003cString, CircularReference5Bean\u003e child)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.CircularReference5Bean",
		"extends": "",
		"Methods": [
			{
				"signature": "public String getId()",
				"documentation": ""
			},
			{
				"signature": "public List\u003cCircularReference4Bean\u003e getChild()",
				"documentation": ""
			},
			{
				"signature": "public void setId(String id)",
				"documentation": ""
			},
			{
				"signature": "public void setChild(List\u003cCircularReference4Bean\u003e child)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaDatasetSuite.SpecificListsBean",
		"extends": "",
		"Methods": [
			{
				"signature": "public ArrayList\u003cInteger\u003e getArrayList()",
				"documentation": ""
			},
			{
				"signature": "public void setArrayList(ArrayList\u003cInteger\u003e arrayList)",
				"documentation": ""
			},
			{
				"signature": "public LinkedList\u003cInteger\u003e getLinkedList()",
				"documentation": ""
			},
			{
				"signature": "public void setLinkedList(LinkedList\u003cInteger\u003e linkedList)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cInteger\u003e getList()",
				"documentation": ""
			},
			{
				"signature": "public void setList(List\u003cInteger\u003e list)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean equals(Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public int hashCode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.JavaDatasetSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaHigherOrderFunctionsSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private void checkAnswer(Dataset\u003cRow\u003e actualDS, List\u003cRow\u003e expected) throws Exception",
				"documentation": ""
			},
			{
				"signature": "@SafeVarargs\n    private static \u003cT\u003e List\u003cRow\u003e toRows(T... objs)",
				"documentation": ""
			},
			{
				"signature": "@SafeVarargs\n    private static \u003cT\u003e T[] makeArray(T... ts)",
				"documentation": ""
			},
			{
				"signature": "private void setUpArrDf()",
				"documentation": ""
			},
			{
				"signature": "private void setUpMapDf()",
				"documentation": ""
			},
			{
				"signature": "@Before\n    public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n    public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testTransform() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testFilter() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testExists() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testForall() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testAggregate() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testZipWith() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testTransformKeys() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testTransformValues() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testMapFilter() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n    public void testMapZipWith() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaRowSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void constructSimpleRow()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void constructComplexRow()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.RowFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaSaveLoadSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private static void checkAnswer(Dataset\u003cRow\u003e actual, List\u003cRow\u003e expected)",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void saveAndLoad()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void saveAndLoadWithSchema()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * It is used for register Java UDF from PySpark\n */",
		"name": "test.org.apache.spark.sql.JavaStringLength",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Integer call(String str) throws Exception",
				"documentation": "/**\n * It is used for register Java UDF from PySpark\n */"
			}
		],
		"interfaces": [
			"org.apache.spark.sql.api.java.UDF1"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaUDAFSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void udf1Test()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaUDFSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void udf1Test()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void udf2Test()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void udf3Test()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void udf4Test()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void udf5Test()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void udf6Test()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void udf7Test()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void sourceTest()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.JavaUDFSuite.StringLengthTest"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.JavaUDFSuite.StringLengthTest",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public Integer call(String str1, String str2)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.api.java.UDF2"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example {@link UserDefinedAggregateFunction} to calculate a special average value of a\n * {@link org.apache.spark.sql.types.DoubleType} column. This special average value is the sum\n * of the average value of input values and 100.0.\n */",
		"name": "test.org.apache.spark.sql.MyDoubleAvg",
		"extends": "org.apache.spark.sql.expressions.UserDefinedAggregateFunction",
		"Methods": [
			{
				"signature": "public MyDoubleAvg()",
				"documentation": "/**\n * An example {@link UserDefinedAggregateFunction} to calculate a special average value of a\n * {@link org.apache.spark.sql.types.DoubleType} column. This special average value is the sum\n * of the average value of input values and 100.0.\n */"
			},
			{
				"signature": "@Override public StructType inputSchema()",
				"documentation": ""
			},
			{
				"signature": "@Override public StructType bufferSchema()",
				"documentation": ""
			},
			{
				"signature": "@Override public DataType dataType()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean deterministic()",
				"documentation": ""
			},
			{
				"signature": "@Override public void initialize(MutableAggregationBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override public void update(MutableAggregationBuffer buffer, Row input)",
				"documentation": ""
			},
			{
				"signature": "@Override public void merge(MutableAggregationBuffer buffer1, Row buffer2)",
				"documentation": ""
			},
			{
				"signature": "@Override public Object evaluate(Row buffer)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.types.DataTypes"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An example {@link UserDefinedAggregateFunction} to calculate the sum of a\n * {@link org.apache.spark.sql.types.DoubleType} column.\n */",
		"name": "test.org.apache.spark.sql.MyDoubleSum",
		"extends": "org.apache.spark.sql.expressions.UserDefinedAggregateFunction",
		"Methods": [
			{
				"signature": "public MyDoubleSum()",
				"documentation": "/**\n * An example {@link UserDefinedAggregateFunction} to calculate the sum of a\n * {@link org.apache.spark.sql.types.DoubleType} column.\n */"
			},
			{
				"signature": "@Override public StructType inputSchema()",
				"documentation": ""
			},
			{
				"signature": "@Override public StructType bufferSchema()",
				"documentation": ""
			},
			{
				"signature": "@Override public DataType dataType()",
				"documentation": ""
			},
			{
				"signature": "@Override public boolean deterministic()",
				"documentation": ""
			},
			{
				"signature": "@Override public void initialize(MutableAggregationBuffer buffer)",
				"documentation": ""
			},
			{
				"signature": "@Override public void update(MutableAggregationBuffer buffer, Row input)",
				"documentation": ""
			},
			{
				"signature": "@Override public void merge(MutableAggregationBuffer buffer1, Row buffer2)",
				"documentation": ""
			},
			{
				"signature": "@Override public Object evaluate(Row buffer)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.hive.JavaDataFrameSuite"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Table getTable(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.TestingV2Source"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedScanBuilder",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedBatch",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedReaderFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedScanBuilder",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedBatch",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedReaderFactory"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedScanBuilder",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void pruneColumns(StructType requiredSchema)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public StructType readSchema()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Filter[] pushFilters(Filter[] filters)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Filter[] pushedFilters()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Scan build()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Batch toBatch()",
				"documentation": ""
			}
		],
		"interfaces": [
			"ScanBuilder",
			"Scan",
			"SupportsPushDownFilters",
			"SupportsPushDownRequiredColumns"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedBatch",
		"extends": "",
		"Methods": [
			{
				"signature": "AdvancedBatch(StructType requiredSchema, Filter[] filters)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public InputPartition[] planInputPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReaderFactory createReaderFactory()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Batch"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2.AdvancedReaderFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "AdvancedReaderFactory(StructType requiredSchema)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReader\u003cInternalRow\u003e createReader(InputPartition partition)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public boolean next() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public InternalRow get()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"PartitionReaderFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Table getTable(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.TestingV2Source"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedScanBuilderWithV2Filter",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedBatchWithV2Filter",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedReaderFactoryWithV2Filter"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedScanBuilderWithV2Filter",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedBatchWithV2Filter",
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedReaderFactoryWithV2Filter"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedScanBuilderWithV2Filter",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void pruneColumns(StructType requiredSchema)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public StructType readSchema()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Predicate[] pushPredicates(Predicate[] predicates)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Predicate[] pushedPredicates()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Scan build()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Batch toBatch()",
				"documentation": ""
			}
		],
		"interfaces": [
			"ScanBuilder",
			"Scan",
			"SupportsPushDownV2Filters",
			"SupportsPushDownRequiredColumns"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedBatchWithV2Filter",
		"extends": "",
		"Methods": [
			{
				"signature": "AdvancedBatchWithV2Filter(StructType requiredSchema, Predicate[] predicates)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public InputPartition[] planInputPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReaderFactory createReaderFactory()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Batch"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter.AdvancedReaderFactoryWithV2Filter",
		"extends": "",
		"Methods": [
			{
				"signature": "AdvancedReaderFactoryWithV2Filter(StructType requiredSchema)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReader\u003cInternalRow\u003e createReader(InputPartition partition)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public boolean next() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public InternalRow get()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"PartitionReaderFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaAdvancedDataSourceV2WithV2Filter"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Table getTable(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.TestingV2Source"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.connector.read.PartitionReader",
			"org.apache.spark.sql.vectorized.ColumnarBatch",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.ColumnarReaderFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.ColumnarReaderFactory"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.MyScanBuilder",
		"extends": "test.org.apache.spark.sql.connector.JavaSimpleScanBuilder",
		"Methods": [
			{
				"signature": "@Override\n    public InputPartition[] planInputPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReaderFactory createReaderFactory()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.ColumnarReaderFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public boolean supportColumnarReads(InputPartition partition)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReader\u003cInternalRow\u003e createReader(InputPartition partition)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReader\u003cColumnarBatch\u003e createColumnarReader(InputPartition partition)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public boolean next() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public ColumnarBatch get()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.PartitionReaderFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.connector.read.PartitionReader",
			"org.apache.spark.sql.vectorized.ColumnarBatch",
			"org.apache.spark.sql.execution.vectorized.OnHeapColumnVector"
		],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Table getTable(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public Transform[] partitioning()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.TestingV2Source"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.connector.expressions.Expressions",
			"org.apache.spark.sql.connector.read.partitioning.KeyGroupedPartitioning",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.SpecificInputPartition",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.SpecificReaderFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.SpecificInputPartition",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.SpecificReaderFactory"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.MyScanBuilder",
		"extends": "test.org.apache.spark.sql.connector.JavaSimpleScanBuilder",
		"Methods": [
			{
				"signature": "@Override\n    public InputPartition[] planInputPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReaderFactory createReaderFactory()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Partitioning outputPartitioning()",
				"documentation": ""
			}
		],
		"interfaces": [
			"SupportsReportPartitioning"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.connector.expressions.Expressions",
			"org.apache.spark.sql.connector.read.partitioning.KeyGroupedPartitioning"
		],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.SpecificInputPartition",
		"extends": "",
		"Methods": [
			{
				"signature": "SpecificInputPartition(int[] i, int[] j)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public InternalRow partitionKey()",
				"documentation": ""
			}
		],
		"interfaces": [
			"InputPartition",
			"HasPartitionKey"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.SpecificReaderFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public PartitionReader\u003cInternalRow\u003e createReader(InputPartition partition)",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public boolean next() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public InternalRow get()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"PartitionReaderFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaRangeInputPartition",
		"extends": "",
		"Methods": [
			{
				"signature": "JavaRangeInputPartition(int start, int end)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.InputPartition"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaReportStatisticsDataSource",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Table getTable(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.TestingV2Source"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.connector.read.Statistics"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.JavaReportStatisticsDataSource.MyScanBuilder"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaReportStatisticsDataSource.MyScanBuilder",
		"extends": "test.org.apache.spark.sql.connector.JavaSimpleScanBuilder",
		"Methods": [
			{
				"signature": "@Override\n    public Statistics estimateStatistics()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public OptionalLong sizeInBytes()",
				"documentation": ""
			},
			{
				"signature": "@Override\n        public OptionalLong numRows()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public InputPartition[] planInputPartitions()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.SupportsReportStatistics"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.connector.read.Statistics"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSchemaRequiredDataSource",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public boolean supportsExternalMetadata()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public StructType inferSchema(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Table getTable(\n      StructType schema, Transform[] partitioning, Map\u003cString, String\u003e properties)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public StructType schema()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.TableProvider"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.connector.JavaSchemaRequiredDataSource.MyScanBuilder"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.JavaSchemaRequiredDataSource.MyScanBuilder"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSchemaRequiredDataSource.MyScanBuilder",
		"extends": "test.org.apache.spark.sql.connector.JavaSimpleScanBuilder",
		"Methods": [
			{
				"signature": "MyScanBuilder(StructType schema)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public StructType readSchema()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public InputPartition[] planInputPartitions()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSchemaRequiredDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleBatchTable",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public StructType schema()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Set\u003cTableCapability\u003e capabilities()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.Table",
			"org.apache.spark.sql.connector.catalog.SupportsRead"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyTable"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleDataSourceV2",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Table getTable(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.TestingV2Source"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.JavaSimpleDataSourceV2.MyScanBuilder"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleDataSourceV2.MyScanBuilder",
		"extends": "test.org.apache.spark.sql.connector.JavaSimpleScanBuilder",
		"Methods": [
			{
				"signature": "@Override\n    public InputPartition[] planInputPartitions()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleReaderFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public PartitionReader\u003cInternalRow\u003e createReader(InputPartition partition)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public boolean next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public InternalRow get()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void close()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.PartitionReaderFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.connector.read.PartitionReader"
		],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleScanBuilder",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Scan build()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Batch toBatch()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public StructType readSchema()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public PartitionReaderFactory createReaderFactory()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.ScanBuilder",
			"org.apache.spark.sql.connector.read.Scan",
			"org.apache.spark.sql.connector.read.Batch"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"test.org.apache.spark.sql.connector.JavaColumnarDataSourceV2.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaPartitionAwareDataSource.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaReportStatisticsDataSource.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaSchemaRequiredDataSource.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaSimpleDataSourceV2.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyScanBuilder"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A HDFS based transactional writable data source which is implemented by java.\n * Each task writes data to `target/_temporary/uniqueId/$jobId-$partitionId-$attemptNumber`.\n * Each job moves files from `target/_temporary/uniqueId/` to `target`.\n */",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public Table getTable(CaseInsensitiveStringMap options)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.TestingV2Source"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyWriteBuilder",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyWrite",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyBatchWrite",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyTable",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVInputPartitionReader",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVReaderFactory",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVDataWriterFactory",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVDataWriter"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyScanBuilder",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyWriteBuilder",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyWrite",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyBatchWrite",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyTable",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVInputPartitionReader",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVReaderFactory",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVDataWriterFactory",
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVDataWriter"
		]
	},
	{
		"documentation": "/**\n * A HDFS based transactional writable data source which is implemented by java.\n * Each task writes data to `target/_temporary/uniqueId/$jobId-$partitionId-$attemptNumber`.\n * Each job moves files from `target/_temporary/uniqueId/` to `target`.\n */",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyScanBuilder",
		"extends": "test.org.apache.spark.sql.connector.JavaSimpleScanBuilder",
		"Methods": [
			{
				"signature": "MyScanBuilder(String path, Configuration conf)",
				"documentation": "/**\n * A HDFS based transactional writable data source which is implemented by java.\n * Each task writes data to `target/_temporary/uniqueId/$jobId-$partitionId-$attemptNumber`.\n * Each job moves files from `target/_temporary/uniqueId/` to `target`.\n */"
			},
			{
				"signature": "@Override\n    public InputPartition[] planInputPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReaderFactory createReaderFactory()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyWriteBuilder",
		"extends": "",
		"Methods": [
			{
				"signature": "MyWriteBuilder(String path, LogicalWriteInfo info)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public WriteBuilder truncate()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Write build()",
				"documentation": ""
			}
		],
		"interfaces": [
			"WriteBuilder",
			"SupportsTruncate"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyWrite",
		"extends": "",
		"Methods": [
			{
				"signature": "MyWrite(String path, String queryId, boolean needTruncate)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public BatchWrite toBatch()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Write"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyBatchWrite",
		"extends": "",
		"Methods": [
			{
				"signature": "MyBatchWrite(String queryId, String path, Configuration conf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onDataWriterCommit(WriterCommitMessage message)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void commit(WriterCommitMessage[] messages)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void abort(WriterCommitMessage[] messages)",
				"documentation": ""
			}
		],
		"interfaces": [
			"BatchWrite"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.MyTable",
		"extends": "test.org.apache.spark.sql.connector.JavaSimpleBatchTable",
		"Methods": [
			{
				"signature": "MyTable(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public WriteBuilder newWriteBuilder(LogicalWriteInfo info)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Set\u003cTableCapability\u003e capabilities()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.SupportsWrite"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVInputPartitionReader",
		"extends": "",
		"Methods": [
			{
				"signature": "JavaCSVInputPartitionReader(String path)",
				"documentation": ""
			},
			{
				"signature": "public String getPath()",
				"documentation": ""
			},
			{
				"signature": "public void setPath(String path)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.InputPartition"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVReaderFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "JavaCSVReaderFactory(SerializableConfiguration conf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public PartitionReader\u003cInternalRow\u003e createReader(InputPartition partition)",
				"documentation": ""
			},
			{
				"signature": "@Override\n          public boolean next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n          public InternalRow get()",
				"documentation": ""
			},
			{
				"signature": "@Override\n          public void close() throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.read.PartitionReaderFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.connector.read.PartitionReader"
		],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVDataWriterFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "JavaCSVDataWriterFactory(String path, String jobId, SerializableConfiguration conf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DataWriter\u003cInternalRow\u003e createWriter(int partitionId, long taskId)",
				"documentation": ""
			}
		],
		"interfaces": [
			"DataWriterFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource.JavaCSVDataWriter",
		"extends": "",
		"Methods": [
			{
				"signature": "JavaCSVDataWriter(FileSystem fs, Path file) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void write(InternalRow record) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public WriterCommitMessage commit() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void abort() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void close()",
				"documentation": ""
			}
		],
		"interfaces": [
			"DataWriter"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.JavaSimpleWritableDataSource"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaAverage",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public BoundFunction bind(StructType inputType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String description()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.functions.UnboundFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaAverage.JavaDoubleAverage",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaAverage.State"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaAverage.JavaDoubleAverage",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaAverage.State"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaAverage.JavaDoubleAverage",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public State\u003cDouble\u003e newAggregationState()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public State\u003cDouble\u003e update(State\u003cDouble\u003e state, InternalRow input)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Double produceResult(State\u003cDouble\u003e state)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public State\u003cDouble\u003e merge(State\u003cDouble\u003e leftState, State\u003cDouble\u003e rightState)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DataType[] inputTypes()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DataType resultType()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String name()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.functions.AggregateFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaAverage"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaAverage.State",
		"extends": "",
		"Methods": [
			{
				"signature": "State(T sum, T count)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaAverage"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd",
		"extends": "",
		"Methods": [
			{
				"signature": "public JavaLongAdd(ScalarFunction\u003cLong\u003e impl)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public BoundFunction bind(StructType inputType)",
				"documentation": ""
			},
			{
				"signature": "private static void checkInputType(DataType type)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String description()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.functions.UnboundFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddBase",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddDefault",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddStaticMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddMismatchMagic"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddBase",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddDefault",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddStaticMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddMismatchMagic"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddBase",
		"extends": "",
		"Methods": [
			{
				"signature": "JavaLongAddBase(boolean isResultNullable)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DataType[] inputTypes()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DataType resultType()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean isResultNullable()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.functions.ScalarFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddDefault",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddStaticMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddMismatchMagic"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddDefault",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddBase",
		"Methods": [
			{
				"signature": "public JavaLongAddDefault(boolean isResultNullable)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Long produceResult(InternalRow input)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddMagic",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddBase",
		"Methods": [
			{
				"signature": "public JavaLongAddMagic(boolean isResultNullable)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String name()",
				"documentation": ""
			},
			{
				"signature": "public long invoke(long left, long right)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddStaticMagic",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddBase",
		"Methods": [
			{
				"signature": "public JavaLongAddStaticMagic(boolean isResultNullable)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String name()",
				"documentation": ""
			},
			{
				"signature": "public static long invoke(long left, long right)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddMismatchMagic",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd.JavaLongAddBase",
		"Methods": [
			{
				"signature": "public JavaLongAddMismatchMagic()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String name()",
				"documentation": ""
			},
			{
				"signature": "public long invoke(int left, int right)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaLongAdd"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test V2 function which add a random number to the input integer.\n */",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd",
		"extends": "",
		"Methods": [
			{
				"signature": "public JavaRandomAdd(BoundFunction fn)",
				"documentation": "/**\n * Test V2 function which add a random number to the input integer.\n */"
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public BoundFunction bind(StructType inputType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String description()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.functions.UnboundFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddBase",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddDefault",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddStaticMagic"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddBase",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public DataType[] inputTypes()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DataType resultType()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean isDeterministic()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.functions.ScalarFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddDefault",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddStaticMagic"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddDefault",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddBase",
		"Methods": [
			{
				"signature": "@Override\n    public Integer produceResult(InternalRow input)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddMagic",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddBase",
		"Methods": [
			{
				"signature": "public int invoke(int input)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddStaticMagic",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaRandomAdd.JavaRandomAddBase",
		"Methods": [
			{
				"signature": "public static int invoke(int input)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen",
		"extends": "",
		"Methods": [
			{
				"signature": "public JavaStrLen(BoundFunction fn)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String name()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public BoundFunction bind(StructType inputType)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String description()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.functions.UnboundFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenDefault",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenStaticMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBoth",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBadStaticMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenNoImpl",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenMagicNullSafe",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenStaticMagicNullSafe"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public DataType[] inputTypes()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public DataType resultType()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String name()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.spark.sql.connector.catalog.functions.ScalarFunction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenDefault",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenStaticMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBoth",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBadStaticMagic",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenNoImpl",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenMagicNullSafe",
			"test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenStaticMagicNullSafe"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenDefault",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"Methods": [
			{
				"signature": "@Override\n    public Integer produceResult(InternalRow input)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenMagic",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"Methods": [
			{
				"signature": "public int invoke(UTF8String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenStaticMagic",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"Methods": [
			{
				"signature": "public static int invoke(UTF8String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBoth",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"Methods": [
			{
				"signature": "@Override\n    public Integer produceResult(InternalRow input)",
				"documentation": ""
			},
			{
				"signature": "public int invoke(UTF8String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBadStaticMagic",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"Methods": [
			{
				"signature": "public static int invoke(String str)",
				"documentation": ""
			},
			{
				"signature": "public int invoke(UTF8String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenNoImpl",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenMagicNullSafe",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"Methods": [
			{
				"signature": "public int invoke(UTF8String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenStaticMagicNullSafe",
		"extends": "test.org.apache.spark.sql.connector.catalog.functions.JavaStrLen.JavaStrLenBase",
		"Methods": [
			{
				"signature": "public static int invoke(UTF8String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A Hadoop KeyProvider that lets us test the interaction\n * with the Hadoop code.\n *\n * https://github.com/apache/orc/blob/rel/release-1.6.7/java/tools/src/test/org/apache/orc/impl/FakeKeyProvider.java\n *\n * This file intentionally keeps the original file except\n * (1) package name, (2) import order, (3) a few indentation\n */",
		"name": "test.org.apache.spark.sql.execution.datasources.orc.FakeKeyProvider",
		"extends": "org.apache.hadoop.crypto.key.KeyProvider",
		"Methods": [
			{
				"signature": "public FakeKeyProvider(Configuration conf)",
				"documentation": "/**\n * A Hadoop KeyProvider that lets us test the interaction\n * with the Hadoop code.\n *\n * https://github.com/apache/orc/blob/rel/release-1.6.7/java/tools/src/test/org/apache/orc/impl/FakeKeyProvider.java\n *\n * This file intentionally keeps the original file except\n * (1) package name, (2) import order, (3) a few indentation\n */"
			},
			{
				"signature": "@Override\n  public KeyVersion getKeyVersion(String name)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public List\u003cString\u003e getKeys()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public List\u003cKeyVersion\u003e getKeyVersions(String name)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Metadata getMetadata(String name)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public KeyVersion createKey(String name, byte[] bytes, Options options)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void deleteKey(String name)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public KeyVersion rollNewVersion(String name, byte[] bytes)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void flush()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.sql.execution.datasources.orc.FakeKeyProvider.TestMetadata",
			"test.org.apache.spark.sql.execution.datasources.orc.FakeKeyProvider.Factory"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.execution.datasources.orc.FakeKeyProvider.TestMetadata",
		"extends": "org.apache.hadoop.crypto.key.KeyProvider.Metadata",
		"Methods": [
			{
				"signature": "TestMetadata(String cipher, int bitLength, int versions)",
				"documentation": ""
			},
			{
				"signature": "public int addVersion()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.execution.datasources.orc.FakeKeyProvider.Factory",
		"extends": "org.apache.hadoop.crypto.key.KeyProviderFactory",
		"Methods": [
			{
				"signature": "@Override\n    public KeyProvider createProvider(URI uri, Configuration conf) throws IOException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Test the RecordBinaryComparator, which compares two UnsafeRows by their binary form.\n */",
		"name": "test.org.apache.spark.sql.execution.sort.RecordBinaryComparatorSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void beforeEach()",
				"documentation": "/**\n * Test the RecordBinaryComparator, which compares two UnsafeRows by their binary form.\n */"
			},
			{
				"signature": "@After\n  public void afterEach()",
				"documentation": ""
			},
			{
				"signature": "private void insertRow(UnsafeRow row)",
				"documentation": ""
			},
			{
				"signature": "private int compare(int index1, int index2)",
				"documentation": ""
			},
			{
				"signature": "private int computeSizeInBytes(int originalSize)",
				"documentation": ""
			},
			{
				"signature": "private long relativeOffset(int numFields)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryComparatorForSingleColumnRow() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryComparatorForMultipleColumnRow() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryComparatorForArrayColumn() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryComparatorForMixedColumns() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryComparatorForNullColumns() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryComparatorWhenSubtractionIsDivisibleByMaxIntValue() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryComparatorWhenSubtractionCanOverflowLongValue() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testBinaryComparatorWhenOnlyTheLastColumnDiffers() throws Exception",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCompareLongsAsLittleEndian()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCompareLongsAsUnsigned()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.Platform",
			"org.apache.spark.unsafe.UnsafeAlignedOffset",
			"org.apache.spark.memory.TaskMemoryManager",
			"org.apache.spark.memory.TestMemoryConsumer"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.sql.streaming.JavaDataStreamReaderWriterSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testForeachBatchAPI() throws TimeoutException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testForeachAPI() throws TimeoutException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This is based on hive-exec-1.2.1\n * {@link org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader}.\n * This class exposes getObjectInspector which can be used for reducing\n * NameNode calls in OrcRelation.\n */",
		"name": "org.apache.hadoop.hive.ql.io.orc.SparkOrcNewRecordReader",
		"extends": "org.apache.hadoop.mapreduce.RecordReader",
		"Methods": [
			{
				"signature": "public SparkOrcNewRecordReader(Reader file, Configuration conf,\n      long offset, long length) throws IOException",
				"documentation": "/**\n * This is based on hive-exec-1.2.1\n * {@link org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.OrcRecordReader}.\n * This class exposes getObjectInspector which can be used for reducing\n * NameNode calls in OrcRelation.\n */"
			},
			{
				"signature": "@Override\n  public void close() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public NullWritable getCurrentKey() throws IOException,\n      InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OrcStruct getCurrentValue() throws IOException,\n      InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public float getProgress() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void initialize(InputSplit split, TaskAttemptContext context)\n      throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean nextKeyValue() throws IOException, InterruptedException",
				"documentation": ""
			},
			{
				"signature": "public ObjectInspector getObjectInspector()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.JavaDataFrameSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "private static void checkAnswer(Dataset\u003cRow\u003e actual, List\u003cRow\u003e expected)",
				"documentation": ""
			},
			{
				"signature": "@Before\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void saveTableAndQueryIt()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUDAF()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"test.org.apache.spark.sql.MyDoubleSum"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void saveTableAndQueryIt()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An empty UDAF that throws a semantic exception\n */",
		"name": "org.apache.spark.sql.hive.execution.UDAFEmpty",
		"extends": "org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver",
		"Methods": [
			{
				"signature": "@Override\n    public GenericUDAFEvaluator getEvaluator(TypeInfo[] info) throws SemanticException",
				"documentation": "/**\n * An empty UDAF that throws a semantic exception\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFIntegerToString",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public String evaluate(Integer i)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFListListInt",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  public long evaluate(Object obj)",
				"documentation": "/**\n   * @param obj\n   *   SQL schema: array\u0026lt;struct\u0026lt;x: int, y: int, z: int\u0026gt;\u0026gt;\n   *   Java Type: List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt;\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFListString",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public String evaluate(Object a)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * UDF that returns a raw (non-parameterized) java List.\n */",
		"name": "org.apache.spark.sql.hive.execution.UDFRawList",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"rawtypes\")\n  public List evaluate(Object o)",
				"documentation": "/**\n * UDF that returns a raw (non-parameterized) java List.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * UDF that returns a raw (non-parameterized) java Map.\n */",
		"name": "org.apache.spark.sql.hive.execution.UDFRawMap",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"rawtypes\")\n  public Map evaluate(Object o)",
				"documentation": "/**\n * UDF that returns a raw (non-parameterized) java Map.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFStringString",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public String evaluate(String s1, String s2)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFToIntIntMap",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public Map\u003cInteger, Integer\u003e evaluate(Object o)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFToListInt",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public ArrayList\u003cInteger\u003e evaluate(Object o)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * UDF that returns a nested list of maps that uses a string as its key and a list of ints as its\n * values.\n */",
		"name": "org.apache.spark.sql.hive.execution.UDFToListMapStringListInt",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public List\u003cMap\u003cString, List\u003cInteger\u003e\u003e\u003e evaluate(Object o)",
				"documentation": "/**\n * UDF that returns a nested list of maps that uses a string as its key and a list of ints as its\n * values.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFToListString",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public List\u003cString\u003e evaluate(Object o)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFToStringIntMap",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public HashMap\u003cString, Integer\u003e evaluate(Object o)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.execution.UDFTwoListList",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public String evaluate(Object o1, Object o2)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * UDF that returns a raw (non-parameterized) java List.\n */",
		"name": "org.apache.spark.sql.hive.execution.UDFWildcardList",
		"extends": "org.apache.hadoop.hive.ql.exec.UDF",
		"Methods": [
			{
				"signature": "public List\u003c?\u003e evaluate(Object o)",
				"documentation": "/**\n * UDF that returns a raw (non-parameterized) java List.\n */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This is a fork of Hive 0.13's org/apache/hadoop/hive/serde2/thrift/test/Complex.java, which\n * does not contain union fields that are not supported by Spark SQL.\n */",
		"name": "org.apache.spark.sql.hive.test.Complex",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * This is a fork of Hive 0.13's org/apache/hadoop/hive/serde2/thrift/test/Complex.java, which\n * does not contain union fields that are not supported by Spark SQL.\n */"
			},
			{
				"signature": "static",
				"documentation": ""
			},
			{
				"signature": "public Complex()",
				"documentation": ""
			},
			{
				"signature": "public Complex(\n    int aint,\n    String aString,\n    List\u003cInteger\u003e lint,\n    List\u003cString\u003e lString,\n    List\u003cIntString\u003e lintString,\n    Map\u003cString,String\u003e mStringString)",
				"documentation": ""
			},
			{
				"signature": "public Complex(Complex other)",
				"documentation": "/**\n   * Performs a deep copy on \u003ci\u003eother\u003c/i\u003e.\n   */"
			},
			{
				"signature": "public Complex deepCopy()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void clear()",
				"documentation": ""
			},
			{
				"signature": "public int getAint()",
				"documentation": ""
			},
			{
				"signature": "public void setAint(int aint)",
				"documentation": ""
			},
			{
				"signature": "public void unsetAint()",
				"documentation": ""
			},
			{
				"signature": "public boolean isSetAint()",
				"documentation": "/** Returns true if field aint is set (has been assigned a value) and false otherwise */"
			},
			{
				"signature": "public void setAintIsSet(boolean value)",
				"documentation": ""
			},
			{
				"signature": "public String getAString()",
				"documentation": ""
			},
			{
				"signature": "public void setAString(String aString)",
				"documentation": ""
			},
			{
				"signature": "public void unsetAString()",
				"documentation": ""
			},
			{
				"signature": "public boolean isSetAString()",
				"documentation": "/** Returns true if field aString is set (has been assigned a value) and false otherwise */"
			},
			{
				"signature": "public void setAStringIsSet(boolean value)",
				"documentation": ""
			},
			{
				"signature": "public int getLintSize()",
				"documentation": ""
			},
			{
				"signature": "public java.util.Iterator\u003cInteger\u003e getLintIterator()",
				"documentation": ""
			},
			{
				"signature": "public void addToLint(int elem)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cInteger\u003e getLint()",
				"documentation": ""
			},
			{
				"signature": "public void setLint(List\u003cInteger\u003e lint)",
				"documentation": ""
			},
			{
				"signature": "public void unsetLint()",
				"documentation": ""
			},
			{
				"signature": "public boolean isSetLint()",
				"documentation": "/** Returns true if field lint is set (has been assigned a value) and false otherwise */"
			},
			{
				"signature": "public void setLintIsSet(boolean value)",
				"documentation": ""
			},
			{
				"signature": "public int getLStringSize()",
				"documentation": ""
			},
			{
				"signature": "public java.util.Iterator\u003cString\u003e getLStringIterator()",
				"documentation": ""
			},
			{
				"signature": "public void addToLString(String elem)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cString\u003e getLString()",
				"documentation": ""
			},
			{
				"signature": "public void setLString(List\u003cString\u003e lString)",
				"documentation": ""
			},
			{
				"signature": "public void unsetLString()",
				"documentation": ""
			},
			{
				"signature": "public boolean isSetLString()",
				"documentation": "/** Returns true if field lString is set (has been assigned a value) and false otherwise */"
			},
			{
				"signature": "public void setLStringIsSet(boolean value)",
				"documentation": ""
			},
			{
				"signature": "public int getLintStringSize()",
				"documentation": ""
			},
			{
				"signature": "public java.util.Iterator\u003cIntString\u003e getLintStringIterator()",
				"documentation": ""
			},
			{
				"signature": "public void addToLintString(IntString elem)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cIntString\u003e getLintString()",
				"documentation": ""
			},
			{
				"signature": "public void setLintString(List\u003cIntString\u003e lintString)",
				"documentation": ""
			},
			{
				"signature": "public void unsetLintString()",
				"documentation": ""
			},
			{
				"signature": "public boolean isSetLintString()",
				"documentation": "/** Returns true if field lintString is set (has been assigned a value) and false otherwise */"
			},
			{
				"signature": "public void setLintStringIsSet(boolean value)",
				"documentation": ""
			},
			{
				"signature": "public int getMStringStringSize()",
				"documentation": ""
			},
			{
				"signature": "public void putToMStringString(String key, String val)",
				"documentation": ""
			},
			{
				"signature": "public Map\u003cString,String\u003e getMStringString()",
				"documentation": ""
			},
			{
				"signature": "public void setMStringString(Map\u003cString,String\u003e mStringString)",
				"documentation": ""
			},
			{
				"signature": "public void unsetMStringString()",
				"documentation": ""
			},
			{
				"signature": "public boolean isSetMStringString()",
				"documentation": "/** Returns true if field mStringString is set (has been assigned a value) and false otherwise */"
			},
			{
				"signature": "public void setMStringStringIsSet(boolean value)",
				"documentation": ""
			},
			{
				"signature": "public void setFieldValue(_Fields field, Object value)",
				"documentation": ""
			},
			{
				"signature": "public Object getFieldValue(_Fields field)",
				"documentation": ""
			},
			{
				"signature": "public boolean isSet(_Fields field)",
				"documentation": "/** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */"
			},
			{
				"signature": "@Override\n  public boolean equals(Object that)",
				"documentation": ""
			},
			{
				"signature": "public boolean equals(Complex that)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "public int compareTo(Complex other)",
				"documentation": ""
			},
			{
				"signature": "public _Fields fieldForId(int fieldId)",
				"documentation": ""
			},
			{
				"signature": "public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException",
				"documentation": ""
			},
			{
				"signature": "public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			},
			{
				"signature": "public void validate() throws org.apache.thrift.TException",
				"documentation": ""
			},
			{
				"signature": "private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException",
				"documentation": ""
			},
			{
				"signature": "private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.thrift.TBase",
			"java.io.Serializable",
			"Cloneable"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.sql.hive.test._Fields",
			"org.apache.spark.sql.hive.test.Complex.ComplexStandardSchemeFactory",
			"org.apache.spark.sql.hive.test.Complex.ComplexStandardScheme",
			"org.apache.spark.sql.hive.test.Complex.ComplexTupleSchemeFactory",
			"org.apache.spark.sql.hive.test.Complex.ComplexTupleScheme"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.sql.hive.test._Fields",
			"org.apache.spark.sql.hive.test.Complex.ComplexStandardSchemeFactory",
			"org.apache.spark.sql.hive.test.Complex.ComplexStandardScheme",
			"org.apache.spark.sql.hive.test.Complex.ComplexTupleSchemeFactory",
			"org.apache.spark.sql.hive.test.Complex.ComplexTupleScheme"
		]
	},
	{
		"documentation": "/** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */",
		"name": "org.apache.spark.sql.hive.test._Fields",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */"
			},
			{
				"signature": "public static _Fields findByThriftId(int fieldId)",
				"documentation": "/**\n     * Find the _Fields constant that matches fieldId, or null if its not found.\n     */"
			},
			{
				"signature": "public static _Fields findByThriftIdOrThrow(int fieldId)",
				"documentation": "/**\n     * Find the _Fields constant that matches fieldId, throwing an exception\n     * if it is not found.\n     */"
			},
			{
				"signature": "public static _Fields findByName(String name)",
				"documentation": "/**\n     * Find the _Fields constant that matches name, or null if its not found.\n     */"
			},
			{
				"signature": "_Fields(short thriftId, String fieldName)",
				"documentation": ""
			},
			{
				"signature": "public short getThriftFieldId()",
				"documentation": ""
			},
			{
				"signature": "public String getFieldName()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.thrift.TFieldIdEnum"
		],
		"type": "enum",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.hive.test.Complex"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.test.Complex.ComplexStandardSchemeFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "public ComplexStandardScheme getScheme()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.thrift.scheme.SchemeFactory"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.hive.test.Complex"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.test.Complex.ComplexStandardScheme",
		"extends": "org.apache.thrift.scheme.StandardScheme",
		"Methods": [
			{
				"signature": "public void read(org.apache.thrift.protocol.TProtocol iprot, Complex struct) throws org.apache.thrift.TException",
				"documentation": ""
			},
			{
				"signature": "public void write(org.apache.thrift.protocol.TProtocol oprot, Complex struct) throws org.apache.thrift.TException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.hive.test.Complex"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.test.Complex.ComplexTupleSchemeFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "public ComplexTupleScheme getScheme()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.thrift.scheme.SchemeFactory"
		],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.hive.test.Complex"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.sql.hive.test.Complex.ComplexTupleScheme",
		"extends": "org.apache.thrift.scheme.TupleScheme",
		"Methods": [
			{
				"signature": "@Override\n    public void write(org.apache.thrift.protocol.TProtocol prot, Complex struct) throws org.apache.thrift.TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void read(org.apache.thrift.protocol.TProtocol prot, Complex struct) throws org.apache.thrift.TException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": true,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.sql.hive.test.Complex"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * AbstractService.\n *\n */",
		"name": "org.apache.hive.service.AbstractService",
		"extends": "",
		"Methods": [
			{
				"signature": "public AbstractService(String name)",
				"documentation": "/**\n   * Construct the service.\n   *\n   * @param name\n   *          service name\n   */"
			},
			{
				"signature": "@Override\n  public String getName()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getStartTime()",
				"documentation": ""
			},
			{
				"signature": "private void ensureCurrentState(Service.STATE currentState)",
				"documentation": "/**\n   * Verify that a service is in a given state.\n   *\n   * @param currentState\n   *          the desired state\n   * @throws IllegalStateException\n   *           if the service state is different from\n   *           the desired state\n   */"
			},
			{
				"signature": "private void changeState(Service.STATE newState)",
				"documentation": "/**\n   * Change to a new state and notify all listeners.\n   * This is a private method that is only invoked from synchronized methods,\n   * which avoid having to clone the listener list. It does imply that\n   * the state change listener methods should be short lived, as they\n   * will delay the state transition.\n   *\n   * @param newState\n   *          new service state\n   */"
			}
		],
		"interfaces": [
			"org.apache.hive.service.Service"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.hive.service.BreakableService",
			"org.apache.hive.service.CompositeService"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This is a service that can be configured to break on any of the lifecycle\n * events, so test the failure handling of other parts of the service\n * infrastructure.\n *\n * It retains a counter to the number of times each entry point is called -\n * these counters are incremented before the exceptions are raised and\n * before the superclass state methods are invoked.\n *\n */",
		"name": "org.apache.hive.service.BreakableService",
		"extends": "org.apache.hive.service.AbstractService",
		"Methods": [
			{
				"signature": "public BreakableService()",
				"documentation": "/**\n * This is a service that can be configured to break on any of the lifecycle\n * events, so test the failure handling of other parts of the service\n * infrastructure.\n *\n * It retains a counter to the number of times each entry point is called -\n * these counters are incremented before the exceptions are raised and\n * before the superclass state methods are invoked.\n *\n */"
			},
			{
				"signature": "public BreakableService(boolean failOnInit,\n                          boolean failOnStart,\n                          boolean failOnStop)",
				"documentation": ""
			},
			{
				"signature": "private int convert(STATE state)",
				"documentation": ""
			},
			{
				"signature": "private void inc(STATE state)",
				"documentation": ""
			},
			{
				"signature": "public int getCount(STATE state)",
				"documentation": ""
			},
			{
				"signature": "private void maybeFail(boolean fail, String action)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void init(HiveConf conf)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void start()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void stop()",
				"documentation": ""
			},
			{
				"signature": "public void setFailOnInit(boolean failOnInit)",
				"documentation": ""
			},
			{
				"signature": "public void setFailOnStart(boolean failOnStart)",
				"documentation": ""
			},
			{
				"signature": "public void setFailOnStop(boolean failOnStop)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.BreakableService.BrokenLifecycleEvent"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.BreakableService.BrokenLifecycleEvent"
		]
	},
	{
		"documentation": "/**\n   * The exception explicitly raised on a failure\n   */",
		"name": "org.apache.hive.service.BreakableService.BrokenLifecycleEvent",
		"extends": "RuntimeException",
		"Methods": [
			{
				"signature": "BrokenLifecycleEvent(String action)",
				"documentation": "/**\n   * The exception explicitly raised on a failure\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.BreakableService"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * CompositeService.\n *\n */",
		"name": "org.apache.hive.service.CompositeService",
		"extends": "org.apache.hive.service.AbstractService",
		"Methods": [
			{
				"signature": "public CompositeService(String name)",
				"documentation": "/**\n * CompositeService.\n *\n */"
			},
			{
				"signature": "public Collection\u003cService\u003e getServices()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.CompositeService.CompositeServiceShutdownHook"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.CompositeService.CompositeServiceShutdownHook"
		]
	},
	{
		"documentation": "/**\n   * JVM Shutdown hook for CompositeService which will stop the given\n   * CompositeService gracefully in case of JVM shutdown.\n   */",
		"name": "org.apache.hive.service.CompositeService.CompositeServiceShutdownHook",
		"extends": "",
		"Methods": [
			{
				"signature": "public CompositeServiceShutdownHook(CompositeService compositeService)",
				"documentation": "/**\n   * JVM Shutdown hook for CompositeService which will stop the given\n   * CompositeService gracefully in case of JVM shutdown.\n   */"
			},
			{
				"signature": "@Override\n    public void run()",
				"documentation": ""
			}
		],
		"interfaces": [
			"Runnable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.CompositeService"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * The cookie signer generates a signature based on SHA digest\n * and appends it to the cookie value generated at the\n * server side. It uses SHA digest algorithm to sign and verify signatures.\n */",
		"name": "org.apache.hive.service.CookieSigner",
		"extends": "",
		"Methods": [
			{
				"signature": "public CookieSigner(byte[] secret)",
				"documentation": "/**\n   * Constructor\n   * @param secret Secret Bytes\n   */"
			},
			{
				"signature": "public String signCookie(String str)",
				"documentation": "/**\n   * Sign the cookie given the string token as input.\n   * @param str Input token\n   * @return Signed token that can be used to create a cookie\n   */"
			},
			{
				"signature": "public String verifyAndExtract(String signedStr)",
				"documentation": "/**\n   * Verify a signed string and extracts the original string.\n   * @param signedStr The already signed string\n   * @return Raw Value of the string without the signature\n   */"
			},
			{
				"signature": "private String getSignature(String str)",
				"documentation": "/**\n   * Get the signature of the input string based on SHA digest algorithm.\n   * @param str Input token\n   * @return Signed String\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.thrift.ThriftHttpServlet"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * FilterService.\n *\n */",
		"name": "org.apache.hive.service.FilterService",
		"extends": "",
		"Methods": [
			{
				"signature": "public FilterService(Service service)",
				"documentation": "/**\n * FilterService.\n *\n */"
			},
			{
				"signature": "@Override\n  public void init(HiveConf config)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void start()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void stop()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void register(ServiceStateChangeListener listener)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void unregister(ServiceStateChangeListener listener)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getName()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public HiveConf getHiveConf()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Service.STATE getServiceState()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getStartTime()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.Service"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Service.\n *\n */",
		"name": "org.apache.hive.service.Service",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.hive.service.AbstractService",
			"org.apache.hive.service.FilterService"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.STATE"
		]
	},
	{
		"documentation": "/**\n   * Service states\n   */",
		"name": "org.apache.hive.service.STATE",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ServiceException.\n *\n */",
		"name": "org.apache.hive.service.ServiceException",
		"extends": "RuntimeException",
		"Methods": [
			{
				"signature": "public ServiceException(Throwable cause)",
				"documentation": "/**\n * ServiceException.\n *\n */"
			},
			{
				"signature": "public ServiceException(String message)",
				"documentation": ""
			},
			{
				"signature": "public ServiceException(String message, Throwable cause)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.CLIService",
			"org.apache.hive.service.cli.thrift.ThriftBinaryCLIService",
			"org.apache.hive.service.cli.thrift.ThriftCLIService",
			"org.apache.hive.service.cli.thrift.ThriftHttpCLIService"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ServiceOperations.\n *\n */",
		"name": "org.apache.hive.service.ServiceOperations",
		"extends": "",
		"Methods": [
			{
				"signature": "private ServiceOperations()",
				"documentation": "/**\n * ServiceOperations.\n *\n */"
			},
			{
				"signature": "public static void ensureCurrentState(Service.STATE state,\n                                        Service.STATE expectedState)",
				"documentation": "/**\n   * Verify that a service is in a given state.\n   * @param state the actual state a service is in\n   * @param expectedState the desired state\n   * @throws IllegalStateException if the service state is different from\n   * the desired state\n   */"
			},
			{
				"signature": "public static void init(Service service, HiveConf configuration)",
				"documentation": "/**\n   * Initialize a service.\n   *\n   * The service state is checked \u003ci\u003ebefore\u003c/i\u003e the operation begins.\n   * This process is \u003ci\u003enot\u003c/i\u003e thread safe.\n   * @param service a service that must be in the state\n   *   {@link Service.STATE#NOTINITED}\n   * @param configuration the configuration to initialize the service with\n   * @throws RuntimeException on a state change failure\n   * @throws IllegalStateException if the service is in the wrong state\n   */"
			},
			{
				"signature": "public static void start(Service service)",
				"documentation": "/**\n   * Start a service.\n   *\n   * The service state is checked \u003ci\u003ebefore\u003c/i\u003e the operation begins.\n   * This process is \u003ci\u003enot\u003c/i\u003e thread safe.\n   * @param service a service that must be in the state\n   *   {@link Service.STATE#INITED}\n   * @throws RuntimeException on a state change failure\n   * @throws IllegalStateException if the service is in the wrong state\n   */"
			},
			{
				"signature": "public static void deploy(Service service, HiveConf configuration)",
				"documentation": "/**\n   * Initialize then start a service.\n   *\n   * The service state is checked \u003ci\u003ebefore\u003c/i\u003e the operation begins.\n   * This process is \u003ci\u003enot\u003c/i\u003e thread safe.\n   * @param service a service that must be in the state\n   *   {@link Service.STATE#NOTINITED}\n   * @param configuration the configuration to initialize the service with\n   * @throws RuntimeException on a state change failure\n   * @throws IllegalStateException if the service is in the wrong state\n   */"
			},
			{
				"signature": "public static void stop(Service service)",
				"documentation": "/**\n   * Stop a service.\n   *\n   * Do nothing if the service is null or not in a state in which it can be/needs to be stopped.\n   *\n   * The service state is checked \u003ci\u003ebefore\u003c/i\u003e the operation begins.\n   * This process is \u003ci\u003enot\u003c/i\u003e thread safe.\n   * @param service a service or null\n   */"
			},
			{
				"signature": "public static Exception stopQuietly(Service service)",
				"documentation": "/**\n   * Stop a service; if it is null do nothing. Exceptions are caught and\n   * logged at warn level. (but not Throwables). This operation is intended to\n   * be used in cleanup operations\n   *\n   * @param service a service; may be null\n   * @return any exception that was caught; null if none was.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ServiceStateChangeListener.\n *\n */",
		"name": "org.apache.hive.service.ServiceStateChangeListener",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.ServiceUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static int indexOfDomainMatch(String userName)",
				"documentation": ""
			},
			{
				"signature": "public static void cleanup(Logger log, java.io.Closeable... closeables)",
				"documentation": "/**\n   * Close the Closeable objects and \u003cb\u003eignore\u003c/b\u003e any {@link IOException} or\n   * null pointers. Must only be used for cleanup in exception handlers.\n   *\n   * @param log the log to record problems to at debug level. Can be null.\n   * @param closeables the objects to close\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.operation.HiveCommandOperation",
			"org.apache.hive.service.cli.thrift.ThriftCLIService"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This authentication provider allows any combination of username and password.\n */",
		"name": "org.apache.hive.service.auth.AnonymousAuthenticationProviderImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public void Authenticate(String user, String password) throws AuthenticationException",
				"documentation": "/**\n * This authentication provider allows any combination of username and password.\n */"
			}
		],
		"interfaces": [
			"org.apache.hive.service.auth.PasswdAuthenticationProvider"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class helps select a {@link PasswdAuthenticationProvider} for a given {@code AuthMethod}.\n */",
		"name": "org.apache.hive.service.auth.AuthenticationProviderFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "private AuthenticationProviderFactory()",
				"documentation": ""
			},
			{
				"signature": "public static PasswdAuthenticationProvider getAuthenticationProvider(AuthMethods authMethod)\n    throws AuthenticationException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.thrift.ThriftHttpServlet"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.auth.AuthMethods"
		]
	},
	{
		"documentation": "/**\n * This class helps select a {@link PasswdAuthenticationProvider} for a given {@code AuthMethod}.\n */",
		"name": "org.apache.hive.service.auth.AuthMethods",
		"extends": "",
		"Methods": [
			{
				"signature": "AuthMethods(String authMethod)",
				"documentation": "/**\n * This class helps select a {@link PasswdAuthenticationProvider} for a given {@code AuthMethod}.\n */"
			},
			{
				"signature": "public String getAuthMethod()",
				"documentation": ""
			},
			{
				"signature": "public static AuthMethods getValidAuthMethod(String authMethodStr)\n      throws AuthenticationException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This authentication provider implements the {@code CUSTOM} authentication. It allows a {@link\n * PasswdAuthenticationProvider} to be specified at configuration time which may additionally\n * implement {@link org.apache.hadoop.conf.Configurable Configurable} to grab Hive's {@link\n * org.apache.hadoop.conf.Configuration Configuration}.\n */",
		"name": "org.apache.hive.service.auth.CustomAuthenticationProviderImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  CustomAuthenticationProviderImpl()",
				"documentation": "/**\n * This authentication provider implements the {@code CUSTOM} authentication. It allows a {@link\n * PasswdAuthenticationProvider} to be specified at configuration time which may additionally\n * implement {@link org.apache.hadoop.conf.Configurable Configurable} to grab Hive's {@link\n * org.apache.hadoop.conf.Configuration Configuration}.\n */"
			},
			{
				"signature": "@Override\n  public void Authenticate(String user, String password) throws AuthenticationException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.auth.PasswdAuthenticationProvider"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class helps in some aspects of authentication. It creates the proper Thrift classes for the\n * given configuration as well as helps with authenticating requests.\n */",
		"name": "org.apache.hive.service.auth.HiveAuthFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": ""
			},
			{
				"signature": "public HiveAuthFactory(HiveConf conf) throws TTransportException, IOException",
				"documentation": ""
			},
			{
				"signature": "public Map\u003cString, String\u003e getSaslProperties()",
				"documentation": ""
			},
			{
				"signature": "public TTransportFactory getAuthTransFactory() throws LoginException",
				"documentation": ""
			},
			{
				"signature": "public TProcessorFactory getAuthProcFactory(ThriftCLIService service) throws LoginException",
				"documentation": "/**\n   * Returns the thrift processor factory for HiveServer2 running in binary mode\n   * @param service\n   * @return\n   * @throws LoginException\n   */"
			},
			{
				"signature": "public String getRemoteUser()",
				"documentation": ""
			},
			{
				"signature": "public String getIpAddress()",
				"documentation": ""
			},
			{
				"signature": "public static void loginFromKeytab(HiveConf hiveConf) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public static UserGroupInformation loginFromSpnegoKeytabAndReturnUGI(HiveConf hiveConf)\n    throws IOException",
				"documentation": ""
			},
			{
				"signature": "public String getDelegationToken(String owner, String renewer, String remoteAddr)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public void cancelDelegationToken(String delegationToken) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public void renewDelegationToken(String delegationToken) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public String verifyDelegationToken(String delegationToken) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public String getUserFromToken(String delegationToken) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public static void verifyProxyAccess(String realUser, String proxyUser, String ipAddress,\n    HiveConf hiveConf) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public static boolean needUgiLogin(UserGroupInformation ugi, String principal, String keytab)",
				"documentation": ""
			},
			{
				"signature": "private static String getKeytabFromUgi()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException"
		],
		"usedBy": [
			"org.apache.hive.service.cli.CLIService",
			"org.apache.hive.service.cli.session.HiveSessionImpl",
			"org.apache.hive.service.cli.thrift.ThriftBinaryCLIService",
			"org.apache.hive.service.cli.thrift.ThriftCLIService",
			"org.apache.hive.service.cli.thrift.ThriftHttpCLIService"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.auth.AuthTypes"
		]
	},
	{
		"documentation": "/**\n * This class helps in some aspects of authentication. It creates the proper Thrift classes for the\n * given configuration as well as helps with authenticating requests.\n */",
		"name": "org.apache.hive.service.auth.AuthTypes",
		"extends": "",
		"Methods": [
			{
				"signature": "AuthTypes(String authType)",
				"documentation": "/**\n * This class helps in some aspects of authentication. It creates the proper Thrift classes for the\n * given configuration as well as helps with authenticating requests.\n */"
			},
			{
				"signature": "public String getAuthName()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Utility functions for HTTP mode authentication.\n */",
		"name": "org.apache.hive.service.auth.HttpAuthUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static String getKerberosServiceTicket(String principal, String host,\n      String serverHttpUrl, boolean assumeSubject) throws Exception",
				"documentation": "/**\n   * @return Stringified Base64 encoded kerberosAuthHeader on success\n   * @throws Exception\n   */"
			},
			{
				"signature": "public static String createCookieToken(String clientUserName)",
				"documentation": "/**\n   * Creates and returns a HS2 cookie token.\n   * @param clientUserName Client User name.\n   * @return An unsigned cookie token generated from input parameters.\n   * The final cookie generated is of the following format :\n   * {@code cu=\u003cusername\u003e\u0026rn=\u003crandomNumber\u003e\u0026s=\u003ccookieSignature\u003e}\n   */"
			},
			{
				"signature": "public static String getUserNameFromCookieToken(String tokenStr)",
				"documentation": "/**\n   * Parses a cookie token to retrieve client user name.\n   * @param tokenStr Token String.\n   * @return A valid user name if input is of valid format, else returns null.\n   */"
			},
			{
				"signature": "private static Map\u003cString, String\u003e splitCookieToken(String tokenStr)",
				"documentation": "/**\n   * Splits the cookie token into attributes pairs.\n   * @param str input token.\n   * @return a map with the attribute pairs of the token if the input is valid.\n   * Else, returns null.\n   */"
			},
			{
				"signature": "private HttpAuthUtils()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.auth.HttpAuthUtils.HttpKerberosClientAction"
		],
		"usedBy": [
			"org.apache.hive.service.cli.thrift.ThriftHttpServlet"
		],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.auth.HttpAuthUtils.HttpKerberosClientAction"
		]
	},
	{
		"documentation": "/**\n   * We'll create an instance of this class within a doAs block so that the client's TGT credentials\n   * can be read from the Subject\n   */",
		"name": "org.apache.hive.service.auth.HttpAuthUtils.HttpKerberosClientAction",
		"extends": "",
		"Methods": [
			{
				"signature": "public HttpKerberosClientAction(String serverPrincipal, String serverHttpUrl)",
				"documentation": "/**\n   * We'll create an instance of this class within a doAs block so that the client's TGT credentials\n   * can be read from the Subject\n   */"
			},
			{
				"signature": "@Override\n    public String run() throws Exception",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.security.PrivilegedExceptionAction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.auth.HttpAuthUtils"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License. See accompanying LICENSE file.\n */",
		"name": "org.apache.hive.service.auth.HttpAuthenticationException",
		"extends": "Exception",
		"Methods": [
			{
				"signature": "public HttpAuthenticationException(Throwable cause)",
				"documentation": "/**\n   * @param cause original exception\n   */"
			},
			{
				"signature": "public HttpAuthenticationException(String msg)",
				"documentation": "/**\n   * @param msg exception message\n   */"
			},
			{
				"signature": "public HttpAuthenticationException(String msg, Throwable cause)",
				"documentation": "/**\n   * @param msg   exception message\n   * @param cause original exception\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.thrift.ThriftHttpServlet",
			"org.apache.hive.service.cli.thrift.HttpKerberosServerAction"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.KerberosSaslHelper",
		"extends": "",
		"Methods": [
			{
				"signature": "public static TProcessorFactory getKerberosProcessorFactory(Server saslServer,\n    ThriftCLIService service)",
				"documentation": ""
			},
			{
				"signature": "public static TTransport getKerberosTransport(String principal, String host,\n    TTransport underlyingTransport, Map\u003cString, String\u003e saslProps, boolean assumeSubject)\n    throws SaslException",
				"documentation": ""
			},
			{
				"signature": "public static TTransport createSubjectAssumedTransport(String principal,\n    TTransport underlyingTransport, Map\u003cString, String\u003e saslProps) throws IOException",
				"documentation": ""
			},
			{
				"signature": "public static TTransport getTokenTransport(String tokenStr, String host,\n    TTransport underlyingTransport, Map\u003cString, String\u003e saslProps) throws SaslException",
				"documentation": ""
			},
			{
				"signature": "private KerberosSaslHelper()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.auth.KerberosSaslHelper.CLIServiceProcessorFactory"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.auth.KerberosSaslHelper.CLIServiceProcessorFactory"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.KerberosSaslHelper.CLIServiceProcessorFactory",
		"extends": "org.apache.thrift.TProcessorFactory",
		"Methods": [
			{
				"signature": "CLIServiceProcessorFactory(Server saslServer, ThriftCLIService service)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public TProcessor getProcessor(TTransport trans)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.auth.KerberosSaslHelper"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.LdapAuthenticationProviderImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "LdapAuthenticationProviderImpl()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void Authenticate(String user, String password) throws AuthenticationException",
				"documentation": ""
			},
			{
				"signature": "private boolean hasDomain(String userName)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.auth.PasswdAuthenticationProvider"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.PamAuthenticationProviderImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "PamAuthenticationProviderImpl()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void Authenticate(String user, String password) throws AuthenticationException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.auth.PasswdAuthenticationProvider"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.PasswdAuthenticationProvider",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.hive.service.auth.AnonymousAuthenticationProviderImpl",
			"org.apache.hive.service.auth.CustomAuthenticationProviderImpl",
			"org.apache.hive.service.auth.LdapAuthenticationProviderImpl",
			"org.apache.hive.service.auth.PamAuthenticationProviderImpl"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.PlainSaslHelper",
		"extends": "",
		"Methods": [
			{
				"signature": "public static TProcessorFactory getPlainProcessorFactory(ThriftCLIService service)",
				"documentation": ""
			},
			{
				"signature": "static",
				"documentation": ""
			},
			{
				"signature": "public static TTransportFactory getPlainTransportFactory(String authTypeStr)\n    throws LoginException",
				"documentation": ""
			},
			{
				"signature": "public static TTransport getPlainTransport(String username, String password,\n    TTransport underlyingTransport) throws SaslException",
				"documentation": ""
			},
			{
				"signature": "private PlainSaslHelper()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.auth.PlainSaslHelper.PlainServerCallbackHandler",
			"org.apache.hive.service.auth.PlainSaslHelper.PlainCallbackHandler",
			"org.apache.hive.service.auth.PlainSaslHelper.SQLPlainProcessorFactory",
			"org.apache.hive.service.auth.PlainSaslServer.SaslPlainProvider"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.auth.PlainSaslHelper.PlainServerCallbackHandler",
			"org.apache.hive.service.auth.PlainSaslHelper.PlainCallbackHandler",
			"org.apache.hive.service.auth.PlainSaslHelper.SQLPlainProcessorFactory"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.PlainSaslHelper.PlainServerCallbackHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "PlainServerCallbackHandler(String authMethodStr) throws AuthenticationException",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void handle(Callback[] callbacks) throws IOException, UnsupportedCallbackException",
				"documentation": ""
			}
		],
		"interfaces": [
			"javax.security.auth.callback.CallbackHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.auth.PlainSaslHelper"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.PlainSaslHelper.PlainCallbackHandler",
		"extends": "",
		"Methods": [
			{
				"signature": "public PlainCallbackHandler(String username, String password)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void handle(Callback[] callbacks) throws IOException, UnsupportedCallbackException",
				"documentation": ""
			}
		],
		"interfaces": [
			"javax.security.auth.callback.CallbackHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.auth.PlainSaslHelper"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.PlainSaslHelper.SQLPlainProcessorFactory",
		"extends": "org.apache.thrift.TProcessorFactory",
		"Methods": [
			{
				"signature": "SQLPlainProcessorFactory(ThriftCLIService service)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public TProcessor getProcessor(TTransport trans)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.auth.PlainSaslHelper"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Sun JDK only provides a PLAIN client and no server. This class implements the Plain SASL server\n * conforming to RFC #4616 (http://www.ietf.org/rfc/rfc4616.txt).\n */",
		"name": "org.apache.hive.service.auth.PlainSaslServer",
		"extends": "",
		"Methods": [
			{
				"signature": "PlainSaslServer(CallbackHandler handler, String authMethodStr) throws SaslException",
				"documentation": "/**\n * Sun JDK only provides a PLAIN client and no server. This class implements the Plain SASL server\n * conforming to RFC #4616 (http://www.ietf.org/rfc/rfc4616.txt).\n */"
			},
			{
				"signature": "@Override\n  public String getMechanismName()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] evaluateResponse(byte[] response) throws SaslException",
				"documentation": ""
			},
			{
				"signature": "Callback[] cbList =",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isComplete()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getAuthorizationID()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] unwrap(byte[] incoming, int offset, int len)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public byte[] wrap(byte[] outgoing, int offset, int len)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object getNegotiatedProperty(String propName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void dispose()",
				"documentation": ""
			}
		],
		"interfaces": [
			"javax.security.sasl.SaslServer"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.auth.PlainSaslServer.SaslPlainProvider"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.auth.PlainSaslServer.SaslPlainServerFactory",
			"org.apache.hive.service.auth.PlainSaslServer.SaslPlainProvider"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.PlainSaslServer.SaslPlainServerFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public SaslServer createSaslServer(String mechanism, String protocol, String serverName,\n      Map\u003cString, ?\u003e props, CallbackHandler cbh)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String[] getMechanismNames(Map\u003cString, ?\u003e props)",
				"documentation": ""
			}
		],
		"interfaces": [
			"javax.security.sasl.SaslServerFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.auth.PlainSaslServer.SaslPlainProvider",
		"extends": "java.security.Provider",
		"Methods": [
			{
				"signature": "public SaslPlainProvider()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.auth.PlainSaslHelper",
			"org.apache.hive.service.auth.PlainSaslServer"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Possible values of SASL quality-of-protection value.\n */",
		"name": "org.apache.hive.service.auth.SaslQOP",
		"extends": "",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * Possible values of SASL quality-of-protection value.\n */"
			},
			{
				"signature": "SaslQOP(String saslQop)",
				"documentation": ""
			},
			{
				"signature": "public String toString()",
				"documentation": ""
			},
			{
				"signature": "public static SaslQOP fromString(String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class is responsible for setting the ipAddress for operations executed via HiveServer2.\n *\n * - IP address is only set for operations that calls listeners with hookContext\n * - IP address is only set if the underlying transport mechanism is socket\n *\n * @see org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext\n */",
		"name": "org.apache.hive.service.auth.TSetIpAddressProcessor",
		"extends": "org.apache.hive.service.rpc.thrift.TCLIService.Processor",
		"Methods": [
			{
				"signature": "public TSetIpAddressProcessor(Iface iface)",
				"documentation": "/**\n * This class is responsible for setting the ipAddress for operations executed via HiveServer2.\n *\n * - IP address is only set for operations that calls listeners with hookContext\n * - IP address is only set if the underlying transport mechanism is socket\n *\n * @see org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext\n */"
			},
			{
				"signature": "@Override\n  public boolean process(final TProtocol in, final TProtocol out) throws TException",
				"documentation": ""
			},
			{
				"signature": "private void setUserName(final TProtocol in)",
				"documentation": ""
			},
			{
				"signature": "protected void setIpAddress(final TProtocol in)",
				"documentation": ""
			},
			{
				"signature": "private TSocket getUnderlyingSocketFromTransport(TTransport transport)",
				"documentation": ""
			},
			{
				"signature": "public static String getUserIpAddress()",
				"documentation": ""
			},
			{
				"signature": "public static String getUserName()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.thrift.ThriftCLIService"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This is used on the client side, where the API explicitly opens a transport to\n * the server using the Subject.doAs().\n */",
		"name": "org.apache.hive.service.auth.TSubjectAssumingTransport",
		"extends": "org.apache.hadoop.hive.thrift.TFilterTransport",
		"Methods": [
			{
				"signature": "public TSubjectAssumingTransport(TTransport wrapped)",
				"documentation": "/**\n * This is used on the client side, where the API explicitly opens a transport to\n * the server using the Subject.doAs().\n */"
			},
			{
				"signature": "@Override\n  public void open() throws TTransportException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * CLIService.\n *\n */",
		"name": "org.apache.hive.service.cli.CLIService",
		"extends": "org.apache.hive.service.CompositeService",
		"Methods": [
			{
				"signature": "static",
				"documentation": "/**\n * CLIService.\n *\n */"
			},
			{
				"signature": "public CLIService(HiveServer2 hiveServer2)",
				"documentation": ""
			},
			{
				"signature": "private void applyAuthorizationConfigPolicy(HiveConf newHiveConf) throws HiveException,\n      MetaException",
				"documentation": ""
			},
			{
				"signature": "private void setupBlockedUdfs()",
				"documentation": ""
			},
			{
				"signature": "public UserGroupInformation getServiceUGI()",
				"documentation": ""
			},
			{
				"signature": "public UserGroupInformation getHttpUGI()",
				"documentation": ""
			},
			{
				"signature": "@Deprecated\n  public SessionHandle openSession(TProtocolVersion protocol, String username, String password,\n      Map\u003cString, String\u003e configuration) throws HiveSQLException",
				"documentation": "/**\n   * @deprecated  Use {@link #openSession(TProtocolVersion, String, String, String, Map)}\n   */"
			},
			{
				"signature": "@Deprecated\n  public SessionHandle openSessionWithImpersonation(TProtocolVersion protocol, String username,\n      String password, Map\u003cString, String\u003e configuration, String delegationToken)\n          throws HiveSQLException",
				"documentation": "/**\n   * @deprecated  Use {@link #openSessionWithImpersonation(TProtocolVersion, String, String, String, Map, String)}\n   */"
			},
			{
				"signature": "public SessionHandle openSession(TProtocolVersion protocol, String username, String password, String ipAddress,\n      Map\u003cString, String\u003e configuration) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public SessionHandle openSessionWithImpersonation(TProtocolVersion protocol, String username,\n      String password, String ipAddress, Map\u003cString, String\u003e configuration, String delegationToken)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SessionHandle openSession(String username, String password, Map\u003cString, String\u003e configuration)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SessionHandle openSessionWithImpersonation(String username, String password, Map\u003cString, String\u003e configuration,\n      String delegationToken) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void closeSession(SessionHandle sessionHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public GetInfoValue getInfo(SessionHandle sessionHandle, GetInfoType getInfoType)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatement(SessionHandle sessionHandle, String statement,\n      Map\u003cString, String\u003e confOverlay) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatement(SessionHandle sessionHandle, String statement,\n        Map\u003cString, String\u003e confOverlay, long queryTimeout) throws HiveSQLException",
				"documentation": "/**\n   * Execute statement on the server with a timeout. This is a blocking call.\n   */"
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatementAsync(SessionHandle sessionHandle, String statement,\n      Map\u003cString, String\u003e confOverlay) throws HiveSQLException",
				"documentation": "/**\n   * Execute statement asynchronously on the server. This is a non-blocking call\n   */"
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatementAsync(SessionHandle sessionHandle, String statement,\n      Map\u003cString, String\u003e confOverlay, long queryTimeout) throws HiveSQLException",
				"documentation": "/**\n   * Execute statement asynchronously on the server with a timeout. This is a non-blocking call\n   */"
			},
			{
				"signature": "@Override\n  public OperationHandle getTypeInfo(SessionHandle sessionHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getCatalogs(SessionHandle sessionHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getSchemas(SessionHandle sessionHandle,\n      String catalogName, String schemaName)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getTables(SessionHandle sessionHandle,\n      String catalogName, String schemaName, String tableName, List\u003cString\u003e tableTypes)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getTableTypes(SessionHandle sessionHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getColumns(SessionHandle sessionHandle,\n      String catalogName, String schemaName, String tableName, String columnName)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getFunctions(SessionHandle sessionHandle,\n      String catalogName, String schemaName, String functionName)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getPrimaryKeys(SessionHandle sessionHandle,\n      String catalog, String schema, String table) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getCrossReference(SessionHandle sessionHandle,\n      String primaryCatalog, String primarySchema, String primaryTable, String foreignCatalog,\n      String foreignSchema, String foreignTable) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationStatus getOperationStatus(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public HiveConf getSessionConf(SessionHandle sessionHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cancelOperation(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void closeOperation(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetMetadata(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet fetchResults(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet fetchResults(OperationHandle opHandle, FetchOrientation orientation,\n                             long maxRows, FetchType fetchType) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory,\n      String owner, String renewer) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cancelDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory,\n      String tokenStr) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void renewDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory,\n      String tokenStr) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getQueryId(TOperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public SessionManager getSessionManager()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.cli.ICLIService"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.ServiceException",
			"org.apache.hive.service.auth.HiveAuthFactory",
			"org.apache.hive.service.cli.session.SessionManager"
		],
		"usedBy": [
			"org.apache.hive.service.server.HiveServer2"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * CLIServiceClient.\n *\n */",
		"name": "org.apache.hive.service.cli.CLIServiceClient",
		"extends": "",
		"Methods": [
			{
				"signature": "public SessionHandle openSession(String username, String password)\n      throws HiveSQLException",
				"documentation": "/**\n * CLIServiceClient.\n *\n */"
			},
			{
				"signature": "@Override\n  public RowSet fetchResults(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.cli.ICLIService"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * CLIServiceUtils.\n *\n */",
		"name": "org.apache.hive.service.cli.CLIServiceUtils",
		"extends": "",
		"Methods": [
			{
				"signature": "public static String patternToRegex(String pattern)",
				"documentation": "/**\n   * Convert a SQL search pattern into an equivalent Java Regex.\n   *\n   * @param pattern input which may contain '%' or '_' wildcard characters, or\n   * these characters escaped using {@code getSearchStringEscape()}.\n   * @return replace %/_ with regex search characters, also handle escaped\n   * characters.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ColumnBasedSet.\n */",
		"name": "org.apache.hive.service.cli.ColumnBasedSet",
		"extends": "",
		"Methods": [
			{
				"signature": "public ColumnBasedSet(TableSchema schema)",
				"documentation": "/**\n * ColumnBasedSet.\n */"
			},
			{
				"signature": "public ColumnBasedSet(TRowSet tRowSet) throws TException",
				"documentation": ""
			},
			{
				"signature": "private ColumnBasedSet(TypeDescriptor[] descriptors, List\u003cColumnBuffer\u003e columns, long startOffset)",
				"documentation": ""
			},
			{
				"signature": "public ColumnBasedSet(TableSchema schema, boolean isBlobBased)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnBasedSet addRow(Object[] fields)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cColumnBuffer\u003e getColumns()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numColumns()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numRows()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ColumnBasedSet extractSubset(int maxRows)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getStartOffset()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setStartOffset(long startOffset)",
				"documentation": ""
			},
			{
				"signature": "public TRowSet toTRowSet()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Iterator\u003cObject[]\u003e iterator()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public Object[] next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void remove()",
				"documentation": ""
			},
			{
				"signature": "public Object[] fill(int index, Object[] convey)",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.cli.RowSet"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ColumnDescriptor.\n *\n */",
		"name": "org.apache.hive.service.cli.ColumnDescriptor",
		"extends": "",
		"Methods": [
			{
				"signature": "public ColumnDescriptor(String name, String comment, TypeDescriptor type, int position)",
				"documentation": "/**\n * ColumnDescriptor.\n *\n */"
			},
			{
				"signature": "public ColumnDescriptor(TColumnDesc tColumnDesc)",
				"documentation": ""
			},
			{
				"signature": "public static ColumnDescriptor newPrimitiveColumnDescriptor(String name, String comment, Type type, int position)",
				"documentation": ""
			},
			{
				"signature": "public String getName()",
				"documentation": ""
			},
			{
				"signature": "public String getComment()",
				"documentation": ""
			},
			{
				"signature": "public TypeDescriptor getTypeDescriptor()",
				"documentation": ""
			},
			{
				"signature": "public int getOrdinalPosition()",
				"documentation": ""
			},
			{
				"signature": "public TColumnDesc toTColumnDesc()",
				"documentation": ""
			},
			{
				"signature": "public Type getType()",
				"documentation": ""
			},
			{
				"signature": "public boolean isPrimitive()",
				"documentation": ""
			},
			{
				"signature": "public String getTypeName()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Protocols before HIVE_CLI_SERVICE_PROTOCOL_V6 (used by RowBasedSet)\n *\n */",
		"name": "org.apache.hive.service.cli.ColumnValue",
		"extends": "",
		"Methods": [
			{
				"signature": "private static TColumnValue booleanValue(Boolean value)",
				"documentation": "/**\n * Protocols before HIVE_CLI_SERVICE_PROTOCOL_V6 (used by RowBasedSet)\n *\n */"
			},
			{
				"signature": "private static TColumnValue byteValue(Byte value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue shortValue(Short value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue intValue(Integer value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue longValue(Long value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue floatValue(Float value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue doubleValue(Double value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue stringValue(String value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue stringValue(HiveChar value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue stringValue(HiveVarchar value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue stringValue(HiveIntervalYearMonth value)",
				"documentation": ""
			},
			{
				"signature": "private static TColumnValue stringValue(HiveIntervalDayTime value)",
				"documentation": ""
			},
			{
				"signature": "public static TColumnValue toTColumnValue(TypeDescriptor typeDescriptor, Object value)",
				"documentation": ""
			},
			{
				"signature": "private static Boolean getBooleanValue(TBoolValue tBoolValue)",
				"documentation": ""
			},
			{
				"signature": "private static Byte getByteValue(TByteValue tByteValue)",
				"documentation": ""
			},
			{
				"signature": "private static Short getShortValue(TI16Value tI16Value)",
				"documentation": ""
			},
			{
				"signature": "private static Integer getIntegerValue(TI32Value tI32Value)",
				"documentation": ""
			},
			{
				"signature": "private static Long getLongValue(TI64Value tI64Value)",
				"documentation": ""
			},
			{
				"signature": "private static Double getDoubleValue(TDoubleValue tDoubleValue)",
				"documentation": ""
			},
			{
				"signature": "private static String getStringValue(TStringValue tStringValue)",
				"documentation": ""
			},
			{
				"signature": "private static Date getDateValue(TStringValue tStringValue)",
				"documentation": ""
			},
			{
				"signature": "private static Timestamp getTimestampValue(TStringValue tStringValue)",
				"documentation": ""
			},
			{
				"signature": "private static byte[] getBinaryValue(TStringValue tString)",
				"documentation": ""
			},
			{
				"signature": "private static BigDecimal getBigDecimalValue(TStringValue tStringValue)",
				"documentation": ""
			},
			{
				"signature": "public static Object toColumnValue(TColumnValue value)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.unsafe.types.UTF8String"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * FetchOrientation.\n *\n */",
		"name": "org.apache.hive.service.cli.FetchOrientation",
		"extends": "",
		"Methods": [
			{
				"signature": "FetchOrientation(TFetchOrientation tFetchOrientation)",
				"documentation": "/**\n * FetchOrientation.\n *\n */"
			},
			{
				"signature": "public static FetchOrientation getFetchOrientation(TFetchOrientation tFetchOrientation)",
				"documentation": ""
			},
			{
				"signature": "public TFetchOrientation toTFetchOrientation()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * FetchType indicates the type of fetchResults request.\n * It maps the TFetchType, which is generated from Thrift interface.\n */",
		"name": "org.apache.hive.service.cli.FetchType",
		"extends": "",
		"Methods": [
			{
				"signature": "FetchType(short tFetchType)",
				"documentation": "/**\n * FetchType indicates the type of fetchResults request.\n * It maps the TFetchType, which is generated from Thrift interface.\n */"
			},
			{
				"signature": "public static FetchType getFetchType(short tFetchType)",
				"documentation": ""
			},
			{
				"signature": "public short toTFetchType()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetInfoType.\n *\n */",
		"name": "org.apache.hive.service.cli.GetInfoType",
		"extends": "",
		"Methods": [
			{
				"signature": "GetInfoType(TGetInfoType tInfoType)",
				"documentation": "/**\n * GetInfoType.\n *\n */"
			},
			{
				"signature": "public static GetInfoType getGetInfoType(TGetInfoType tGetInfoType)",
				"documentation": ""
			},
			{
				"signature": "public TGetInfoType toTGetInfoType()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetInfoValue.\n *\n */",
		"name": "org.apache.hive.service.cli.GetInfoValue",
		"extends": "",
		"Methods": [
			{
				"signature": "public GetInfoValue(String stringValue)",
				"documentation": "/**\n * GetInfoValue.\n *\n */"
			},
			{
				"signature": "public GetInfoValue(short shortValue)",
				"documentation": ""
			},
			{
				"signature": "public GetInfoValue(int intValue)",
				"documentation": ""
			},
			{
				"signature": "public GetInfoValue(long longValue)",
				"documentation": ""
			},
			{
				"signature": "public GetInfoValue(TGetInfoValue tGetInfoValue)",
				"documentation": ""
			},
			{
				"signature": "public TGetInfoValue toTGetInfoValue()",
				"documentation": ""
			},
			{
				"signature": "public String getStringValue()",
				"documentation": ""
			},
			{
				"signature": "public short getShortValue()",
				"documentation": ""
			},
			{
				"signature": "public int getIntValue()",
				"documentation": ""
			},
			{
				"signature": "public long getLongValue()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.session.HiveSessionImpl"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.Handle",
		"extends": "",
		"Methods": [
			{
				"signature": "public Handle()",
				"documentation": ""
			},
			{
				"signature": "public Handle(HandleIdentifier handleId)",
				"documentation": ""
			},
			{
				"signature": "public Handle(THandleIdentifier tHandleIdentifier)",
				"documentation": ""
			},
			{
				"signature": "public HandleIdentifier getHandleIdentifier()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object obj)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.hive.service.cli.OperationHandle",
			"org.apache.hive.service.cli.SessionHandle"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * HandleIdentifier.\n *\n */",
		"name": "org.apache.hive.service.cli.HandleIdentifier",
		"extends": "",
		"Methods": [
			{
				"signature": "public HandleIdentifier()",
				"documentation": "/**\n * HandleIdentifier.\n *\n */"
			},
			{
				"signature": "public HandleIdentifier(UUID publicId, UUID secretId)",
				"documentation": ""
			},
			{
				"signature": "public HandleIdentifier(THandleIdentifier tHandleId)",
				"documentation": ""
			},
			{
				"signature": "public UUID getPublicId()",
				"documentation": ""
			},
			{
				"signature": "public UUID getSecretId()",
				"documentation": ""
			},
			{
				"signature": "public THandleIdentifier toTHandleIdentifier()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object obj)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * HiveSQLException.\n *\n */",
		"name": "org.apache.hive.service.cli.HiveSQLException",
		"extends": "java.sql.SQLException",
		"Methods": [
			{
				"signature": "public HiveSQLException()",
				"documentation": "/**\n   *\n   */"
			},
			{
				"signature": "public HiveSQLException(String reason)",
				"documentation": "/**\n   * @param reason\n   */"
			},
			{
				"signature": "public HiveSQLException(Throwable cause)",
				"documentation": "/**\n   * @param cause\n   */"
			},
			{
				"signature": "public HiveSQLException(String reason, String sqlState)",
				"documentation": "/**\n   * @param reason\n   * @param sqlState\n   */"
			},
			{
				"signature": "public HiveSQLException(String reason, Throwable cause)",
				"documentation": "/**\n   * @param reason\n   * @param cause\n   */"
			},
			{
				"signature": "public HiveSQLException(String reason, String sqlState, int vendorCode)",
				"documentation": "/**\n   * @param reason\n   * @param sqlState\n   * @param vendorCode\n   */"
			},
			{
				"signature": "public HiveSQLException(String reason, String sqlState, Throwable cause)",
				"documentation": "/**\n   * @param reason\n   * @param sqlState\n   * @param cause\n   */"
			},
			{
				"signature": "public HiveSQLException(String reason, String sqlState, int vendorCode, Throwable cause)",
				"documentation": "/**\n   * @param reason\n   * @param sqlState\n   * @param vendorCode\n   * @param cause\n   */"
			},
			{
				"signature": "public HiveSQLException(TStatus status)",
				"documentation": ""
			},
			{
				"signature": "public TStatus toTStatus()",
				"documentation": "/**\n   * Converts current object to a {@link TStatus} object\n   * @return a {@link TStatus} object\n   */"
			},
			{
				"signature": "public static TStatus toTStatus(Exception e)",
				"documentation": "/**\n   * Converts the specified {@link Exception} object into a {@link TStatus} object\n   * @param e a {@link Exception} object\n   * @return a {@link TStatus} object\n   */"
			},
			{
				"signature": "public static List\u003cString\u003e toString(Throwable ex)",
				"documentation": "/**\n   * Converts a {@link Throwable} object into a flattened list of texts including its stack trace\n   * and the stack traces of the nested causes.\n   * @param ex  a {@link Throwable} object\n   * @return    a flattened list of texts including the {@link Throwable} object's stack trace\n   *            and the stack traces of the nested causes.\n   */"
			},
			{
				"signature": "private static List\u003cString\u003e toString(Throwable cause, StackTraceElement[] parent)",
				"documentation": ""
			},
			{
				"signature": "private static List\u003cString\u003e enroll(Throwable ex, StackTraceElement[] trace, int max)",
				"documentation": ""
			},
			{
				"signature": "public static Throwable toCause(List\u003cString\u003e details)",
				"documentation": "/**\n   * Converts a flattened list of texts including the stack trace and the stack\n   * traces of the nested causes into a {@link Throwable} object.\n   * @param details a flattened list of texts including the stack trace and the stack\n   *                traces of the nested causes\n   * @return        a {@link Throwable} object\n   */"
			},
			{
				"signature": "private static Throwable toStackTrace(List\u003cString\u003e details, StackTraceElement[] parent, int index)",
				"documentation": ""
			},
			{
				"signature": "private static Throwable newInstance(String className, String message)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.auth.HiveAuthFactory",
			"org.apache.hive.service.cli.operation.ExecuteStatementOperation",
			"org.apache.hive.service.cli.operation.GetColumnsOperation",
			"org.apache.hive.service.cli.operation.GetFunctionsOperation",
			"org.apache.hive.service.cli.operation.GetSchemasOperation",
			"org.apache.hive.service.cli.operation.GetTableTypesOperation",
			"org.apache.hive.service.cli.operation.GetTablesOperation",
			"org.apache.hive.service.cli.operation.GetTypeInfoOperation",
			"org.apache.hive.service.cli.operation.HiveCommandOperation",
			"org.apache.hive.service.cli.operation.MetadataOperation",
			"org.apache.hive.service.cli.operation.Operation",
			"org.apache.hive.service.cli.operation.OperationManager",
			"org.apache.hive.service.cli.operation.SQLOperation",
			"org.apache.hive.service.cli.session.HiveSessionImpl",
			"org.apache.hive.service.cli.session.HiveSessionImplwithUGI",
			"org.apache.hive.service.cli.session.SessionManager"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.ICLIService",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.hive.service.cli.CLIService",
			"org.apache.hive.service.cli.CLIServiceClient"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.OperationHandle",
		"extends": "org.apache.hive.service.cli.Handle",
		"Methods": [
			{
				"signature": "public OperationHandle(OperationType opType, TProtocolVersion protocol)",
				"documentation": ""
			},
			{
				"signature": "public OperationHandle(TOperationHandle tOperationHandle)",
				"documentation": ""
			},
			{
				"signature": "public OperationHandle(TOperationHandle tOperationHandle, TProtocolVersion protocol)",
				"documentation": ""
			},
			{
				"signature": "public OperationType getOperationType()",
				"documentation": ""
			},
			{
				"signature": "public void setHasResultSet(boolean hasResultSet)",
				"documentation": ""
			},
			{
				"signature": "public boolean hasResultSet()",
				"documentation": ""
			},
			{
				"signature": "public TOperationHandle toTOperationHandle()",
				"documentation": ""
			},
			{
				"signature": "public TProtocolVersion getProtocolVersion()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int hashCode()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean equals(Object obj)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.operation.Operation"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * OperationState.\n *\n */",
		"name": "org.apache.hive.service.cli.OperationState",
		"extends": "",
		"Methods": [
			{
				"signature": "OperationState(TOperationState tOperationState, boolean terminal)",
				"documentation": "/**\n * OperationState.\n *\n */"
			},
			{
				"signature": "public static OperationState getOperationState(TOperationState tOperationState)",
				"documentation": ""
			},
			{
				"signature": "public static void validateTransition(OperationState oldState,\n      OperationState newState)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public void validateTransition(OperationState newState)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public TOperationState toTOperationState()",
				"documentation": ""
			},
			{
				"signature": "public boolean isTerminal()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * OperationStatus\n *\n */",
		"name": "org.apache.hive.service.cli.OperationStatus",
		"extends": "",
		"Methods": [
			{
				"signature": "public OperationStatus(OperationState state, HiveSQLException operationException)",
				"documentation": "/**\n * OperationStatus\n *\n */"
			},
			{
				"signature": "public OperationState getState()",
				"documentation": ""
			},
			{
				"signature": "public HiveSQLException getOperationException()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.operation.Operation"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * OperationType.\n *\n */",
		"name": "org.apache.hive.service.cli.OperationType",
		"extends": "",
		"Methods": [
			{
				"signature": "OperationType(TOperationType tOpType)",
				"documentation": "/**\n * OperationType.\n *\n */"
			},
			{
				"signature": "public static OperationType getOperationType(TOperationType tOperationType)",
				"documentation": ""
			},
			{
				"signature": "public TOperationType toTOperationType()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * RowBasedSet\n */",
		"name": "org.apache.hive.service.cli.RowBasedSet",
		"extends": "",
		"Methods": [
			{
				"signature": "public RowBasedSet(TableSchema schema)",
				"documentation": "/**\n * RowBasedSet\n */"
			},
			{
				"signature": "public RowBasedSet(TRowSet tRowSet)",
				"documentation": ""
			},
			{
				"signature": "private RowBasedSet(TypeDescriptor[] descriptors, List\u003cTRow\u003e rows, long startOffset)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowBasedSet addRow(Object[] fields)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numColumns()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public int numRows()",
				"documentation": ""
			},
			{
				"signature": "public RowBasedSet extractSubset(int maxRows)",
				"documentation": ""
			},
			{
				"signature": "public long getStartOffset()",
				"documentation": ""
			},
			{
				"signature": "public void setStartOffset(long startOffset)",
				"documentation": ""
			},
			{
				"signature": "public int getSize()",
				"documentation": ""
			},
			{
				"signature": "public TRowSet toTRowSet()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Iterator\u003cObject[]\u003e iterator()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public boolean hasNext()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public Object[] next()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void remove()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.cli.RowSet"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.RowBasedSet.RemovableList"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.cli.RowBasedSet.RemovableList"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.RowBasedSet.RemovableList",
		"extends": "java.util.ArrayList",
		"Methods": [
			{
				"signature": "RemovableList()",
				"documentation": ""
			},
			{
				"signature": "RemovableList(List\u003cE\u003e rows)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void removeRange(int fromIndex, int toIndex)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.RowBasedSet"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.RowSet",
		"extends": "Iterable",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.hive.service.cli.ColumnBasedSet",
			"org.apache.hive.service.cli.RowBasedSet"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.RowSetFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "public static RowSet create(TableSchema schema, TProtocolVersion version, boolean isBlobBased)",
				"documentation": ""
			},
			{
				"signature": "public static RowSet create(TRowSet results, TProtocolVersion version) throws TException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.operation.GetCatalogsOperation",
			"org.apache.hive.service.cli.operation.GetColumnsOperation",
			"org.apache.hive.service.cli.operation.GetFunctionsOperation",
			"org.apache.hive.service.cli.operation.GetSchemasOperation",
			"org.apache.hive.service.cli.operation.GetTableTypesOperation",
			"org.apache.hive.service.cli.operation.GetTablesOperation",
			"org.apache.hive.service.cli.operation.GetTypeInfoOperation",
			"org.apache.hive.service.cli.operation.HiveCommandOperation",
			"org.apache.hive.service.cli.operation.OperationManager",
			"org.apache.hive.service.cli.operation.SQLOperation"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * SessionHandle.\n *\n */",
		"name": "org.apache.hive.service.cli.SessionHandle",
		"extends": "org.apache.hive.service.cli.Handle",
		"Methods": [
			{
				"signature": "public SessionHandle(TProtocolVersion protocol)",
				"documentation": "/**\n * SessionHandle.\n *\n */"
			},
			{
				"signature": "public SessionHandle(TSessionHandle tSessionHandle)",
				"documentation": ""
			},
			{
				"signature": "public SessionHandle(TSessionHandle tSessionHandle, TProtocolVersion protocol)",
				"documentation": ""
			},
			{
				"signature": "public SessionHandle(HandleIdentifier handleId, TProtocolVersion protocol)",
				"documentation": ""
			},
			{
				"signature": "public UUID getSessionId()",
				"documentation": ""
			},
			{
				"signature": "public TSessionHandle toTSessionHandle()",
				"documentation": ""
			},
			{
				"signature": "public TProtocolVersion getProtocolVersion()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String toString()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.session.HiveSessionImpl"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * TableSchema.\n *\n */",
		"name": "org.apache.hive.service.cli.TableSchema",
		"extends": "",
		"Methods": [
			{
				"signature": "public TableSchema()",
				"documentation": "/**\n * TableSchema.\n *\n */"
			},
			{
				"signature": "public TableSchema(int numColumns)",
				"documentation": ""
			},
			{
				"signature": "public TableSchema(TTableSchema tTableSchema)",
				"documentation": ""
			},
			{
				"signature": "public TableSchema(List\u003cFieldSchema\u003e fieldSchemas)",
				"documentation": ""
			},
			{
				"signature": "public TableSchema(Schema schema)",
				"documentation": ""
			},
			{
				"signature": "public List\u003cColumnDescriptor\u003e getColumnDescriptors()",
				"documentation": ""
			},
			{
				"signature": "public ColumnDescriptor getColumnDescriptorAt(int pos)",
				"documentation": ""
			},
			{
				"signature": "public int getSize()",
				"documentation": ""
			},
			{
				"signature": "public void clear()",
				"documentation": ""
			},
			{
				"signature": "public TTableSchema toTTableSchema()",
				"documentation": ""
			},
			{
				"signature": "public TypeDescriptor[] toTypeDescriptors()",
				"documentation": ""
			},
			{
				"signature": "public TableSchema addPrimitiveColumn(String columnName, Type columnType, String columnComment)",
				"documentation": ""
			},
			{
				"signature": "public TableSchema addStringColumn(String columnName, String columnComment)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.operation.GetCatalogsOperation",
			"org.apache.hive.service.cli.operation.GetColumnsOperation",
			"org.apache.hive.service.cli.operation.GetFunctionsOperation",
			"org.apache.hive.service.cli.operation.GetSchemasOperation",
			"org.apache.hive.service.cli.operation.GetTableTypesOperation",
			"org.apache.hive.service.cli.operation.GetTablesOperation",
			"org.apache.hive.service.cli.operation.GetTypeInfoOperation",
			"org.apache.hive.service.cli.operation.HiveCommandOperation",
			"org.apache.hive.service.cli.operation.OperationManager",
			"org.apache.hive.service.cli.operation.SQLOperation"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * TypeDescriptor.\n *\n */",
		"name": "org.apache.hive.service.cli.TypeDescriptor",
		"extends": "",
		"Methods": [
			{
				"signature": "public TypeDescriptor(Type type)",
				"documentation": "/**\n * TypeDescriptor.\n *\n */"
			},
			{
				"signature": "public TypeDescriptor(TTypeDesc tTypeDesc)",
				"documentation": ""
			},
			{
				"signature": "public TypeDescriptor(String typeName)",
				"documentation": ""
			},
			{
				"signature": "public Type getType()",
				"documentation": ""
			},
			{
				"signature": "public TTypeDesc toTTypeDesc()",
				"documentation": ""
			},
			{
				"signature": "public String getTypeName()",
				"documentation": ""
			},
			{
				"signature": "public TypeQualifiers getTypeQualifiers()",
				"documentation": ""
			},
			{
				"signature": "public void setTypeQualifiers(TypeQualifiers typeQualifiers)",
				"documentation": ""
			},
			{
				"signature": "public Integer getColumnSize()",
				"documentation": "/**\n   * The column size for this type.\n   * For numeric data this is the maximum precision.\n   * For character data this is the length in characters.\n   * For datetime types this is the length in characters of the String representation\n   * (assuming the maximum allowed precision of the fractional seconds component).\n   * For binary data this is the length in bytes.\n   * Null is returned for data types where the column size is not applicable.\n   */"
			},
			{
				"signature": "public Integer getPrecision()",
				"documentation": "/**\n   * Maximum precision for numeric types.\n   * Returns null for non-numeric types.\n   * @return\n   */"
			},
			{
				"signature": "public Integer getDecimalDigits()",
				"documentation": "/**\n   * The number of fractional digits for this type.\n   * Null is returned for data types where this is not applicable.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * This class holds type qualifier information for a primitive type,\n * such as char/varchar length or decimal precision/scale.\n */",
		"name": "org.apache.hive.service.cli.TypeQualifiers",
		"extends": "",
		"Methods": [
			{
				"signature": "public TypeQualifiers()",
				"documentation": "/**\n * This class holds type qualifier information for a primitive type,\n * such as char/varchar length or decimal precision/scale.\n */"
			},
			{
				"signature": "public Integer getCharacterMaximumLength()",
				"documentation": ""
			},
			{
				"signature": "public void setCharacterMaximumLength(int characterMaximumLength)",
				"documentation": ""
			},
			{
				"signature": "public TTypeQualifiers toTTypeQualifiers()",
				"documentation": ""
			},
			{
				"signature": "public static TypeQualifiers fromTTypeQualifiers(TTypeQualifiers ttq)",
				"documentation": ""
			},
			{
				"signature": "public static TypeQualifiers fromTypeInfo(PrimitiveTypeInfo pti)",
				"documentation": ""
			},
			{
				"signature": "public Integer getPrecision()",
				"documentation": ""
			},
			{
				"signature": "public void setPrecision(Integer precision)",
				"documentation": ""
			},
			{
				"signature": "public Integer getScale()",
				"documentation": ""
			},
			{
				"signature": "public void setScale(Integer scale)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ClassicTableTypeMapping.\n * Classic table type mapping :\n *  Managed Table to Table\n *  External Table to Table\n *  Virtual View to View\n */",
		"name": "org.apache.hive.service.cli.operation.ClassicTableTypeMapping",
		"extends": "",
		"Methods": [
			{
				"signature": "public ClassicTableTypeMapping()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String[] mapToHiveType(String clientTypeName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String mapToClientType(String hiveTypeName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Set\u003cString\u003e getTableTypeNames()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.cli.operation.TableTypeMapping"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.cli.operation.ClassicTableTypes"
		]
	},
	{
		"documentation": "/**\n * ClassicTableTypeMapping.\n * Classic table type mapping :\n *  Managed Table to Table\n *  External Table to Table\n *  Virtual View to View\n */",
		"name": "org.apache.hive.service.cli.operation.ClassicTableTypes",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.operation.ExecuteStatementOperation",
		"extends": "org.apache.hive.service.cli.operation.Operation",
		"Methods": [
			{
				"signature": "public ExecuteStatementOperation(HiveSession parentSession, String statement,\n      Map\u003cString, String\u003e confOverlay, boolean runInBackground)",
				"documentation": ""
			},
			{
				"signature": "public String getStatement()",
				"documentation": ""
			},
			{
				"signature": "public static ExecuteStatementOperation newExecuteStatementOperation(HiveSession parentSession,\n     String statement, Map\u003cString, String\u003e confOverlay, boolean runAsync, long queryTimeout)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "protected void registerCurrentOperationLog()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.hive.service.cli.operation.HiveCommandOperation",
			"org.apache.hive.service.cli.operation.SQLOperation"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetCatalogsOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetCatalogsOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "protected GetCatalogsOperation(HiveSession parentSession)",
				"documentation": "/**\n * GetCatalogsOperation.\n *\n */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetColumnsOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetColumnsOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "protected GetColumnsOperation(HiveSession parentSession, String catalogName, String schemaName,\n      String tableName, String columnName)",
				"documentation": "/**\n * GetColumnsOperation.\n *\n */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private List\u003cHivePrivilegeObject\u003e getPrivObjs(Map\u003cString, List\u003cString\u003e\u003e db2Tabs)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetCrossReferenceOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetCrossReferenceOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "public GetCrossReferenceOperation(HiveSession parentSession,\n                                    String parentCatalogName, String parentSchemaName, String parentTableName,\n                                    String foreignCatalog, String foreignSchema, String foreignTable)",
				"documentation": "/**\n  PKTABLE_CAT String =\u003e parent key table catalog (may be null)\n  PKTABLE_SCHEM String =\u003e parent key table schema (may be null)\n  PKTABLE_NAME String =\u003e parent key table name\n  PKCOLUMN_NAME String =\u003e parent key column name\n  FKTABLE_CAT String =\u003e foreign key table catalog (may be null) being exported (may be null)\n  FKTABLE_SCHEM String =\u003e foreign key table schema (may be null) being exported (may be null)\n  FKTABLE_NAME String =\u003e foreign key table name being exported\n  FKCOLUMN_NAME String =\u003e foreign key column name being exported\n  KEY_SEQ short =\u003e sequence number within foreign key( a value of 1 represents the first column of the foreign key, a value of 2 would represent the second column within the foreign key).\n  UPDATE_RULE short =\u003e What happens to foreign key when parent key is updated:\n  importedNoAction - do not allow update of parent key if it has been imported\n  importedKeyCascade - change imported key to agree with parent key update\n  importedKeySetNull - change imported key to NULL if its parent key has been updated\n  importedKeySetDefault - change imported key to default values if its parent key has been updated\n  importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility)\n  DELETE_RULE short =\u003e What happens to the foreign key when parent key is deleted.\n  importedKeyNoAction - do not allow delete of parent key if it has been imported\n  importedKeyCascade - delete rows that import a deleted key\n  importedKeySetNull - change imported key to NULL if its primary key has been deleted\n  importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility)\n  importedKeySetDefault - change imported key to default if its parent key has been deleted\n  FK_NAME String =\u003e foreign key name (may be null)\n  PK_NAME String =\u003e parent key name (may be null)\n  DEFERRABILITY short =\u003e can the evaluation of foreign key constraints be deferred until commit\n  importedKeyInitiallyDeferred - see SQL92 for definition\n  importedKeyInitiallyImmediate - see SQL92 for definition\n  importedKeyNotDeferrable - see SQL92 for definition\n */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetFunctionsOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetFunctionsOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "public GetFunctionsOperation(HiveSession parentSession,\n      String catalogName, String schemaName, String functionName)",
				"documentation": "/**\n * GetFunctionsOperation.\n *\n */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetPrimaryKeysOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetPrimaryKeysOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "public GetPrimaryKeysOperation(HiveSession parentSession,\n                                 String catalogName, String schemaName, String tableName)",
				"documentation": "/**\n  TABLE_CAT String =\u003e table catalog (may be null)\n  TABLE_SCHEM String =\u003e table schema (may be null)\n  TABLE_NAME String =\u003e table name\n  COLUMN_NAME String =\u003e column name\n  KEY_SEQ short =\u003e sequence number within primary key( a value of 1 represents the first column of the primary key, a value of 2 would represent the second column within the primary key).\n  PK_NAME String =\u003e primary key name (may be null)\n  */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetSchemasOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetSchemasOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "protected GetSchemasOperation(HiveSession parentSession,\n      String catalogName, String schemaName)",
				"documentation": "/**\n * GetSchemasOperation.\n *\n */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetTableTypesOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetTableTypesOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "protected GetTableTypesOperation(HiveSession parentSession)",
				"documentation": "/**\n * GetTableTypesOperation.\n *\n */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetTablesOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetTablesOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "protected GetTablesOperation(HiveSession parentSession,\n      String catalogName, String schemaName, String tableName,\n      List\u003cString\u003e tableTypes)",
				"documentation": "/**\n * GetTablesOperation.\n *\n */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * GetTypeInfoOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.GetTypeInfoOperation",
		"extends": "org.apache.hive.service.cli.operation.MetadataOperation",
		"Methods": [
			{
				"signature": "protected GetTypeInfoOperation(HiveSession parentSession)",
				"documentation": "/**\n * GetTypeInfoOperation.\n *\n */"
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Executes a HiveCommand\n */",
		"name": "org.apache.hive.service.cli.operation.HiveCommandOperation",
		"extends": "org.apache.hive.service.cli.operation.ExecuteStatementOperation",
		"Methods": [
			{
				"signature": "protected HiveCommandOperation(HiveSession parentSession, String statement,\n      CommandProcessor commandProcessor, Map\u003cString, String\u003e confOverlay)",
				"documentation": "/**\n   * For processors other than Hive queries (Driver), they output to session.out (a temp file)\n   * first and the fetchOne/fetchN/fetchAll functions get the output from pipeIn.\n   */"
			},
			{
				"signature": "private void setupSessionIO(SessionState sessionState)",
				"documentation": ""
			},
			{
				"signature": "private void tearDownSessionIO()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private List\u003cString\u003e readResults(int nLines) throws HiveSQLException",
				"documentation": "/**\n   * Reads the temporary results for non-Hive (non-Driver) commands to the\n   * resulting List of strings.\n   * @param nLines number of lines read at once. If it is \u003c= 0, then read all lines.\n   */"
			},
			{
				"signature": "private void cleanTmpFile()",
				"documentation": ""
			},
			{
				"signature": "private void resetResultReader()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.ServiceUtils",
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * HiveTableTypeMapping.\n * Default table type mapping\n *\n */",
		"name": "org.apache.hive.service.cli.operation.HiveTableTypeMapping",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n  public String[] mapToHiveType(String clientTypeName)",
				"documentation": "/**\n * HiveTableTypeMapping.\n * Default table type mapping\n *\n */"
			},
			{
				"signature": "@Override\n  public String mapToClientType(String hiveTypeName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Set\u003cString\u003e getTableTypeNames()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.cli.operation.TableTypeMapping"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * An Appender to divert logs from individual threads to the LogObject they belong to.\n */",
		"name": "org.apache.hive.service.cli.operation.LogDivertAppender",
		"extends": "org.apache.logging.log4j.core.appender.AbstractWriterAppender",
		"Methods": [
			{
				"signature": "private static StringLayout getLayout(boolean isVerbose, StringLayout lo)",
				"documentation": "/** This is where the log message will go to */"
			},
			{
				"signature": "private static StringLayout initLayout(OperationLog.LoggingLevel loggingMode)",
				"documentation": ""
			},
			{
				"signature": "public LogDivertAppender(OperationManager operationManager,\n    OperationLog.LoggingLevel loggingMode)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void append(LogEvent event)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.operation.LogDivertAppender.NameFilter"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.cli.operation.LogDivertAppender.NameFilter"
		]
	},
	{
		"documentation": "/**\n   * A log filter that filters messages coming from the logger with the given names.\n   * It be used as a white list filter or a black list filter.\n   * We apply black list filter on the Loggers used by the log diversion stuff, so that\n   * they don't generate more logs for themselves when they process logs.\n   * White list filter is used for less verbose log collection\n   */",
		"name": "org.apache.hive.service.cli.operation.LogDivertAppender.NameFilter",
		"extends": "",
		"Methods": [
			{
				"signature": "private void setCurrentNamePattern(OperationLog.LoggingLevel mode)",
				"documentation": "/**\n   * A log filter that filters messages coming from the logger with the given names.\n   * It be used as a white list filter or a black list filter.\n   * We apply black list filter on the Loggers used by the log diversion stuff, so that\n   * they don't generate more logs for themselves when they process logs.\n   * White list filter is used for less verbose log collection\n   */"
			},
			{
				"signature": "NameFilter(\n      OperationLog.LoggingLevel loggingMode, OperationManager op)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result getOnMismatch()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result getOnMatch()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object... objects)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1, Object o2)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1, Object o2, Object o3)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1, Object o2, Object o3, Object o4)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1, Object o2, Object o3, Object o4, Object o5)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1, Object o2, Object o3, Object o4, Object o5, Object o6)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1, Object o2, Object o3, Object o4, Object o5, Object o6, Object o7)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1, Object o2, Object o3, Object o4, Object o5, Object o6, Object o7, Object o8)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, String s, Object o, Object o1, Object o2, Object o3, Object o4, Object o5, Object o6, Object o7, Object o8, Object o9)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, Object o, Throwable throwable)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(org.apache.logging.log4j.core.Logger logger, Level level, Marker marker, Message message, Throwable throwable)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public Result filter(LogEvent logEvent)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public State getState()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void initialize()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void start()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void stop()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean isStarted()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public boolean isStopped()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.logging.log4j.core.Filter"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.operation.LogDivertAppender"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * MetadataOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.MetadataOperation",
		"extends": "org.apache.hive.service.cli.operation.Operation",
		"Methods": [
			{
				"signature": "protected MetadataOperation(HiveSession parentSession, OperationType opType)",
				"documentation": "/**\n * MetadataOperation.\n *\n */"
			},
			{
				"signature": "@Override\n  public void close() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "protected String convertIdentifierPattern(final String pattern, boolean datanucleusFormat)",
				"documentation": "/**\n   * Convert wildchars and escape sequence from JDBC format to datanucleous/regex\n   */"
			},
			{
				"signature": "protected String convertSchemaPattern(final String pattern)",
				"documentation": "/**\n   * Convert wildchars and escape sequence of schema pattern from JDBC format to datanucleous/regex\n   * The schema pattern treats empty string also as wildchar\n   */"
			},
			{
				"signature": "private String convertPattern(final String pattern, boolean datanucleusFormat)",
				"documentation": "/**\n   * Convert a pattern containing JDBC catalog search wildcards into\n   * Java regex patterns.\n   *\n   * @param pattern input which may contain '%' or '_' wildcard characters, or\n   * these characters escaped using {@link #getSearchStringEscape()}.\n   * @return replace %/_ with regex search characters, also handle escaped\n   * characters.\n   *\n   * The datanucleus module expects the wildchar as '*'. The columns search on the\n   * other hand is done locally inside the hive code and that requires the regex wildchar\n   * format '.*'  This is driven by the datanucleusFormat flag.\n   */"
			},
			{
				"signature": "protected boolean isAuthV2Enabled()",
				"documentation": ""
			},
			{
				"signature": "protected void authorizeMetaGets(HiveOperationType opType, List\u003cHivePrivilegeObject\u003e inpObjs)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "protected void authorizeMetaGets(HiveOperationType opType, List\u003cHivePrivilegeObject\u003e inpObjs,\n      String cmdString) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.hive.service.cli.operation.GetCatalogsOperation",
			"org.apache.hive.service.cli.operation.GetColumnsOperation",
			"org.apache.hive.service.cli.operation.GetCrossReferenceOperation",
			"org.apache.hive.service.cli.operation.GetFunctionsOperation",
			"org.apache.hive.service.cli.operation.GetPrimaryKeysOperation",
			"org.apache.hive.service.cli.operation.GetSchemasOperation",
			"org.apache.hive.service.cli.operation.GetTableTypesOperation",
			"org.apache.hive.service.cli.operation.GetTablesOperation",
			"org.apache.hive.service.cli.operation.GetTypeInfoOperation"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.operation.Operation",
		"extends": "",
		"Methods": [
			{
				"signature": "protected Operation(HiveSession parentSession, OperationType opType)",
				"documentation": ""
			},
			{
				"signature": "protected Operation(HiveSession parentSession, Map\u003cString, String\u003e confOverlay,\n      OperationType opType)",
				"documentation": ""
			},
			{
				"signature": "protected Operation(HiveSession parentSession,\n      Map\u003cString, String\u003e confOverlay, OperationType opType, boolean runInBackground)",
				"documentation": ""
			},
			{
				"signature": "public Future\u003c?\u003e getBackgroundHandle()",
				"documentation": ""
			},
			{
				"signature": "protected void setBackgroundHandle(Future\u003c?\u003e backgroundHandle)",
				"documentation": ""
			},
			{
				"signature": "public boolean shouldRunAsync()",
				"documentation": ""
			},
			{
				"signature": "public void setConfiguration(HiveConf configuration)",
				"documentation": ""
			},
			{
				"signature": "public HiveConf getConfiguration()",
				"documentation": ""
			},
			{
				"signature": "public HiveSession getParentSession()",
				"documentation": ""
			},
			{
				"signature": "public OperationHandle getHandle()",
				"documentation": ""
			},
			{
				"signature": "public TProtocolVersion getProtocolVersion()",
				"documentation": ""
			},
			{
				"signature": "public OperationType getType()",
				"documentation": ""
			},
			{
				"signature": "public OperationStatus getStatus()",
				"documentation": ""
			},
			{
				"signature": "public boolean hasResultSet()",
				"documentation": ""
			},
			{
				"signature": "protected void setHasResultSet(boolean hasResultSet)",
				"documentation": ""
			},
			{
				"signature": "public OperationLog getOperationLog()",
				"documentation": ""
			},
			{
				"signature": "protected final OperationState setState(OperationState newState) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public boolean isTimedOut(long current)",
				"documentation": ""
			},
			{
				"signature": "public long getLastAccessTime()",
				"documentation": ""
			},
			{
				"signature": "public long getOperationTimeout()",
				"documentation": ""
			},
			{
				"signature": "public void setOperationTimeout(long operationTimeout)",
				"documentation": ""
			},
			{
				"signature": "protected void setOperationException(HiveSQLException operationException)",
				"documentation": ""
			},
			{
				"signature": "protected final void assertState(OperationState state) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public boolean isRunning()",
				"documentation": ""
			},
			{
				"signature": "public boolean isFinished()",
				"documentation": ""
			},
			{
				"signature": "public boolean isCanceled()",
				"documentation": ""
			},
			{
				"signature": "public boolean isFailed()",
				"documentation": ""
			},
			{
				"signature": "protected void createOperationLog()",
				"documentation": ""
			},
			{
				"signature": "protected void unregisterOperationLog()",
				"documentation": ""
			},
			{
				"signature": "protected void beforeRun()",
				"documentation": "/**\n   * Invoked before runInternal().\n   * Set up some preconditions, or configurations.\n   */"
			},
			{
				"signature": "protected void afterRun()",
				"documentation": "/**\n   * Invoked after runInternal(), even if an exception is thrown in runInternal().\n   * Clean up resources, which was set up in beforeRun().\n   */"
			},
			{
				"signature": "public void run() throws HiveSQLException",
				"documentation": "/**\n   * Implemented by subclass of Operation class to execute specific behaviors.\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "protected void cleanupOperationLog()",
				"documentation": ""
			},
			{
				"signature": "public void cancel() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public void close() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public RowSet getNextRowSet() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "protected void validateDefaultFetchOrientation(FetchOrientation orientation)\n      throws HiveSQLException",
				"documentation": "/**\n   * Verify if the given fetch orientation is part of the default orientation types.\n   * @param orientation\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "protected void validateFetchOrientation(FetchOrientation orientation,\n      EnumSet\u003cFetchOrientation\u003e supportedOrientations) throws HiveSQLException",
				"documentation": "/**\n   * Verify if the given fetch orientation is part of the supported orientation types.\n   * @param orientation\n   * @param supportedOrientations\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "protected HiveSQLException toSQLException(String prefix, CommandProcessorResponse response)",
				"documentation": ""
			},
			{
				"signature": "protected Map\u003cString, String\u003e getConfOverlay()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.hive.service.cli.operation.ExecuteStatementOperation",
			"org.apache.hive.service.cli.operation.MetadataOperation"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.OperationHandle",
			"org.apache.hive.service.cli.OperationStatus"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * OperationManager.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.OperationManager",
		"extends": "org.apache.hive.service.AbstractService",
		"Methods": [
			{
				"signature": "public OperationManager()",
				"documentation": "/**\n * OperationManager.\n *\n */"
			},
			{
				"signature": "private void initOperationLogCapture(String loggingMode)",
				"documentation": ""
			},
			{
				"signature": "public ExecuteStatementOperation newExecuteStatementOperation(HiveSession parentSession,\n      String statement, Map\u003cString, String\u003e confOverlay, boolean runAsync)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public ExecuteStatementOperation newExecuteStatementOperation(HiveSession parentSession,\n      String statement, Map\u003cString, String\u003e confOverlay, boolean runAsync, long queryTimeout)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public GetTypeInfoOperation newGetTypeInfoOperation(HiveSession parentSession)",
				"documentation": ""
			},
			{
				"signature": "public GetCatalogsOperation newGetCatalogsOperation(HiveSession parentSession)",
				"documentation": ""
			},
			{
				"signature": "public GetSchemasOperation newGetSchemasOperation(HiveSession parentSession,\n      String catalogName, String schemaName)",
				"documentation": ""
			},
			{
				"signature": "public MetadataOperation newGetTablesOperation(HiveSession parentSession,\n      String catalogName, String schemaName, String tableName,\n      List\u003cString\u003e tableTypes)",
				"documentation": ""
			},
			{
				"signature": "public GetTableTypesOperation newGetTableTypesOperation(HiveSession parentSession)",
				"documentation": ""
			},
			{
				"signature": "public GetColumnsOperation newGetColumnsOperation(HiveSession parentSession,\n      String catalogName, String schemaName, String tableName, String columnName)",
				"documentation": ""
			},
			{
				"signature": "public GetFunctionsOperation newGetFunctionsOperation(HiveSession parentSession,\n      String catalogName, String schemaName, String functionName)",
				"documentation": ""
			},
			{
				"signature": "public GetPrimaryKeysOperation newGetPrimaryKeysOperation(HiveSession parentSession,\n      String catalogName, String schemaName, String tableName)",
				"documentation": ""
			},
			{
				"signature": "public GetCrossReferenceOperation newGetCrossReferenceOperation(\n      HiveSession session, String primaryCatalog, String primarySchema,\n      String primaryTable, String foreignCatalog, String foreignSchema,\n      String foreignTable)",
				"documentation": ""
			},
			{
				"signature": "public Operation getOperation(OperationHandle operationHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public OperationStatus getOperationStatus(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public void cancelOperation(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public void closeOperation(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public TableSchema getOperationResultSetSchema(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public RowSet getOperationNextRowSet(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public RowSet getOperationNextRowSet(OperationHandle opHandle,\n      FetchOrientation orientation, long maxRows)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public RowSet getOperationLogRowSet(OperationHandle opHandle,\n      FetchOrientation orientation, long maxRows)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private boolean isFetchFirst(FetchOrientation fetchOrientation)",
				"documentation": ""
			},
			{
				"signature": "private Schema getLogSchema()",
				"documentation": ""
			},
			{
				"signature": "public OperationLog getOperationLogByThread()",
				"documentation": ""
			},
			{
				"signature": "public List\u003cOperation\u003e removeExpiredOperations(OperationHandle[] handles)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [
			"org.apache.hive.service.cli.session.SessionManager"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * SQLOperation.\n *\n */",
		"name": "org.apache.hive.service.cli.operation.SQLOperation",
		"extends": "org.apache.hive.service.cli.operation.ExecuteStatementOperation",
		"Methods": [
			{
				"signature": "public SQLOperation(HiveSession parentSession, String statement, Map\u003cString, String\u003e confOverlay,\n      boolean runInBackground, long queryTimeout)",
				"documentation": "/**\n * SQLOperation.\n *\n */"
			},
			{
				"signature": "public void prepare(QueryState queryState) throws HiveSQLException",
				"documentation": "/***\n   * Compile the query and extract metadata\n   * @param queryState\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "private void runQuery(HiveConf sqlOperationConf) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void runInternal() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private UserGroupInformation getCurrentUGI(HiveConf opConfig) throws HiveSQLException",
				"documentation": "/**\n   * Returns the current UGI on the stack\n   * @param opConfig\n   * @return UserGroupInformation\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "private Hive getSessionHive() throws HiveSQLException",
				"documentation": "/**\n   * Returns the ThreadLocal Hive for the current thread\n   * @return Hive\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "private void cleanup(OperationState state) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cancel() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetSchema() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private RowSet decode(List\u003cObject\u003e rows, RowSet rowSet) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private RowSet prepareFromRow(List\u003cObject\u003e rows, RowSet rowSet) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private RowSet decodeFromString(List\u003cObject\u003e rows, RowSet rowSet)\n      throws SQLException, SerDeException",
				"documentation": ""
			},
			{
				"signature": "private AbstractSerDe getSerDe() throws SQLException",
				"documentation": ""
			},
			{
				"signature": "private HiveConf getConfigForOperation() throws HiveSQLException",
				"documentation": "/**\n   * If there are query specific settings to overlay, then create a copy of config\n   * There are two cases we need to clone the session config that's being passed to hive driver\n   * 1. Async query -\n   *    If the client changes a config setting, that shouldn't reflect in the execution already underway\n   * 2. confOverlay -\n   *    The query specific settings should only be applied to the query config and not session\n   * @return new configuration\n   * @throws HiveSQLException\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.RowSetFactory",
			"org.apache.hive.service.cli.TableSchema"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.operation.TableTypeMapping",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.hive.service.cli.operation.ClassicTableTypeMapping",
			"org.apache.hive.service.cli.operation.HiveTableTypeMapping"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.operation.TableTypeMappingFactory",
		"extends": "",
		"Methods": [
			{
				"signature": "public static TableTypeMapping getTableTypeMapping(String mappingType)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.cli.operation.TableTypeMappings"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.operation.TableTypeMappings",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.session.HiveSession",
		"extends": "org.apache.hive.service.cli.session.HiveSessionBase",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.hive.service.cli.session.HiveSessionImpl"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Methods that don't need to be executed under a doAs\n * context are here. Rest of them in HiveSession interface\n */",
		"name": "org.apache.hive.service.cli.session.HiveSessionBase",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.hive.service.cli.session.HiveSession"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * HiveSessionHookContext.\n * Interface passed to the HiveServer2 session hook execution. This enables\n * the hook implementation to access session config, user and session handle\n */",
		"name": "org.apache.hive.service.cli.session.HiveSessionHookContext",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.hive.service.cli.session.HiveSessionHookContextImpl"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n *\n * HiveSessionHookContextImpl.\n * Session hook context implementation which is created by session  manager\n * and passed to hook invocation.\n */",
		"name": "org.apache.hive.service.cli.session.HiveSessionHookContextImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "HiveSessionHookContextImpl(HiveSession hiveSession)",
				"documentation": "/**\n *\n * HiveSessionHookContextImpl.\n * Session hook context implementation which is created by session  manager\n * and passed to hook invocation.\n */"
			},
			{
				"signature": "@Override\n  public HiveConf getSessionConf()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getSessionUser()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getSessionHandle()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.cli.session.HiveSessionHookContext"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * HiveSession\n *\n */",
		"name": "org.apache.hive.service.cli.session.HiveSessionImpl",
		"extends": "",
		"Methods": [
			{
				"signature": "public HiveSessionImpl(TProtocolVersion protocol, String username, String password,\n      HiveConf serverhiveConf, String ipAddress)",
				"documentation": "/**\n * HiveSession\n *\n */"
			},
			{
				"signature": "public void open(Map\u003cString, String\u003e sessionConfMap) throws HiveSQLException",
				"documentation": "/**\n   * Opens a new HiveServer2 session for the client connection.\n   * Creates a new SessionState object that will be associated with this HiveServer2 session.\n   * When the server executes multiple queries in the same session,\n   * this SessionState object is reused across multiple queries.\n   * Note that if doAs is true, this call goes through a proxy object,\n   * which wraps the method logic in a UserGroupInformation#doAs.\n   * That's why it is important to create SessionState here rather than in the constructor.\n   */"
			},
			{
				"signature": "private void processGlobalInitFile()",
				"documentation": ""
			},
			{
				"signature": "private void configureSession(Map\u003cString, String\u003e sessionConfMap) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public static int setVariable(String varname, String varvalue) throws Exception",
				"documentation": ""
			},
			{
				"signature": "private static void setConf(String varname, String key, String varvalue, boolean register)\n          throws IllegalArgumentException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setOperationLogSessionDir(File operationLogRootDir)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public boolean isOperationLogEnabled()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public File getOperationLogSessionDir()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TProtocolVersion getProtocolVersion()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SessionManager getSessionManager()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setSessionManager(SessionManager sessionManager)",
				"documentation": ""
			},
			{
				"signature": "private OperationManager getOperationManager()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setOperationManager(OperationManager operationManager)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SessionHandle getSessionHandle()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getUsername()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getPassword()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public HiveConf getHiveConf()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public IMetaStoreClient getMetaStoreClient() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public GetInfoValue getInfo(GetInfoType getInfoType)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatement(String statement, Map\u003cString, String\u003e confOverlay)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatement(String statement, Map\u003cString, String\u003e confOverlay,\n      long queryTimeout) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatementAsync(String statement, Map\u003cString, String\u003e confOverlay)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatementAsync(String statement, Map\u003cString, String\u003e confOverlay,\n      long queryTimeout) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private OperationHandle executeStatementInternal(String statement,\n      Map\u003cString, String\u003e confOverlay, boolean runAsync, long queryTimeout) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getTypeInfo()\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getCatalogs()\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getSchemas(String catalogName, String schemaName)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getTables(String catalogName, String schemaName, String tableName,\n      List\u003cString\u003e tableTypes)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getTableTypes()\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getColumns(String catalogName, String schemaName,\n      String tableName, String columnName)  throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getFunctions(String catalogName, String schemaName, String functionName)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private void cleanupPipeoutFile()",
				"documentation": ""
			},
			{
				"signature": "private void cleanupSessionLogDir()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SessionState getSessionState()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getUserName()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setUserName(String userName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getLastAccessTime()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void closeExpiredOperations()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public long getNoOperationTime()",
				"documentation": ""
			},
			{
				"signature": "private void closeTimedOutOperations(List\u003cOperation\u003e operations)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cancelOperation(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void closeOperation(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetMetadata(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet fetchResults(OperationHandle opHandle, FetchOrientation orientation,\n      long maxRows, FetchType fetchType) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "protected HiveSession getSession()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getIpAddress()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void setIpAddress(String ipAddress)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getDelegationToken(HiveAuthFactory authFactory, String owner, String renewer)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cancelDelegationToken(HiveAuthFactory authFactory, String tokenStr)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void renewDelegationToken(HiveAuthFactory authFactory, String tokenStr)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private String getUserFromToken(HiveAuthFactory authFactory, String tokenStr) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getPrimaryKeys(String catalog, String schema,\n      String table) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getCrossReference(String primaryCatalog,\n      String primarySchema, String primaryTable, String foreignCatalog,\n      String foreignSchema, String foreignTable) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.cli.session.HiveSession"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.hive.service.cli.session.HiveSessionImplwithUGI"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.auth.HiveAuthFactory",
			"org.apache.hive.service.cli.GetInfoValue",
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.SessionHandle",
			"org.apache.hive.service.cli.session.GlobalHivercFileProcessor"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.cli.session.GlobalHivercFileProcessor"
		]
	},
	{
		"documentation": "/**\n   * It is used for processing hiverc file from HiveServer2 side.\n   */",
		"name": "org.apache.hive.service.cli.session.GlobalHivercFileProcessor",
		"extends": "org.apache.hadoop.hive.common.cli.HiveFileProcessor",
		"Methods": [
			{
				"signature": "@Override\n    protected BufferedReader loadFile(String fileName) throws IOException",
				"documentation": "/**\n   * It is used for processing hiverc file from HiveServer2 side.\n   */"
			},
			{
				"signature": "@Override\n    protected int processCmd(String cmd)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.session.HiveSessionImpl"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n *\n * HiveSessionImplwithUGI.\n * HiveSession with connecting user's UGI and delegation token if required\n */",
		"name": "org.apache.hive.service.cli.session.HiveSessionImplwithUGI",
		"extends": "org.apache.hive.service.cli.session.HiveSessionImpl",
		"Methods": [
			{
				"signature": "public HiveSessionImplwithUGI(TProtocolVersion protocol, String username, String password,\n      HiveConf hiveConf, String ipAddress, String delegationToken) throws HiveSQLException",
				"documentation": "/**\n *\n * HiveSessionImplwithUGI.\n * HiveSession with connecting user's UGI and delegation token if required\n */"
			},
			{
				"signature": "public void setSessionUGI(String owner) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public UserGroupInformation getSessionUgi()",
				"documentation": ""
			},
			{
				"signature": "public String getDelegationToken()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close() throws HiveSQLException",
				"documentation": "/**\n   * Close the file systems for the session and remove it from the FileSystem cache.\n   * Cancel the session's delegation token and close the metastore connection\n   */"
			},
			{
				"signature": "private void setDelegationToken(String delegationTokenStr) throws HiveSQLException",
				"documentation": "/**\n   * Enable delegation token for the session\n   * save the token string and set the token.signature in hive conf. The metastore client uses\n   * this token.signature to determine where to use kerberos or delegation token\n   * @throws HiveException\n   * @throws IOException\n   */"
			},
			{
				"signature": "private void cancelDelegationToken() throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected HiveSession getSession()",
				"documentation": ""
			},
			{
				"signature": "public void setProxySession(HiveSession proxySession)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getDelegationToken(HiveAuthFactory authFactory, String owner,\n      String renewer) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cancelDelegationToken(HiveAuthFactory authFactory, String tokenStr)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void renewDelegationToken(HiveAuthFactory authFactory, String tokenStr)\n      throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Proxy wrapper on HiveSession to execute operations\n * by impersonating given user\n */",
		"name": "org.apache.hive.service.cli.session.HiveSessionProxy",
		"extends": "",
		"Methods": [
			{
				"signature": "public HiveSessionProxy(HiveSession hiveSession, UserGroupInformation ugi)",
				"documentation": "/**\n * Proxy wrapper on HiveSession to execute operations\n * by impersonating given user\n */"
			},
			{
				"signature": "public static HiveSession getProxy(HiveSession hiveSession, UserGroupInformation ugi)\n      throws IllegalArgumentException, HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Object invoke(Object arg0, final Method method, final Object[] args)\n      throws Throwable",
				"documentation": ""
			},
			{
				"signature": "private Object invoke(final Method method, final Object[] args) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.lang.reflect.InvocationHandler"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * SessionManager.\n *\n */",
		"name": "org.apache.hive.service.cli.session.SessionManager",
		"extends": "org.apache.hive.service.CompositeService",
		"Methods": [
			{
				"signature": "public SessionManager(HiveServer2 hiveServer2)",
				"documentation": "/**\n * SessionManager.\n *\n */"
			},
			{
				"signature": "private void createBackgroundOperationPool()",
				"documentation": ""
			},
			{
				"signature": "private void initOperationLogRootDir()",
				"documentation": ""
			},
			{
				"signature": "private void startTimeoutChecker()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void run()",
				"documentation": ""
			},
			{
				"signature": "private void sleepFor(long interval)",
				"documentation": ""
			},
			{
				"signature": "private void shutdownTimeoutChecker()",
				"documentation": ""
			},
			{
				"signature": "private void cleanupLoggingRootDir()",
				"documentation": ""
			},
			{
				"signature": "public SessionHandle openSession(TProtocolVersion protocol, String username, String password, String ipAddress,\n      Map\u003cString, String\u003e sessionConf) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public SessionHandle openSession(TProtocolVersion protocol, String username, String password, String ipAddress,\n      Map\u003cString, String\u003e sessionConf, boolean withImpersonation, String delegationToken)\n          throws HiveSQLException",
				"documentation": "/**\n   * Opens a new session and creates a session handle.\n   * The username passed to this method is the effective username.\n   * If withImpersonation is true (==doAs true) we wrap all the calls in HiveSession\n   * within a UGI.doAs, where UGI corresponds to the effective user.\n   *\n   * Please see {@code org.apache.hive.service.cli.thrift.ThriftCLIService.getUserName()} for\n   * more details.\n   *\n   * @param protocol\n   * @param username\n   * @param password\n   * @param ipAddress\n   * @param sessionConf\n   * @param withImpersonation\n   * @param delegationToken\n   * @return\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "public void closeSession(SessionHandle sessionHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public HiveSession getSession(SessionHandle sessionHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "public OperationManager getOperationManager()",
				"documentation": ""
			},
			{
				"signature": "public static void setIpAddress(String ipAddress)",
				"documentation": ""
			},
			{
				"signature": "public static void clearIpAddress()",
				"documentation": ""
			},
			{
				"signature": "public static String getIpAddress()",
				"documentation": ""
			},
			{
				"signature": "public static void setUserName(String userName)",
				"documentation": ""
			},
			{
				"signature": "public static void clearUserName()",
				"documentation": ""
			},
			{
				"signature": "public static String getUserName()",
				"documentation": ""
			},
			{
				"signature": "public static void setProxyUserName(String userName)",
				"documentation": ""
			},
			{
				"signature": "public static String getProxyUserName()",
				"documentation": ""
			},
			{
				"signature": "public static void clearProxyUserName()",
				"documentation": ""
			},
			{
				"signature": "public Future\u003c?\u003e submitBackgroundOperation(Runnable r)",
				"documentation": ""
			},
			{
				"signature": "public int getOpenSessionCount()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.HiveSQLException",
			"org.apache.hive.service.cli.operation.OperationManager",
			"org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup"
		],
		"usedBy": [
			"org.apache.hive.service.cli.CLIService",
			"org.apache.hive.service.cli.thrift.ThriftCLIService",
			"org.apache.hive.service.cli.thrift.ThriftHttpServlet"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.thrift.ThriftBinaryCLIService",
		"extends": "org.apache.hive.service.cli.thrift.ThriftCLIService",
		"Methods": [
			{
				"signature": "public ThriftBinaryCLIService(CLIService cliService)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected void initializeServer()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected void stopServer()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void run()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.ServiceException",
			"org.apache.hive.service.auth.HiveAuthFactory",
			"org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup"
		],
		"usedBy": [
			"org.apache.hive.service.server.HiveServer2"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ThriftCLIService.\n *\n */",
		"name": "org.apache.hive.service.cli.thrift.ThriftCLIService",
		"extends": "org.apache.hive.service.AbstractService",
		"Methods": [
			{
				"signature": "public ThriftCLIService(CLIService service, String serviceName)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public ServerContext createContext(\n          TProtocol input, TProtocol output)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void deleteContext(ServerContext serverContext,\n          TProtocol input, TProtocol output)",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void preServe()",
				"documentation": ""
			},
			{
				"signature": "@Override\n      public void processContext(ServerContext serverContext,\n          TTransport input, TTransport output)",
				"documentation": ""
			},
			{
				"signature": "public int getPortNumber()",
				"documentation": ""
			},
			{
				"signature": "public InetAddress getServerIPAddress()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetDelegationTokenResp GetDelegationToken(TGetDelegationTokenReq req)\n      throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TCancelDelegationTokenResp CancelDelegationToken(TCancelDelegationTokenReq req)\n      throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TRenewDelegationTokenResp RenewDelegationToken(TRenewDelegationTokenReq req)\n      throws TException",
				"documentation": ""
			},
			{
				"signature": "private TStatus notSupportTokenErrorStatus()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TOpenSessionResp OpenSession(TOpenSessionReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TSetClientInfoResp SetClientInfo(TSetClientInfoReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "private String getIpAddress()",
				"documentation": ""
			},
			{
				"signature": "private String getUserName(TOpenSessionReq req) throws HiveSQLException",
				"documentation": "/**\n   * Returns the effective username.\n   * 1. If hive.server2.allow.user.substitution = false: the username of the connecting user\n   * 2. If hive.server2.allow.user.substitution = true: the username of the end user,\n   * that the connecting user is trying to proxy for.\n   * This includes a check whether the connecting user is allowed to proxy for the end user.\n   * @param req\n   * @return\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "private String getShortName(String userName)",
				"documentation": ""
			},
			{
				"signature": "SessionHandle getSessionHandle(TOpenSessionReq req, TOpenSessionResp res)\n      throws HiveSQLException, LoginException, IOException",
				"documentation": "/**\n   * Create a session handle\n   * @param req\n   * @param res\n   * @return\n   * @throws HiveSQLException\n   * @throws LoginException\n   * @throws IOException\n   */"
			},
			{
				"signature": "private String getDelegationToken(String userName)\n      throws HiveSQLException, LoginException, IOException",
				"documentation": ""
			},
			{
				"signature": "private TProtocolVersion getMinVersion(TProtocolVersion... versions)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TCloseSessionResp CloseSession(TCloseSessionReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetInfoResp GetInfo(TGetInfoReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TExecuteStatementResp ExecuteStatement(TExecuteStatementReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetTypeInfoResp GetTypeInfo(TGetTypeInfoReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetCatalogsResp GetCatalogs(TGetCatalogsReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetSchemasResp GetSchemas(TGetSchemasReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetTablesResp GetTables(TGetTablesReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetTableTypesResp GetTableTypes(TGetTableTypesReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetColumnsResp GetColumns(TGetColumnsReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetFunctionsResp GetFunctions(TGetFunctionsReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetOperationStatusResp GetOperationStatus(TGetOperationStatusReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TCancelOperationResp CancelOperation(TCancelOperationReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TCloseOperationResp CloseOperation(TCloseOperationReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetResultSetMetadataResp GetResultSetMetadata(TGetResultSetMetadataReq req)\n      throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TFetchResultsResp FetchResults(TFetchResultsReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetPrimaryKeysResp GetPrimaryKeys(TGetPrimaryKeysReq req)\n      throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetCrossReferenceResp GetCrossReference(TGetCrossReferenceReq req)\n      throws TException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException",
				"documentation": ""
			},
			{
				"signature": "private String getProxyUser(String realUser, Map\u003cString, String\u003e sessionConf,\n      String ipAddress) throws HiveSQLException",
				"documentation": "/**\n   * If the proxy user name is provided then check privileges to substitute the user.\n   * @param realUser\n   * @param sessionConf\n   * @param ipAddress\n   * @return\n   * @throws HiveSQLException\n   */"
			},
			{
				"signature": "private boolean isKerberosAuthMode()",
				"documentation": ""
			}
		],
		"interfaces": [
			"TCLIService.Iface",
			"Runnable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.hive.service.cli.thrift.ThriftBinaryCLIService",
			"org.apache.hive.service.cli.thrift.ThriftHttpCLIService"
		],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.ServiceException",
			"org.apache.hive.service.ServiceUtils",
			"org.apache.hive.service.auth.HiveAuthFactory",
			"org.apache.hive.service.auth.TSetIpAddressProcessor",
			"org.apache.hive.service.cli.session.SessionManager",
			"org.apache.hive.service.cli.thrift.ThriftCLIService.ThriftCLIServerContext"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.cli.thrift.ThriftCLIService.ThriftCLIServerContext"
		]
	},
	{
		"documentation": "/**\n * ThriftCLIService.\n *\n */",
		"name": "org.apache.hive.service.cli.thrift.ThriftCLIService.ThriftCLIServerContext",
		"extends": "",
		"Methods": [
			{
				"signature": "public void setSessionHandle(SessionHandle sessionHandle)",
				"documentation": "/**\n * ThriftCLIService.\n *\n */"
			},
			{
				"signature": "public SessionHandle getSessionHandle()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.thrift.server.ServerContext"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.thrift.ThriftCLIService"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * ThriftCLIServiceClient.\n *\n */",
		"name": "org.apache.hive.service.cli.thrift.ThriftCLIServiceClient",
		"extends": "CLIServiceClient",
		"Methods": [
			{
				"signature": "public ThriftCLIServiceClient(TCLIService.Iface cliService)",
				"documentation": "/**\n * ThriftCLIServiceClient.\n *\n */"
			},
			{
				"signature": "public void checkStatus(TStatus status) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SessionHandle openSession(String username, String password,\n      Map\u003cString, String\u003e configuration)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public SessionHandle openSessionWithImpersonation(String username, String password,\n      Map\u003cString, String\u003e configuration, String delegationToken) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void closeSession(SessionHandle sessionHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public GetInfoValue getInfo(SessionHandle sessionHandle, GetInfoType infoType)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatement(SessionHandle sessionHandle, String statement,\n      Map\u003cString, String\u003e confOverlay) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatement(SessionHandle sessionHandle, String statement,\n      Map\u003cString, String\u003e confOverlay, long queryTimeout) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatementAsync(SessionHandle sessionHandle, String statement,\n      Map\u003cString, String\u003e confOverlay) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle executeStatementAsync(SessionHandle sessionHandle, String statement,\n      Map\u003cString, String\u003e confOverlay, long queryTimeout) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "private OperationHandle executeStatementInternal(SessionHandle sessionHandle, String statement,\n      Map\u003cString, String\u003e confOverlay, boolean isAsync, long queryTimeout) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getTypeInfo(SessionHandle sessionHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getCatalogs(SessionHandle sessionHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getSchemas(SessionHandle sessionHandle, String catalogName,\n      String schemaName)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getTables(SessionHandle sessionHandle, String catalogName,\n      String schemaName, String tableName, List\u003cString\u003e tableTypes)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getTableTypes(SessionHandle sessionHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getColumns(SessionHandle sessionHandle,\n      String catalogName, String schemaName, String tableName, String columnName)\n          throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getFunctions(SessionHandle sessionHandle,\n      String catalogName, String schemaName, String functionName) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationStatus getOperationStatus(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cancelOperation(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void closeOperation(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public TableSchema getResultSetMetadata(OperationHandle opHandle)\n      throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet fetchResults(OperationHandle opHandle, FetchOrientation orientation, long maxRows,\n      FetchType fetchType) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public RowSet fetchResults(OperationHandle opHandle) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory,\n      String owner, String renewer) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void cancelDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory,\n      String tokenStr) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void renewDelegationToken(SessionHandle sessionHandle, HiveAuthFactory authFactory,\n      String tokenStr) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getPrimaryKeys(SessionHandle sessionHandle,\n      String catalog, String schema, String table) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public OperationHandle getCrossReference(SessionHandle sessionHandle,\n      String primaryCatalog, String primarySchema, String primaryTable,\n      String foreignCatalog, String foreignSchema, String foreignTable) throws HiveSQLException",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public String getQueryId(TOperationHandle operationHandle) throws HiveSQLException",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.thrift.ThriftHttpCLIService",
		"extends": "org.apache.hive.service.cli.thrift.ThriftCLIService",
		"Methods": [
			{
				"signature": "public ThriftHttpCLIService(CLIService cliService)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected void initializeServer()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  protected void stopServer()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void run()",
				"documentation": "/**\n   * Configure Jetty to serve http requests. Example of a client connection URL:\n   * http://localhost:10000/servlets/thrifths2/ A gateway may cause actual target URL to differ,\n   * e.g. http://gateway:port/hive2/servlets/thrifths2/\n   */"
			},
			{
				"signature": "private String getHttpPath(String httpPath)",
				"documentation": "/**\n   * The config parameter can be like \"path\", \"/path\", \"/path/\", \"path/*\", \"/path1/path2/*\" and so on.\n   * httpPath should end up as \"/*\", \"/path/*\" or \"/path1/../pathN/*\"\n   * @param httpPath\n   * @return\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.ServiceException",
			"org.apache.hive.service.auth.HiveAuthFactory",
			"org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup"
		],
		"usedBy": [
			"org.apache.hive.service.server.HiveServer2"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n *\n * ThriftHttpServlet\n *\n */",
		"name": "org.apache.hive.service.cli.thrift.ThriftHttpServlet",
		"extends": "org.apache.thrift.server.TServlet",
		"Methods": [
			{
				"signature": "public ThriftHttpServlet(TProcessor processor, TProtocolFactory protocolFactory,\n      String authType, UserGroupInformation serviceUGI, UserGroupInformation httpUGI,\n      HiveAuthFactory hiveAuthFactory)",
				"documentation": "/**\n *\n * ThriftHttpServlet\n *\n */"
			},
			{
				"signature": "@Override\n  protected void doPost(HttpServletRequest request, HttpServletResponse response)\n      throws ServletException, IOException",
				"documentation": ""
			},
			{
				"signature": "private String getClientNameFromCookie(Cookie[] cookies)",
				"documentation": "/**\n   * Retrieves the client name from cookieString. If the cookie does not\n   * correspond to a valid client, the function returns null.\n   * @param cookies HTTP Request cookies.\n   * @return Client Username if cookieString has a HS2 Generated cookie that is currently valid.\n   * Else, returns null.\n   */"
			},
			{
				"signature": "private String toCookieStr(Cookie[] cookies)",
				"documentation": "/**\n   * Convert cookie array to human readable cookie string\n   * @param cookies Cookie Array\n   * @return String containing all the cookies separated by a newline character.\n   * Each cookie is of the format [key]=[value]\n   */"
			},
			{
				"signature": "private String validateCookie(HttpServletRequest request) throws UnsupportedEncodingException",
				"documentation": "/**\n   * Validate the request cookie. This function iterates over the request cookie headers\n   * and finds a cookie that represents a valid client/server session. If it finds one, it\n   * returns the client name associated with the session. Else, it returns null.\n   * @param request The HTTP Servlet Request send by the client\n   * @return Client Username if the request has valid HS2 cookie, else returns null\n   * @throws UnsupportedEncodingException\n   */"
			},
			{
				"signature": "private Cookie createCookie(String str) throws UnsupportedEncodingException",
				"documentation": "/**\n   * Generate a server side cookie given the cookie value as the input.\n   * @param str Input string token.\n   * @return The generated cookie.\n   * @throws UnsupportedEncodingException\n   */"
			},
			{
				"signature": "private static String getHttpOnlyCookieHeader(Cookie cookie)",
				"documentation": "/**\n   * Generate httponly cookie from HS2 cookie\n   * @param cookie HS2 generated cookie\n   * @return The httponly cookie\n   */"
			},
			{
				"signature": "private String doPasswdAuth(HttpServletRequest request, String authType)\n      throws HttpAuthenticationException",
				"documentation": "/**\n   * Do the LDAP/PAM authentication\n   * @param request\n   * @param authType\n   * @throws HttpAuthenticationException\n   */"
			},
			{
				"signature": "private String doTokenAuth(HttpServletRequest request, HttpServletResponse response)\n      throws HttpAuthenticationException",
				"documentation": ""
			},
			{
				"signature": "private String doKerberosAuth(HttpServletRequest request)\n      throws HttpAuthenticationException",
				"documentation": "/**\n   * Do the GSS-API kerberos authentication.\n   * We already have a logged in subject in the form of serviceUGI,\n   * which GSS-API will extract information from.\n   * In case of a SPNego request we use the httpUGI,\n   * for the authenticating service tickets.\n   * @param request\n   * @return\n   * @throws HttpAuthenticationException\n   */"
			},
			{
				"signature": "private String getUsername(HttpServletRequest request, String authType)\n      throws HttpAuthenticationException",
				"documentation": ""
			},
			{
				"signature": "private String getPassword(HttpServletRequest request, String authType)\n      throws HttpAuthenticationException",
				"documentation": ""
			},
			{
				"signature": "private String[] getAuthHeaderTokens(HttpServletRequest request,\n      String authType) throws HttpAuthenticationException",
				"documentation": ""
			},
			{
				"signature": "private String getAuthHeader(HttpServletRequest request, String authType)\n      throws HttpAuthenticationException",
				"documentation": "/**\n   * Returns the base64 encoded auth header payload\n   * @param request\n   * @param authType\n   * @return\n   * @throws HttpAuthenticationException\n   */"
			},
			{
				"signature": "private boolean isKerberosAuthMode(String authType)",
				"documentation": ""
			},
			{
				"signature": "private static String getDoAsQueryParam(String queryString)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.CookieSigner",
			"org.apache.hive.service.auth.AuthenticationProviderFactory",
			"org.apache.hive.service.auth.HttpAuthUtils",
			"org.apache.hive.service.auth.HttpAuthenticationException",
			"org.apache.hive.service.cli.session.SessionManager",
			"org.apache.hive.service.cli.thrift.HttpKerberosServerAction"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.cli.thrift.HttpKerberosServerAction"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.hive.service.cli.thrift.HttpKerberosServerAction",
		"extends": "",
		"Methods": [
			{
				"signature": "HttpKerberosServerAction(HttpServletRequest request,\n        UserGroupInformation serviceUGI)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public String run() throws HttpAuthenticationException",
				"documentation": ""
			},
			{
				"signature": "private String getPrincipalWithoutRealm(String fullPrincipal)\n        throws HttpAuthenticationException",
				"documentation": ""
			},
			{
				"signature": "private String getPrincipalWithoutRealmAndHost(String fullPrincipal)\n        throws HttpAuthenticationException",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.security.PrivilegedExceptionAction"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.auth.HttpAuthenticationException"
		],
		"usedBy": [
			"org.apache.hive.service.cli.thrift.ThriftHttpServlet"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * HiveServer2.\n *\n */",
		"name": "org.apache.hive.service.server.HiveServer2",
		"extends": "org.apache.hive.service.CompositeService",
		"Methods": [
			{
				"signature": "public HiveServer2()",
				"documentation": "/**\n * HiveServer2.\n *\n */"
			},
			{
				"signature": "public static boolean isHTTPTransportMode(HiveConf hiveConf)",
				"documentation": ""
			},
			{
				"signature": "private static void startHiveServer2() throws Throwable",
				"documentation": ""
			},
			{
				"signature": "public static void main(String[] args)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.hive.service.cli.CLIService",
			"org.apache.hive.service.cli.thrift.ThriftBinaryCLIService",
			"org.apache.hive.service.cli.thrift.ThriftHttpCLIService",
			"org.apache.hive.service.server.HiveServer2.ServerOptionsProcessorResponse",
			"org.apache.hive.service.server.HiveServer2.HelpOptionExecutor",
			"org.apache.hive.service.server.HiveServer2.StartOptionExecutor"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.hive.service.server.HiveServer2.ServerOptionsProcessor",
			"org.apache.hive.service.server.HiveServer2.ServerOptionsProcessorResponse",
			"org.apache.hive.service.server.ServerOptionsExecutor",
			"org.apache.hive.service.server.HiveServer2.HelpOptionExecutor",
			"org.apache.hive.service.server.HiveServer2.StartOptionExecutor"
		]
	},
	{
		"documentation": "/**\n   * ServerOptionsProcessor.\n   * Process arguments given to HiveServer2 (-hiveconf property=value)\n   * Set properties in System properties\n   * Create an appropriate response object,\n   * which has executor to execute the appropriate command based on the parsed options.\n   */",
		"name": "org.apache.hive.service.server.HiveServer2.ServerOptionsProcessor",
		"extends": "",
		"Methods": [
			{
				"signature": "@SuppressWarnings(\"static-access\")\n    public ServerOptionsProcessor(String serverName)",
				"documentation": "/**\n   * ServerOptionsProcessor.\n   * Process arguments given to HiveServer2 (-hiveconf property=value)\n   * Set properties in System properties\n   * Create an appropriate response object,\n   * which has executor to execute the appropriate command based on the parsed options.\n   */"
			},
			{
				"signature": "public ServerOptionsProcessorResponse parse(String[] argv)",
				"documentation": ""
			},
			{
				"signature": "StringBuilder getDebugMessage()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * The response sent back from {@link ServerOptionsProcessor#parse(String[])}\n   */",
		"name": "org.apache.hive.service.server.HiveServer2.ServerOptionsProcessorResponse",
		"extends": "",
		"Methods": [
			{
				"signature": "ServerOptionsProcessorResponse(ServerOptionsExecutor serverOptionsExecutor)",
				"documentation": "/**\n   * The response sent back from {@link ServerOptionsProcessor#parse(String[])}\n   */"
			},
			{
				"signature": "ServerOptionsExecutor getServerOptionsExecutor()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.server.HiveServer2"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * The executor interface for running the appropriate HiveServer2 command based on parsed options\n   */",
		"name": "org.apache.hive.service.server.ServerOptionsExecutor",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "interface",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [
			"org.apache.hive.service.server.HiveServer2.HelpOptionExecutor",
			"org.apache.hive.service.server.HiveServer2.StartOptionExecutor"
		],
		"uses": [],
		"usedBy": [],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * HelpOptionExecutor: executes the --help option by printing out the usage\n   */",
		"name": "org.apache.hive.service.server.HiveServer2.HelpOptionExecutor",
		"extends": "",
		"Methods": [
			{
				"signature": "HelpOptionExecutor(String serverName, Options options)",
				"documentation": "/**\n   * HelpOptionExecutor: executes the --help option by printing out the usage\n   */"
			},
			{
				"signature": "@Override\n    public void execute()",
				"documentation": ""
			}
		],
		"interfaces": [
			"org.apache.hive.service.server.ServerOptionsExecutor"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.server.HiveServer2"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n   * StartOptionExecutor: starts HiveServer2.\n   * This is the default executor, when no option is specified.\n   */",
		"name": "org.apache.hive.service.server.HiveServer2.StartOptionExecutor",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public void execute()",
				"documentation": "/**\n   * StartOptionExecutor: starts HiveServer2.\n   * This is the default executor, when no option is specified.\n   */"
			}
		],
		"interfaces": [
			"org.apache.hive.service.server.ServerOptionsExecutor"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.server.HiveServer2"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A ThreadFactory for constructing new HiveServer2 threads that lets you plug\n * in custom cleanup code to be called before this thread is GC-ed.\n * Currently cleans up the following:\n * 1. ThreadLocal RawStore object:\n * In case of an embedded metastore, HiveServer2 threads (foreground and background)\n * end up caching a ThreadLocal RawStore object. The ThreadLocal RawStore object has\n * an instance of PersistenceManagerFactory and PersistenceManager.\n * The PersistenceManagerFactory keeps a cache of PersistenceManager objects,\n * which are only removed when PersistenceManager#close method is called.\n * HiveServer2 uses ExecutorService for managing thread pools for foreground and background threads.\n * ExecutorService unfortunately does not provide any hooks to be called,\n * when a thread from the pool is terminated.\n * As a solution, we're using this ThreadFactory to keep a cache of RawStore objects per thread.\n * And we are doing clean shutdown in the finalizer for each thread.\n */",
		"name": "org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup",
		"extends": "",
		"Methods": [
			{
				"signature": "public ThreadFactoryWithGarbageCleanup(String threadPoolName)",
				"documentation": "/**\n * A ThreadFactory for constructing new HiveServer2 threads that lets you plug\n * in custom cleanup code to be called before this thread is GC-ed.\n * Currently cleans up the following:\n * 1. ThreadLocal RawStore object:\n * In case of an embedded metastore, HiveServer2 threads (foreground and background)\n * end up caching a ThreadLocal RawStore object. The ThreadLocal RawStore object has\n * an instance of PersistenceManagerFactory and PersistenceManager.\n * The PersistenceManagerFactory keeps a cache of PersistenceManager objects,\n * which are only removed when PersistenceManager#close method is called.\n * HiveServer2 uses ExecutorService for managing thread pools for foreground and background threads.\n * ExecutorService unfortunately does not provide any hooks to be called,\n * when a thread from the pool is terminated.\n * As a solution, we're using this ThreadFactory to keep a cache of RawStore objects per thread.\n * And we are doing clean shutdown in the finalizer for each thread.\n */"
			},
			{
				"signature": "@Override\n  public Thread newThread(Runnable runnable)",
				"documentation": ""
			},
			{
				"signature": "public static Map\u003cLong, RawStore\u003e getThreadRawStoreMap()",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.util.concurrent.ThreadFactory"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.hive.service.cli.session.SessionManager",
			"org.apache.hive.service.cli.thrift.ThriftBinaryCLIService",
			"org.apache.hive.service.cli.thrift.ThriftHttpCLIService"
		],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * A HiveServer2 thread used to construct new server threads.\n * In particular, this thread ensures an orderly cleanup,\n * when killed by its corresponding ExecutorService.\n */",
		"name": "org.apache.hive.service.server.ThreadWithGarbageCleanup",
		"extends": "Thread",
		"Methods": [
			{
				"signature": "public ThreadWithGarbageCleanup(Runnable runnable)",
				"documentation": "/**\n * A HiveServer2 thread used to construct new server threads.\n * In particular, this thread ensures an orderly cleanup,\n * when killed by its corresponding ExecutorService.\n */"
			},
			{
				"signature": "@Override\n  public void finalize() throws Throwable",
				"documentation": "/**\n   * Add any Thread specific garbage cleanup code here.\n   * Currently, it shuts down the RawStore object for this thread if it is not null.\n   */"
			},
			{
				"signature": "private void cleanRawStore()",
				"documentation": ""
			},
			{
				"signature": "public void cacheThreadLocalRawStore()",
				"documentation": "/**\n   * Cache the ThreadLocal RawStore object. Called from the corresponding thread.\n   */"
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.status.api.v1.streaming.BatchStatus",
		"extends": "",
		"Methods": [
			{
				"signature": "public static BatchStatus fromString(String str)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.util.EnumUtil"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n *\n * Represents the state of a StreamingContext.\n */",
		"name": "org.apache.spark.streaming.StreamingContextState",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "enum",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.annotation.DeveloperApi"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n *\n * This abstract class represents a write ahead log (aka journal) that is used by Spark Streaming\n * to save the received data (by receivers) and associated metadata to a reliable storage, so that\n * they can be recovered after driver failures. See the Spark documentation for more information\n * on how to plug in your own custom implementation of a write ahead log.\n */",
		"name": "org.apache.spark.streaming.util.WriteAheadLog",
		"extends": "",
		"Methods": [],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * :: DeveloperApi ::\n *\n * This abstract class represents a handle that refers to a record written in a\n * {@link org.apache.spark.streaming.util.WriteAheadLog WriteAheadLog}.\n * It must contain all the information necessary for the record to be read and returned by\n * an implementation of the WriteAheadLog class.\n *\n * @see org.apache.spark.streaming.util.WriteAheadLog\n */",
		"name": "org.apache.spark.streaming.util.WriteAheadLogRecordHandle",
		"extends": "",
		"Methods": [],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaDurationSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testLess()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLessEq()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGreater()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGreaterEq()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPlus()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMinus()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTimes()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testDiv()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMilliseconds()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSeconds()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMinutes()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaMapWithStateSuite",
		"extends": "org.apache.spark.streaming.LocalJavaStreamingContext",
		"Methods": [
			{
				"signature": "public void testAPI()",
				"documentation": "/**\n   * This test is only for testing the APIs. It's not necessary to run it.\n   */"
			},
			{
				"signature": "@Test\n  public void testBasicFunction()",
				"documentation": ""
			},
			{
				"signature": "private \u003cK, S, T\u003e void testOperation(\n      List\u003cList\u003cK\u003e\u003e input,\n      StateSpec\u003cK, Integer, S, T\u003e mapWithStateSpec,\n      List\u003cSet\u003cT\u003e\u003e expectedOutputs,\n      List\u003cSet\u003cTuple2\u003cK, S\u003e\u003e\u003e expectedStateSnapshots)",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.api.java.Optional"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaReceiverAPISuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n  public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n  public void tearDown()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReceiver() throws InterruptedException",
				"documentation": ""
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.streaming.JavaReceiverAPISuite.JavaSocketReceiver"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.streaming.JavaReceiverAPISuite.JavaSocketReceiver"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaReceiverAPISuite.JavaSocketReceiver",
		"extends": "org.apache.spark.streaming.receiver.Receiver",
		"Methods": [
			{
				"signature": "JavaSocketReceiver(String host_ , int port_)",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onStart()",
				"documentation": ""
			},
			{
				"signature": "@Override\n    public void onStop()",
				"documentation": ""
			},
			{
				"signature": "private void receive()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.streaming.JavaReceiverAPISuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaStreamingListenerAPISuite",
		"extends": "JavaStreamingListener",
		"Methods": [
			{
				"signature": "@Override\n  public void onStreamingStarted(JavaStreamingListenerStreamingStarted streamingStarted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onReceiverStarted(JavaStreamingListenerReceiverStarted receiverStarted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onReceiverError(JavaStreamingListenerReceiverError receiverError)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onReceiverStopped(JavaStreamingListenerReceiverStopped receiverStopped)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onBatchSubmitted(JavaStreamingListenerBatchSubmitted batchSubmitted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onBatchStarted(JavaStreamingListenerBatchStarted batchStarted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onBatchCompleted(JavaStreamingListenerBatchCompleted batchCompleted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onOutputOperationStarted(\n      JavaStreamingListenerOutputOperationStarted outputOperationStarted)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void onOutputOperationCompleted(\n      JavaStreamingListenerOutputOperationCompleted outputOperationCompleted)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaTimeSuite",
		"extends": "",
		"Methods": [
			{
				"signature": "@Test\n  public void testLess()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLessEq()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGreater()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGreaterEq()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPlus()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMinusTime()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMinusDuration()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaWriteAheadLogSuite",
		"extends": "org.apache.spark.streaming.util.WriteAheadLog",
		"Methods": [
			{
				"signature": "@Override\n  public WriteAheadLogRecordHandle write(ByteBuffer record, long time)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public ByteBuffer read(WriteAheadLogRecordHandle handle)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public Iterator\u003cByteBuffer\u003e readAll()",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void clean(long threshTime, boolean waitForCompletion)",
				"documentation": ""
			},
			{
				"signature": "@Override\n  public void close()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCustomWAL()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.network.util.JavaUtils",
			"org.apache.spark.streaming.JavaWriteAheadLogSuite.JavaWriteAheadLogSuiteHandle",
			"org.apache.spark.streaming.JavaWriteAheadLogSuite.Record"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"org.apache.spark.streaming.JavaWriteAheadLogSuite.JavaWriteAheadLogSuiteHandle",
			"org.apache.spark.streaming.JavaWriteAheadLogSuite.Record"
		]
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaWriteAheadLogSuite.JavaWriteAheadLogSuiteHandle",
		"extends": "org.apache.spark.streaming.util.WriteAheadLogRecordHandle",
		"Methods": [
			{
				"signature": "JavaWriteAheadLogSuiteHandle(int idx)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.streaming.JavaWriteAheadLogSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.JavaWriteAheadLogSuite.Record",
		"extends": "",
		"Methods": [
			{
				"signature": "Record(long tym, int idx, ByteBuffer buf)",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"org.apache.spark.streaming.JavaWriteAheadLogSuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "org.apache.spark.streaming.LocalJavaStreamingContext",
		"extends": "",
		"Methods": [
			{
				"signature": "@Before\n    public void setUp()",
				"documentation": ""
			},
			{
				"signature": "@After\n    public void tearDown()",
				"documentation": ""
			}
		],
		"interfaces": [],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [
			"org.apache.spark.streaming.JavaMapWithStateSuite"
		],
		"implementedBy": [],
		"uses": [],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "/**\n * Most of these tests replicate org.apache.spark.streaming.JavaAPISuite using java 8\n * lambda syntax.\n */",
		"name": "test.org.apache.spark.streaming.Java8APISuite",
		"extends": "org.apache.spark.streaming.LocalJavaStreamingContext",
		"Methods": [
			{
				"signature": "@Test\n  public void testMap()",
				"documentation": "/**\n * Most of these tests replicate org.apache.spark.streaming.JavaAPISuite using java 8\n * lambda syntax.\n */"
			},
			{
				"signature": "@Test\n  public void testFilter()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMapPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduce()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduceByWindow()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testVariousTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTransformWith()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testVariousTransformWith()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testStreamingContextTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFlatMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairFlatMap()",
				"documentation": ""
			},
			{
				"signature": "public static \u003cT extends Comparable\u003cT\u003e\u003e void assertOrderInvariantEquals(\n    List\u003cList\u003cT\u003e\u003e expected, List\u003cList\u003cT\u003e\u003e actual)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairFilter()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairMapPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairMap2()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairToPairFlatMapWithChangingTypes()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairReduceByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCombineByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduceByKeyAndWindow()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUpdateStateByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduceByKeyAndWindowWithInverse()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairToNormalRDDTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMapValues()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFlatMapValues()",
				"documentation": ""
			},
			{
				"signature": "public void testMapWithStateAPI()",
				"documentation": "/**\n   * This test is only for testing the APIs. It's not necessary to run it.\n   */"
			}
		],
		"interfaces": [
			"java.io.Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.api.java.Optional"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.streaming.JavaAPISuite",
		"extends": "org.apache.spark.streaming.LocalJavaStreamingContext",
		"Methods": [
			{
				"signature": "public static void equalIterator(Iterator\u003c?\u003e a, Iterator\u003c?\u003e b)",
				"documentation": ""
			},
			{
				"signature": "public static void equalIterable(Iterable\u003c?\u003e a, Iterable\u003c?\u003e b)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testInitialization()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testContextState()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCount()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testWindow()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testWindowWithSlideDuration()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFilter()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRepartitionMorePartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRepartitionFewerPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGlom()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMapPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduce()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduceByWindowWithInverse()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduceByWindowWithoutInverse()",
				"documentation": ""
			},
			{
				"signature": "private void testReduceByWindow(boolean withInverse)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testQueueStream()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testVariousTransform()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  @Test\n  public void testTransformWith()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testVariousTransformWith()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  @Test\n  public void testStreamingContextTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFlatMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testForeachRDD()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairFlatMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUnion()",
				"documentation": ""
			},
			{
				"signature": "public static \u003cT\u003e void assertOrderInvariantEquals(\n      List\u003cList\u003cT\u003e\u003e expected, List\u003cList\u003cT\u003e\u003e actual)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairFilter()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairMap()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairMapPartitions()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairMap2()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairToPairFlatMapWithChangingTypes()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairGroupByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairReduceByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCombineByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCountByValue()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testGroupByKeyAndWindow()",
				"documentation": ""
			},
			{
				"signature": "private static Set\u003cTuple2\u003cString, HashSet\u003cInteger\u003e\u003e\u003e\n    convert(List\u003cTuple2\u003cString, List\u003cInteger\u003e\u003e\u003e listOfTuples)",
				"documentation": ""
			},
			{
				"signature": "private static Tuple2\u003cString, HashSet\u003cInteger\u003e\u003e convert(Tuple2\u003cString, List\u003cInteger\u003e\u003e tuple)",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduceByKeyAndWindow()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUpdateStateByKey()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testUpdateStateByKeyWithInitial()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testReduceByKeyAndWindowWithInverse()",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  @Test\n  public void testCountByValueAndWindow()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testPairToNormalRDDTransform()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testMapValues()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFlatMapValues()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCoGroup()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testJoin()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testLeftOuterJoin()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testCheckpointMasterRecovery() throws InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testContextGetOrCreate() throws InterruptedException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSocketTextStream()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testSocketString()",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testTextFileStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testFileStream() throws IOException",
				"documentation": ""
			},
			{
				"signature": "@Test\n  public void testRawSocketStream()",
				"documentation": ""
			},
			{
				"signature": "private static List\u003cList\u003cString\u003e\u003e fileTestPrepare(File testDir) throws IOException",
				"documentation": ""
			},
			{
				"signature": "@SuppressWarnings(\"unchecked\")\n  \n  private void compileSaveAsJavaAPI(JavaPairDStream\u003cLongWritable,Text\u003e pds)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Serializable"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [
			"org.apache.spark.api.java.Optional",
			"test.org.apache.spark.streaming.JavaAPISuite.IntegerSum",
			"test.org.apache.spark.streaming.JavaAPISuite.IntegerDifference"
		],
		"usedBy": [],
		"isPrivate": false,
		"InnerClasses": [
			"test.org.apache.spark.streaming.JavaAPISuite.IntegerSum",
			"test.org.apache.spark.streaming.JavaAPISuite.IntegerDifference"
		]
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.streaming.JavaAPISuite.IntegerSum",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public Integer call(Integer i1, Integer i2)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Function2"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.streaming.JavaAPISuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	},
	{
		"documentation": "",
		"name": "test.org.apache.spark.streaming.JavaAPISuite.IntegerDifference",
		"extends": "",
		"Methods": [
			{
				"signature": "@Override\n    public Integer call(Integer i1, Integer i2)",
				"documentation": ""
			}
		],
		"interfaces": [
			"Function2"
		],
		"type": "class",
		"isTest": false,
		"testClasses": [],
		"subClasses": [],
		"implementedBy": [],
		"uses": [],
		"usedBy": [
			"test.org.apache.spark.streaming.JavaAPISuite"
		],
		"isPrivate": true,
		"InnerClasses": []
	}
]
